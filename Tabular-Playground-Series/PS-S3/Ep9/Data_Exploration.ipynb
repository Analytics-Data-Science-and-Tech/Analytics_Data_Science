{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39494987",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost lightgbm catboost optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534b0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor \n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "import optuna \n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'analytics-data-science-competitions'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "file_key_1 = 'Tabular-Playground-Series/PS-S3/Ep9/train.csv'\n",
    "file_key_2 = 'Tabular-Playground-Series/PS-S3/Ep9/test.csv'\n",
    "file_key_3 = 'Tabular-Playground-Series/PS-S3/Ep9/sample_submission.csv'\n",
    "\n",
    "bucket_object_1 = bucket.Object(file_key_1)\n",
    "file_object_1 = bucket_object_1.get()\n",
    "file_content_stream_1 = file_object_1.get('Body')\n",
    "\n",
    "bucket_object_2 = bucket.Object(file_key_2)\n",
    "file_object_2 = bucket_object_2.get()\n",
    "file_content_stream_2 = file_object_2.get('Body')\n",
    "\n",
    "bucket_object_3 = bucket.Object(file_key_3)\n",
    "file_object_3 = bucket_object_3.get()\n",
    "file_content_stream_3 = file_object_3.get('Body')\n",
    "\n",
    "## Reading data files\n",
    "train = pd.read_csv(file_content_stream_1)\n",
    "test = pd.read_csv(file_content_stream_2)\n",
    "submission = pd.read_csv(file_content_stream_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707943fc",
   "metadata": {},
   "source": [
    "# Baseline Modeling 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns = ['id', 'Strength'], axis = 1)\n",
    "Y = train['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "XGB_cv_scores, XGB_imp = list(), list()\n",
    "XGB_preds = list()\n",
    "\n",
    "lgb_cv_scores, lgb_imp = list(), list()\n",
    "lgb_preds = list()\n",
    "\n",
    "cat_cv_scores, cat_imp = list(), list()\n",
    "cat_preds = list()\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.7, \n",
    "                              gamma = 0.8, \n",
    "                              learning_rate = 0.01, \n",
    "                              max_depth = 7, \n",
    "                              min_child_weight = 10, \n",
    "                              n_estimators = 1000, \n",
    "                              subsample = 0.7).fit(X_train, Y_train)\n",
    "        XGB_imp.append(XGB_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        XGB_cv_scores.append(mean_squared_error(Y_test, XGB_pred_1, squared = False))\n",
    "        XGB_preds.append(XGB_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 1000,\n",
    "                               max_depth = 7,\n",
    "                               learning_rate = 0.01,\n",
    "                               num_leaves = 20,\n",
    "                               lambda_l1 = 3,\n",
    "                               lambda_l2 = 3,\n",
    "                               bagging_fraction = 0.7,\n",
    "                               feature_fraction = 0.7).fit(X_train, Y_train)\n",
    "        lgb_imp.append(lgb_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        lgb_cv_scores.append(mean_squared_error(Y_test, lgb_pred_1, squared = False))\n",
    "        lgb_preds.append(lgb_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 1000,\n",
    "                                   learning_rate = 0.01,\n",
    "                                   depth = 7,\n",
    "                                   random_strength = 0.5,\n",
    "                                   bagging_temperature = 0.7,\n",
    "                                   border_count = 30,\n",
    "                                   l2_leaf_reg = 5,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        cat_imp.append(cat_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        cat_cv_scores.append(mean_squared_error(Y_test, cat_pred_1, squared = False))\n",
    "        cat_preds.append(cat_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_cv_score = np.mean(XGB_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the XGBoost model is:', XGB_cv_score)\n",
    "\n",
    "lgb_cv_score = np.mean(lgb_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the LightGBM model is:', lgb_cv_score)\n",
    "\n",
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f58feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preds_test = pd.DataFrame(cat_preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = cat_preds_test\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('catboost_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391bec8",
   "metadata": {},
   "source": [
    "# Baseline Modeling 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].mean()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "XGB_cv_scores, XGB_imp = list(), list()\n",
    "XGB_preds = list()\n",
    "\n",
    "lgb_cv_scores, lgb_imp = list(), list()\n",
    "lgb_preds = list()\n",
    "\n",
    "cat_cv_scores, cat_imp = list(), list()\n",
    "cat_preds = list()\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.7, \n",
    "                              gamma = 0.8, \n",
    "                              learning_rate = 0.01, \n",
    "                              max_depth = 7, \n",
    "                              min_child_weight = 10, \n",
    "                              n_estimators = 1000, \n",
    "                              subsample = 0.7).fit(X_train, Y_train)\n",
    "        XGB_imp.append(XGB_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        XGB_cv_scores.append(mean_squared_error(Y_test, XGB_pred_1, squared = False))\n",
    "        XGB_preds.append(XGB_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 1000,\n",
    "                               max_depth = 7,\n",
    "                               learning_rate = 0.01,\n",
    "                               num_leaves = 20,\n",
    "                               lambda_l1 = 3,\n",
    "                               lambda_l2 = 3,\n",
    "                               bagging_fraction = 0.7,\n",
    "                               feature_fraction = 0.7).fit(X_train, Y_train)\n",
    "        lgb_imp.append(lgb_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        lgb_cv_scores.append(mean_squared_error(Y_test, lgb_pred_1, squared = False))\n",
    "        lgb_preds.append(lgb_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 1000,\n",
    "                                   learning_rate = 0.01,\n",
    "                                   depth = 7,\n",
    "                                   random_strength = 0.5,\n",
    "                                   bagging_temperature = 0.7,\n",
    "                                   border_count = 30,\n",
    "                                   l2_leaf_reg = 5,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        cat_imp.append(cat_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        cat_cv_scores.append(mean_squared_error(Y_test, cat_pred_1, squared = False))\n",
    "        cat_preds.append(cat_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c10ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_cv_score = np.mean(XGB_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the XGBoost model is:', XGB_cv_score)\n",
    "\n",
    "lgb_cv_score = np.mean(lgb_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the LightGBM model is:', lgb_cv_score)\n",
    "\n",
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preds_test = pd.DataFrame(cat_preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = cat_preds_test\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('catboost_submission_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da212d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].median()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "XGB_cv_scores, XGB_imp = list(), list()\n",
    "XGB_preds = list()\n",
    "\n",
    "lgb_cv_scores, lgb_imp = list(), list()\n",
    "lgb_preds = list()\n",
    "\n",
    "cat_cv_scores, cat_imp = list(), list()\n",
    "cat_preds = list()\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "#         XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "#                               colsample_bytree = 0.7, \n",
    "#                               gamma = 0.8, \n",
    "#                               learning_rate = 0.01, \n",
    "#                               max_depth = 7, \n",
    "#                               min_child_weight = 10, \n",
    "#                               n_estimators = 1000, \n",
    "#                               subsample = 0.7).fit(X_train, Y_train)\n",
    "#         XGB_imp.append(XGB_md.feature_importances_)\n",
    "        \n",
    "#         ## Predicting on X_test and test\n",
    "#         XGB_pred_1 = XGB_md.predict(X_test)\n",
    "#         XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        \n",
    "#         ## Computing rmse\n",
    "#         XGB_cv_scores.append(mean_squared_error(Y_test, XGB_pred_1, squared = False))\n",
    "#         XGB_preds.append(XGB_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "#         lgb_md = LGBMRegressor(n_estimators = 1000,\n",
    "#                                max_depth = 7,\n",
    "#                                learning_rate = 0.01,\n",
    "#                                num_leaves = 20,\n",
    "#                                lambda_l1 = 3,\n",
    "#                                lambda_l2 = 3,\n",
    "#                                bagging_fraction = 0.7,\n",
    "#                                feature_fraction = 0.7).fit(X_train, Y_train)\n",
    "#         lgb_imp.append(lgb_md.feature_importances_)\n",
    "        \n",
    "#         ## Predicting on X_test and test\n",
    "#         lgb_pred_1 = lgb_md.predict(X_test)\n",
    "#         lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        \n",
    "#         ## Computing rmse\n",
    "#         lgb_cv_scores.append(mean_squared_error(Y_test, lgb_pred_1, squared = False))\n",
    "#         lgb_preds.append(lgb_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 4738,\n",
    "                                   learning_rate = 0.003143666241424718,\n",
    "                                   depth = 4,\n",
    "                                   random_strength = 0.29823973415192867,\n",
    "                                   bagging_temperature = 0.3408793603898661,\n",
    "                                   border_count = 112,\n",
    "                                   l2_leaf_reg = 17,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        cat_imp.append(cat_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        cat_cv_scores.append(mean_squared_error(Y_test, cat_pred_1, squared = False))\n",
    "        cat_preds.append(cat_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9b7988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is: 11.880628569091348\n"
     ]
    }
   ],
   "source": [
    "# XGB_cv_score = np.mean(XGB_cv_scores)    \n",
    "# print('The average oof rmse score over 5-folds (run 5 times) of the XGBoost model is:', XGB_cv_score)\n",
    "\n",
    "# lgb_cv_score = np.mean(lgb_cv_scores)    \n",
    "# print('The average oof rmse score over 5-folds (run 5 times) of the LightGBM model is:', lgb_cv_score)\n",
    "\n",
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d8323",
   "metadata": {},
   "source": [
    "# Baseline 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24550d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedEnsemble(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _rmse_loss(self, coef, X, y):\n",
    "        \n",
    "        ens = coef[0]*X[:, 0] + coef[1]*X[:, 1] + coef[2]*X[:, 2]\n",
    "        ll = mean_squared_error(y, ens, squared = False)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._rmse_loss, X = X, y = y)\n",
    "        initial_coef = [1/3, 1/3, 1/3]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \n",
    "        ens = coef[0]*X[:, 0] + coef[1]*X[:, 1] + coef[2]*X[:, 2]\n",
    "        return ens\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54bbcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:27<09:50, 147.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [04:48<07:11, 143.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [07:05<04:40, 140.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [09:13<02:15, 135.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:35<00:00, 139.09s/it]\n"
     ]
    }
   ],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].median()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "ens_cv_scores, preds = list(), list()\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.671460244215802, \n",
    "                              gamma = 2.5281806276307384, \n",
    "                              learning_rate = 0.002046162779305807, \n",
    "                              max_depth = 8, \n",
    "                              min_child_weight = 80, \n",
    "                              n_estimators = 2690, \n",
    "                              subsample = 0.44886485549735244).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 5420,\n",
    "                               max_depth = 3,\n",
    "                               learning_rate = 0.0014779400349972686,\n",
    "                               num_leaves = 61,\n",
    "                               lambda_l1 = 7.384172796287736,\n",
    "                               lambda_l2 = 0.10456555506292783,\n",
    "                               bagging_fraction = 0.22841166601766863,\n",
    "                               feature_fraction = 0.659898030).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 4738,\n",
    "                                   learning_rate = 0.003143666241424718,\n",
    "                                   depth = 4,\n",
    "                                   random_strength = 0.29823973415192867,\n",
    "                                   bagging_temperature = 0.3408793603898661,\n",
    "                                   border_count = 112,\n",
    "                                   l2_leaf_reg = 17,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        \n",
    "        ######################\n",
    "        ## Optimal Ensemble ##\n",
    "        ######################\n",
    "        \n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        models_pred_oof = np.transpose((XGB_pred_1, lgb_pred_1, cat_pred_1))\n",
    "\n",
    "        opt_ens = OptimizedEnsemble()\n",
    "        opt_ens.fit(models_pred_oof, Y_test)\n",
    "        coef = opt_ens.coefficients()\n",
    "        \n",
    "        ens_pred = opt_ens.predict(models_pred_oof, coef)\n",
    "        ens_cv_scores.append(mean_squared_error(Y_test, ens_pred, squared = False))\n",
    "        \n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        models_pred = np.transpose((XGB_pred_2, lgb_pred_2, cat_pred_2))\n",
    "        \n",
    "        ens_preds = opt_ens.predict(models_pred, coef)\n",
    "        preds.append(ens_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76fef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the ensemble model is: 11.835470374627613\n"
     ]
    }
   ],
   "source": [
    "ens_cv_score = np.mean(ens_cv_scores) # first run    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the ensemble model is:', ens_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7282081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the ensemble model is: 11.835470374627613\n"
     ]
    }
   ],
   "source": [
    "ens_cv_score = np.mean(ens_cv_scores)  # second run  \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the ensemble model is:', ens_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ec0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_preds_test = pd.DataFrame(preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = ens_preds_test\n",
    "submission.to_csv('Ensemble_Optuna_baseline_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca251a32",
   "metadata": {},
   "source": [
    "## Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0343f937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CementComponent</th>\n",
       "      <th>BlastFurnaceSlag</th>\n",
       "      <th>FlyAshComponent</th>\n",
       "      <th>WaterComponent</th>\n",
       "      <th>SuperplasticizerComponent</th>\n",
       "      <th>CoarseAggregateComponent</th>\n",
       "      <th>FineAggregateComponent</th>\n",
       "      <th>AgeInDays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>879.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CementComponent  BlastFurnaceSlag  FlyAshComponent  WaterComponent  \\\n",
       "0            102.0               0.0              0.0           192.0   \n",
       "1            102.0             153.0              0.0           192.0   \n",
       "2            102.0             153.0              0.0           192.0   \n",
       "3            102.0             153.0              0.0           192.0   \n",
       "4            102.0             153.0              0.0           192.0   \n",
       "\n",
       "   SuperplasticizerComponent  CoarseAggregateComponent  \\\n",
       "0                        0.0                     879.0   \n",
       "1                        0.0                     887.0   \n",
       "2                        0.0                     887.0   \n",
       "3                        0.0                     887.0   \n",
       "4                        0.0                     887.0   \n",
       "\n",
       "   FineAggregateComponent  AgeInDays  \n",
       "0                   942.0          3  \n",
       "1                   942.0          3  \n",
       "2                   942.0          7  \n",
       "3                   942.0         28  \n",
       "4                   942.0         90  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2837d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e831f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6b5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d783c87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590cb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8e17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99979183",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].median()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "ens_cv_scores, preds = list(), list()\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "#         #############    \n",
    "#         ## XGBoost ##\n",
    "#         #############\n",
    "        \n",
    "#         XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "#                               colsample_bytree = 0.671460244215802, \n",
    "#                               gamma = 2.5281806276307384, \n",
    "#                               learning_rate = 0.002046162779305807, \n",
    "#                               max_depth = 8, \n",
    "#                               min_child_weight = 80, \n",
    "#                               n_estimators = 2690, \n",
    "#                               subsample = 0.44886485549735244).fit(X_train, Y_train)\n",
    "        \n",
    "#         ##############\n",
    "#         ## LightGBM ##\n",
    "#         ##############\n",
    "        \n",
    "#         lgb_md = LGBMRegressor(n_estimators = 5420,\n",
    "#                                max_depth = 3,\n",
    "#                                learning_rate = 0.0014779400349972686,\n",
    "#                                num_leaves = 61,\n",
    "#                                lambda_l1 = 7.384172796287736,\n",
    "#                                lambda_l2 = 0.10456555506292783,\n",
    "#                                bagging_fraction = 0.22841166601766863,\n",
    "#                                feature_fraction = 0.659898030).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 4738,\n",
    "                                   learning_rate = 0.003143666241424718,\n",
    "                                   depth = 4,\n",
    "                                   random_strength = 0.29823973415192867,\n",
    "                                   bagging_temperature = 0.3408793603898661,\n",
    "                                   border_count = 112,\n",
    "                                   l2_leaf_reg = 17,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        \n",
    "        ######################\n",
    "        ## Optimal Ensemble ##\n",
    "        ######################\n",
    "        \n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        models_pred_oof = np.transpose((XGB_pred_1, lgb_pred_1, cat_pred_1))\n",
    "\n",
    "        opt_ens = OptimizedEnsemble()\n",
    "        opt_ens.fit(models_pred_oof, Y_test)\n",
    "        coef = opt_ens.coefficients()\n",
    "        \n",
    "        ens_pred = opt_ens.predict(models_pred_oof, coef)\n",
    "        ens_cv_scores.append(mean_squared_error(Y_test, ens_pred, squared = False))\n",
    "        \n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        models_pred = np.transpose((XGB_pred_2, lgb_pred_2, cat_pred_2))\n",
    "        \n",
    "        ens_preds = opt_ens.predict(models_pred, coef)\n",
    "        preds.append(ens_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34927aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6058b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e2084eb",
   "metadata": {},
   "source": [
    "# Baseline 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c4e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedEnsemble(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _rmse_loss(self, coef, X, y):\n",
    "        \n",
    "        ens = coef[0]*X[:, 0] + coef[1]*X[:, 1] + coef[2]*X[:, 2]\n",
    "        ll = mean_squared_error(y, ens, squared = False)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._rmse_loss, X = X, y = y)\n",
    "        initial_coef = [1/3, 1/3, 1/3]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \n",
    "        ens = coef[0]*X[:, 0] + coef[1]*X[:, 1] + coef[2]*X[:, 2]\n",
    "        return ens\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54306d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:24<01:37, 24.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:45<01:08, 22.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:12<00:48, 24.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:37<00:24, 24.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:06<00:00, 25.23s/it]\n"
     ]
    }
   ],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].median()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "X['WaterComponent_to_Cement_ratio'] = X['WaterComponent'] / (X['CementComponent'] + 1e-6)\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "test_baseline['WaterComponent_to_Cement_ratio'] = test_baseline['WaterComponent'] / (test_baseline['CementComponent'] + 1e-6)\n",
    "\n",
    "ens_cv_scores, preds = list(), list()\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.7940111565728297, \n",
    "                              gamma = 3.3316249893010292, \n",
    "                              learning_rate = 0.0009167011149361065, \n",
    "                              max_depth = 8, \n",
    "                              min_child_weight = 89, \n",
    "                              n_estimators = 6766, \n",
    "                              subsample = 0.3771654611184001).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 4273,\n",
    "                               max_depth = 4,\n",
    "                               learning_rate = 0.002480759916271656,\n",
    "                               num_leaves = 5,\n",
    "                               lambda_l1 = 1.2582711296889206,\n",
    "                               lambda_l2 = 0.018040323452000916,\n",
    "                               bagging_fraction = 0.7658802307213928,\n",
    "                               feature_fraction = 0.4895760311486668).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 416,\n",
    "                                   learning_rate = 0.031225761812299576,\n",
    "                                   depth = 4,\n",
    "                                   random_strength = 0.19789440193456237,\n",
    "                                   bagging_temperature = 0.2831892755259466,\n",
    "                                   border_count = 56,\n",
    "                                   l2_leaf_reg = 26,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        \n",
    "        ######################\n",
    "        ## Optimal Ensemble ##\n",
    "        ######################\n",
    "        \n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        models_pred_oof = np.transpose((XGB_pred_1, lgb_pred_1, cat_pred_1))\n",
    "\n",
    "        opt_ens = OptimizedEnsemble()\n",
    "        opt_ens.fit(models_pred_oof, Y_test)\n",
    "        coef = opt_ens.coefficients()\n",
    "        \n",
    "        ens_pred = opt_ens.predict(models_pred_oof, coef)\n",
    "        ens_cv_scores.append(mean_squared_error(Y_test, ens_pred, squared = False))\n",
    "        \n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        models_pred = np.transpose((XGB_pred_2, lgb_pred_2, cat_pred_2))\n",
    "        \n",
    "        ens_preds = opt_ens.predict(models_pred, coef)\n",
    "        preds.append(ens_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "182b9e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the ensemble model is: 11.813790317617261\n"
     ]
    }
   ],
   "source": [
    "ens_cv_score = np.mean(ens_cv_scores) \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the ensemble model is:', ens_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49a168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_preds_test = pd.DataFrame(preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = ens_preds_test\n",
    "submission.to_csv('Ensemble_Optuna_baseline_submission_2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
