{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5c025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting fastparquet\n",
      "  Using cached fastparquet-0.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Collecting cramjam>=2.3.0\n",
      "  Using cached cramjam-2.6.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from fastparquet) (2021.11.1)\n",
      "Requirement already satisfied: numpy>=1.18 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from fastparquet) (1.20.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from fastparquet) (21.0)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from fastparquet) (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from pandas>=1.1.0->fastparquet) (2021.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from packaging->fastparquet) (3.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.16.0)\n",
      "Installing collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.6.1 fastparquet-0.8.3\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastparquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a95d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0.7301891713.csv</th>\n",
       "      <th>0.6750726968.csv</th>\n",
       "      <th>0.7194704070.csv</th>\n",
       "      <th>0.7107007521.csv</th>\n",
       "      <th>0.6952032365.csv</th>\n",
       "      <th>0.7311830751.csv</th>\n",
       "      <th>0.6736005999.csv</th>\n",
       "      <th>0.7205109360.csv</th>\n",
       "      <th>0.7298116981.csv</th>\n",
       "      <th>0.6737624943.csv</th>\n",
       "      <th>0.7105097012.csv</th>\n",
       "      <th>0.7163693161.csv</th>\n",
       "      <th>0.7225567087.csv</th>\n",
       "      <th>0.6872126167.csv</th>\n",
       "      <th>0.7292947020.csv</th>\n",
       "      <th>0.6798503735.csv</th>\n",
       "      <th>0.7130672940.csv</th>\n",
       "      <th>0.6709092823.csv</th>\n",
       "      <th>0.7391105353.csv</th>\n",
       "      <th>0.6616334858.csv</th>\n",
       "      <th>0.6897190987.csv</th>\n",
       "      <th>0.6963558296.csv</th>\n",
       "      <th>0.7170726761.csv</th>\n",
       "      <th>0.7156669797.csv</th>\n",
       "      <th>0.6814483512.csv</th>\n",
       "      <th>0.7167410246.csv</th>\n",
       "      <th>0.6981892492.csv</th>\n",
       "      <th>0.7119194656.csv</th>\n",
       "      <th>0.6932545022.csv</th>\n",
       "      <th>0.6593521643.csv</th>\n",
       "      <th>0.6743157708.csv</th>\n",
       "      <th>0.7125579799.csv</th>\n",
       "      <th>0.7069356577.csv</th>\n",
       "      <th>0.6884713711.csv</th>\n",
       "      <th>0.6702128703.csv</th>\n",
       "      <th>0.6564147356.csv</th>\n",
       "      <th>0.6977737011.csv</th>\n",
       "      <th>0.6638768547.csv</th>\n",
       "      <th>0.7032756348.csv</th>\n",
       "      <th>0.6862982027.csv</th>\n",
       "      <th>0.6864219834.csv</th>\n",
       "      <th>0.7088904051.csv</th>\n",
       "      <th>0.7212180410.csv</th>\n",
       "      <th>0.7421980731.csv</th>\n",
       "      <th>0.6825341422.csv</th>\n",
       "      <th>0.6848737762.csv</th>\n",
       "      <th>0.7144978161.csv</th>\n",
       "      <th>0.6658856476.csv</th>\n",
       "      <th>0.7195078907.csv</th>\n",
       "      <th>...</th>\n",
       "      <th>0.6870791547.csv</th>\n",
       "      <th>0.6967096004.csv</th>\n",
       "      <th>0.7060967827.csv</th>\n",
       "      <th>0.7113625862.csv</th>\n",
       "      <th>0.6694947166.csv</th>\n",
       "      <th>0.6900693308.csv</th>\n",
       "      <th>0.7128591265.csv</th>\n",
       "      <th>0.6818718747.csv</th>\n",
       "      <th>0.6858034178.csv</th>\n",
       "      <th>0.7122377012.csv</th>\n",
       "      <th>0.7144335192.csv</th>\n",
       "      <th>0.7083170570.csv</th>\n",
       "      <th>0.7142499638.csv</th>\n",
       "      <th>0.7136242881.csv</th>\n",
       "      <th>0.6888742488.csv</th>\n",
       "      <th>0.7087145574.csv</th>\n",
       "      <th>0.6566530931.csv</th>\n",
       "      <th>0.7197358544.csv</th>\n",
       "      <th>0.7101817035.csv</th>\n",
       "      <th>0.7520219713.csv</th>\n",
       "      <th>0.7215952204.csv</th>\n",
       "      <th>0.6795805487.csv</th>\n",
       "      <th>0.7083706040.csv</th>\n",
       "      <th>0.6941218921.csv</th>\n",
       "      <th>0.6926641668.csv</th>\n",
       "      <th>0.6869857003.csv</th>\n",
       "      <th>0.7198785912.csv</th>\n",
       "      <th>0.7287761089.csv</th>\n",
       "      <th>0.7035686386.csv</th>\n",
       "      <th>0.7336496744.csv</th>\n",
       "      <th>0.6872533788.csv</th>\n",
       "      <th>0.7005346672.csv</th>\n",
       "      <th>0.7369069391.csv</th>\n",
       "      <th>0.7156537235.csv</th>\n",
       "      <th>0.7121002711.csv</th>\n",
       "      <th>0.7289228406.csv</th>\n",
       "      <th>0.7174789519.csv</th>\n",
       "      <th>0.7032181627.csv</th>\n",
       "      <th>0.6668147232.csv</th>\n",
       "      <th>0.7345858839.csv</th>\n",
       "      <th>0.7056743178.csv</th>\n",
       "      <th>0.6850481924.csv</th>\n",
       "      <th>0.7359858591.csv</th>\n",
       "      <th>0.7068431477.csv</th>\n",
       "      <th>0.7121179915.csv</th>\n",
       "      <th>0.6613117872.csv</th>\n",
       "      <th>0.6843637618.csv</th>\n",
       "      <th>0.6781392004.csv</th>\n",
       "      <th>0.7222809303.csv</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.913897</td>\n",
       "      <td>2.142608</td>\n",
       "      <td>1.502359</td>\n",
       "      <td>1.453620</td>\n",
       "      <td>1.336538</td>\n",
       "      <td>1.599475</td>\n",
       "      <td>1.236069</td>\n",
       "      <td>1.054479</td>\n",
       "      <td>1.131611</td>\n",
       "      <td>0.984009</td>\n",
       "      <td>1.171605</td>\n",
       "      <td>1.207453</td>\n",
       "      <td>1.400562</td>\n",
       "      <td>1.156730</td>\n",
       "      <td>1.655767</td>\n",
       "      <td>0.405465</td>\n",
       "      <td>1.194848</td>\n",
       "      <td>1.542418</td>\n",
       "      <td>1.921342</td>\n",
       "      <td>0.641228</td>\n",
       "      <td>1.025477</td>\n",
       "      <td>1.378848</td>\n",
       "      <td>1.257294</td>\n",
       "      <td>1.280277</td>\n",
       "      <td>1.467210</td>\n",
       "      <td>1.443668</td>\n",
       "      <td>1.391678</td>\n",
       "      <td>1.419787</td>\n",
       "      <td>1.054552</td>\n",
       "      <td>3.539317</td>\n",
       "      <td>0.983944</td>\n",
       "      <td>1.476214</td>\n",
       "      <td>1.234424</td>\n",
       "      <td>1.307563</td>\n",
       "      <td>2.385436</td>\n",
       "      <td>2.001724</td>\n",
       "      <td>1.257358</td>\n",
       "      <td>1.689581</td>\n",
       "      <td>1.290093</td>\n",
       "      <td>1.277571</td>\n",
       "      <td>1.269417</td>\n",
       "      <td>0.953358</td>\n",
       "      <td>0.838936</td>\n",
       "      <td>1.437067</td>\n",
       "      <td>1.378712</td>\n",
       "      <td>-0.090514</td>\n",
       "      <td>1.220395</td>\n",
       "      <td>2.258836</td>\n",
       "      <td>1.228620</td>\n",
       "      <td>...</td>\n",
       "      <td>1.172375</td>\n",
       "      <td>1.109329</td>\n",
       "      <td>1.245783</td>\n",
       "      <td>1.377996</td>\n",
       "      <td>1.435160</td>\n",
       "      <td>1.195812</td>\n",
       "      <td>1.595924</td>\n",
       "      <td>1.338426</td>\n",
       "      <td>1.170736</td>\n",
       "      <td>1.263989</td>\n",
       "      <td>0.925031</td>\n",
       "      <td>0.862942</td>\n",
       "      <td>1.198344</td>\n",
       "      <td>1.414648</td>\n",
       "      <td>1.305720</td>\n",
       "      <td>0.937820</td>\n",
       "      <td>0.674106</td>\n",
       "      <td>1.475712</td>\n",
       "      <td>1.064896</td>\n",
       "      <td>0.678348</td>\n",
       "      <td>1.330522</td>\n",
       "      <td>1.242018</td>\n",
       "      <td>1.256842</td>\n",
       "      <td>1.234825</td>\n",
       "      <td>1.252730</td>\n",
       "      <td>1.259111</td>\n",
       "      <td>1.237227</td>\n",
       "      <td>1.548106</td>\n",
       "      <td>1.496501</td>\n",
       "      <td>1.588877</td>\n",
       "      <td>1.322937</td>\n",
       "      <td>1.301587</td>\n",
       "      <td>1.143404</td>\n",
       "      <td>1.285821</td>\n",
       "      <td>1.362650</td>\n",
       "      <td>1.133906</td>\n",
       "      <td>1.368576</td>\n",
       "      <td>1.159616</td>\n",
       "      <td>2.507343</td>\n",
       "      <td>0.925301</td>\n",
       "      <td>1.451954</td>\n",
       "      <td>1.199074</td>\n",
       "      <td>1.364975</td>\n",
       "      <td>1.650185</td>\n",
       "      <td>1.184054</td>\n",
       "      <td>0.660613</td>\n",
       "      <td>1.385376</td>\n",
       "      <td>1.596353</td>\n",
       "      <td>1.544845</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.678169</td>\n",
       "      <td>0.576310</td>\n",
       "      <td>0.298877</td>\n",
       "      <td>0.301982</td>\n",
       "      <td>0.698756</td>\n",
       "      <td>1.131453</td>\n",
       "      <td>0.569003</td>\n",
       "      <td>0.426954</td>\n",
       "      <td>0.510710</td>\n",
       "      <td>0.430144</td>\n",
       "      <td>0.221108</td>\n",
       "      <td>0.264951</td>\n",
       "      <td>1.556663</td>\n",
       "      <td>1.082496</td>\n",
       "      <td>1.407428</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>0.920520</td>\n",
       "      <td>0.907372</td>\n",
       "      <td>1.113602</td>\n",
       "      <td>0.179283</td>\n",
       "      <td>1.125816</td>\n",
       "      <td>0.637407</td>\n",
       "      <td>0.381426</td>\n",
       "      <td>0.376108</td>\n",
       "      <td>1.140530</td>\n",
       "      <td>0.507529</td>\n",
       "      <td>0.556460</td>\n",
       "      <td>0.232753</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.810242</td>\n",
       "      <td>1.154435</td>\n",
       "      <td>0.464158</td>\n",
       "      <td>0.571005</td>\n",
       "      <td>0.498050</td>\n",
       "      <td>0.208698</td>\n",
       "      <td>0.693851</td>\n",
       "      <td>0.753772</td>\n",
       "      <td>0.798899</td>\n",
       "      <td>0.556192</td>\n",
       "      <td>0.626027</td>\n",
       "      <td>0.599505</td>\n",
       "      <td>0.451303</td>\n",
       "      <td>0.421232</td>\n",
       "      <td>1.578557</td>\n",
       "      <td>1.518368</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>0.332251</td>\n",
       "      <td>0.617989</td>\n",
       "      <td>0.828814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323980</td>\n",
       "      <td>0.483191</td>\n",
       "      <td>0.284638</td>\n",
       "      <td>0.408454</td>\n",
       "      <td>0.674884</td>\n",
       "      <td>0.355031</td>\n",
       "      <td>0.396140</td>\n",
       "      <td>1.250862</td>\n",
       "      <td>0.718760</td>\n",
       "      <td>0.147479</td>\n",
       "      <td>0.554945</td>\n",
       "      <td>0.257595</td>\n",
       "      <td>0.372066</td>\n",
       "      <td>0.811467</td>\n",
       "      <td>0.535556</td>\n",
       "      <td>0.407907</td>\n",
       "      <td>0.046973</td>\n",
       "      <td>0.404249</td>\n",
       "      <td>0.377877</td>\n",
       "      <td>0.950909</td>\n",
       "      <td>0.288525</td>\n",
       "      <td>0.595133</td>\n",
       "      <td>0.411427</td>\n",
       "      <td>0.758979</td>\n",
       "      <td>0.378051</td>\n",
       "      <td>0.539336</td>\n",
       "      <td>0.649062</td>\n",
       "      <td>0.266136</td>\n",
       "      <td>0.484547</td>\n",
       "      <td>0.557583</td>\n",
       "      <td>0.529086</td>\n",
       "      <td>1.068429</td>\n",
       "      <td>0.249275</td>\n",
       "      <td>0.325167</td>\n",
       "      <td>0.454034</td>\n",
       "      <td>0.363870</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.191891</td>\n",
       "      <td>0.913241</td>\n",
       "      <td>0.593457</td>\n",
       "      <td>0.576423</td>\n",
       "      <td>0.604565</td>\n",
       "      <td>0.851182</td>\n",
       "      <td>0.545539</td>\n",
       "      <td>0.652546</td>\n",
       "      <td>0.147133</td>\n",
       "      <td>0.615547</td>\n",
       "      <td>0.412286</td>\n",
       "      <td>0.128416</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.222971</td>\n",
       "      <td>1.493293</td>\n",
       "      <td>1.997012</td>\n",
       "      <td>1.580462</td>\n",
       "      <td>1.181609</td>\n",
       "      <td>1.527942</td>\n",
       "      <td>1.655077</td>\n",
       "      <td>1.549787</td>\n",
       "      <td>1.668255</td>\n",
       "      <td>1.272369</td>\n",
       "      <td>1.688099</td>\n",
       "      <td>1.579537</td>\n",
       "      <td>-0.373970</td>\n",
       "      <td>1.167650</td>\n",
       "      <td>1.493627</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>1.961822</td>\n",
       "      <td>1.120913</td>\n",
       "      <td>1.743965</td>\n",
       "      <td>0.915126</td>\n",
       "      <td>1.470307</td>\n",
       "      <td>1.403919</td>\n",
       "      <td>1.821767</td>\n",
       "      <td>1.689505</td>\n",
       "      <td>1.227045</td>\n",
       "      <td>1.376995</td>\n",
       "      <td>1.348107</td>\n",
       "      <td>1.619013</td>\n",
       "      <td>0.584926</td>\n",
       "      <td>1.492138</td>\n",
       "      <td>1.441973</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.537328</td>\n",
       "      <td>1.360002</td>\n",
       "      <td>1.502614</td>\n",
       "      <td>1.400423</td>\n",
       "      <td>1.176323</td>\n",
       "      <td>1.449841</td>\n",
       "      <td>1.581607</td>\n",
       "      <td>1.417900</td>\n",
       "      <td>1.377263</td>\n",
       "      <td>1.590156</td>\n",
       "      <td>1.667259</td>\n",
       "      <td>1.592731</td>\n",
       "      <td>1.206471</td>\n",
       "      <td>0.565491</td>\n",
       "      <td>1.875071</td>\n",
       "      <td>1.503783</td>\n",
       "      <td>1.565373</td>\n",
       "      <td>...</td>\n",
       "      <td>1.275068</td>\n",
       "      <td>1.503897</td>\n",
       "      <td>1.652910</td>\n",
       "      <td>1.629185</td>\n",
       "      <td>1.435720</td>\n",
       "      <td>1.459440</td>\n",
       "      <td>1.472051</td>\n",
       "      <td>1.205913</td>\n",
       "      <td>1.217617</td>\n",
       "      <td>1.356744</td>\n",
       "      <td>1.547532</td>\n",
       "      <td>1.491958</td>\n",
       "      <td>1.631098</td>\n",
       "      <td>1.402725</td>\n",
       "      <td>1.328202</td>\n",
       "      <td>1.506278</td>\n",
       "      <td>0.673713</td>\n",
       "      <td>1.670106</td>\n",
       "      <td>1.525643</td>\n",
       "      <td>1.863093</td>\n",
       "      <td>1.558563</td>\n",
       "      <td>1.463832</td>\n",
       "      <td>1.483774</td>\n",
       "      <td>1.248124</td>\n",
       "      <td>1.574611</td>\n",
       "      <td>1.385882</td>\n",
       "      <td>1.543597</td>\n",
       "      <td>1.735119</td>\n",
       "      <td>1.300939</td>\n",
       "      <td>0.887515</td>\n",
       "      <td>1.389234</td>\n",
       "      <td>1.073305</td>\n",
       "      <td>1.716729</td>\n",
       "      <td>1.470162</td>\n",
       "      <td>1.566478</td>\n",
       "      <td>1.463176</td>\n",
       "      <td>1.638943</td>\n",
       "      <td>1.477231</td>\n",
       "      <td>1.721804</td>\n",
       "      <td>1.291973</td>\n",
       "      <td>1.703252</td>\n",
       "      <td>1.275325</td>\n",
       "      <td>1.562809</td>\n",
       "      <td>1.723188</td>\n",
       "      <td>1.644262</td>\n",
       "      <td>0.939329</td>\n",
       "      <td>1.258014</td>\n",
       "      <td>1.263616</td>\n",
       "      <td>1.449718</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.896871</td>\n",
       "      <td>1.278293</td>\n",
       "      <td>0.973904</td>\n",
       "      <td>1.035144</td>\n",
       "      <td>1.308422</td>\n",
       "      <td>0.783468</td>\n",
       "      <td>1.386619</td>\n",
       "      <td>0.824402</td>\n",
       "      <td>2.278990</td>\n",
       "      <td>1.024375</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>1.001671</td>\n",
       "      <td>0.373937</td>\n",
       "      <td>1.295792</td>\n",
       "      <td>1.057832</td>\n",
       "      <td>0.994623</td>\n",
       "      <td>1.556343</td>\n",
       "      <td>1.081597</td>\n",
       "      <td>1.754679</td>\n",
       "      <td>0.603690</td>\n",
       "      <td>0.924175</td>\n",
       "      <td>1.172713</td>\n",
       "      <td>0.905020</td>\n",
       "      <td>1.081385</td>\n",
       "      <td>0.965547</td>\n",
       "      <td>1.095021</td>\n",
       "      <td>1.106409</td>\n",
       "      <td>1.159985</td>\n",
       "      <td>0.324042</td>\n",
       "      <td>1.479513</td>\n",
       "      <td>1.223506</td>\n",
       "      <td>0.954404</td>\n",
       "      <td>1.265987</td>\n",
       "      <td>1.090734</td>\n",
       "      <td>1.270504</td>\n",
       "      <td>1.310926</td>\n",
       "      <td>0.944462</td>\n",
       "      <td>1.314942</td>\n",
       "      <td>1.200462</td>\n",
       "      <td>1.219480</td>\n",
       "      <td>0.953552</td>\n",
       "      <td>1.150811</td>\n",
       "      <td>0.928220</td>\n",
       "      <td>1.056390</td>\n",
       "      <td>0.843976</td>\n",
       "      <td>-0.166987</td>\n",
       "      <td>1.022170</td>\n",
       "      <td>1.166871</td>\n",
       "      <td>1.137476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976177</td>\n",
       "      <td>1.129621</td>\n",
       "      <td>1.208627</td>\n",
       "      <td>1.062720</td>\n",
       "      <td>1.269867</td>\n",
       "      <td>1.039596</td>\n",
       "      <td>1.197524</td>\n",
       "      <td>0.879743</td>\n",
       "      <td>1.172203</td>\n",
       "      <td>0.790902</td>\n",
       "      <td>0.747272</td>\n",
       "      <td>1.001915</td>\n",
       "      <td>0.991064</td>\n",
       "      <td>1.215666</td>\n",
       "      <td>1.112823</td>\n",
       "      <td>0.956059</td>\n",
       "      <td>0.008708</td>\n",
       "      <td>1.143529</td>\n",
       "      <td>0.900674</td>\n",
       "      <td>1.387107</td>\n",
       "      <td>1.340876</td>\n",
       "      <td>1.136052</td>\n",
       "      <td>1.200682</td>\n",
       "      <td>1.098196</td>\n",
       "      <td>1.233457</td>\n",
       "      <td>1.530723</td>\n",
       "      <td>1.094137</td>\n",
       "      <td>0.994070</td>\n",
       "      <td>0.821261</td>\n",
       "      <td>0.726283</td>\n",
       "      <td>1.197614</td>\n",
       "      <td>1.140601</td>\n",
       "      <td>0.525599</td>\n",
       "      <td>1.026553</td>\n",
       "      <td>1.113339</td>\n",
       "      <td>1.366259</td>\n",
       "      <td>1.147440</td>\n",
       "      <td>1.110037</td>\n",
       "      <td>1.107421</td>\n",
       "      <td>1.149415</td>\n",
       "      <td>1.293589</td>\n",
       "      <td>1.054019</td>\n",
       "      <td>1.304492</td>\n",
       "      <td>1.696110</td>\n",
       "      <td>1.026975</td>\n",
       "      <td>0.627869</td>\n",
       "      <td>0.987531</td>\n",
       "      <td>0.870218</td>\n",
       "      <td>1.035372</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.058965</td>\n",
       "      <td>2.544616</td>\n",
       "      <td>2.858902</td>\n",
       "      <td>2.998670</td>\n",
       "      <td>3.241791</td>\n",
       "      <td>2.775474</td>\n",
       "      <td>3.488716</td>\n",
       "      <td>2.573477</td>\n",
       "      <td>3.378161</td>\n",
       "      <td>2.027585</td>\n",
       "      <td>2.722684</td>\n",
       "      <td>2.738472</td>\n",
       "      <td>3.371317</td>\n",
       "      <td>3.193845</td>\n",
       "      <td>2.805438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>3.016500</td>\n",
       "      <td>3.235045</td>\n",
       "      <td>3.953457</td>\n",
       "      <td>1.255074</td>\n",
       "      <td>2.829039</td>\n",
       "      <td>3.143655</td>\n",
       "      <td>2.941832</td>\n",
       "      <td>2.673290</td>\n",
       "      <td>2.515048</td>\n",
       "      <td>3.085532</td>\n",
       "      <td>3.332468</td>\n",
       "      <td>3.181993</td>\n",
       "      <td>2.018135</td>\n",
       "      <td>2.945808</td>\n",
       "      <td>3.904138</td>\n",
       "      <td>3.623315</td>\n",
       "      <td>2.737631</td>\n",
       "      <td>2.846201</td>\n",
       "      <td>3.081115</td>\n",
       "      <td>2.864492</td>\n",
       "      <td>3.006279</td>\n",
       "      <td>3.374874</td>\n",
       "      <td>2.746529</td>\n",
       "      <td>3.100904</td>\n",
       "      <td>2.876392</td>\n",
       "      <td>2.644707</td>\n",
       "      <td>2.685909</td>\n",
       "      <td>2.617825</td>\n",
       "      <td>2.506095</td>\n",
       "      <td>0.972523</td>\n",
       "      <td>2.827976</td>\n",
       "      <td>3.341034</td>\n",
       "      <td>3.003345</td>\n",
       "      <td>...</td>\n",
       "      <td>3.121378</td>\n",
       "      <td>2.451609</td>\n",
       "      <td>2.867927</td>\n",
       "      <td>2.945429</td>\n",
       "      <td>3.384986</td>\n",
       "      <td>3.164904</td>\n",
       "      <td>2.863122</td>\n",
       "      <td>2.612441</td>\n",
       "      <td>3.120637</td>\n",
       "      <td>2.520899</td>\n",
       "      <td>2.848479</td>\n",
       "      <td>2.800744</td>\n",
       "      <td>2.666093</td>\n",
       "      <td>2.965629</td>\n",
       "      <td>2.998184</td>\n",
       "      <td>2.819584</td>\n",
       "      <td>1.508360</td>\n",
       "      <td>3.100806</td>\n",
       "      <td>2.817177</td>\n",
       "      <td>3.202041</td>\n",
       "      <td>2.866316</td>\n",
       "      <td>3.209280</td>\n",
       "      <td>2.889581</td>\n",
       "      <td>3.514552</td>\n",
       "      <td>2.836488</td>\n",
       "      <td>2.295757</td>\n",
       "      <td>2.916437</td>\n",
       "      <td>3.205187</td>\n",
       "      <td>3.005812</td>\n",
       "      <td>3.575979</td>\n",
       "      <td>2.869147</td>\n",
       "      <td>2.760525</td>\n",
       "      <td>3.139726</td>\n",
       "      <td>2.831662</td>\n",
       "      <td>2.876452</td>\n",
       "      <td>2.794520</td>\n",
       "      <td>3.009266</td>\n",
       "      <td>2.896961</td>\n",
       "      <td>2.852543</td>\n",
       "      <td>3.470820</td>\n",
       "      <td>3.095699</td>\n",
       "      <td>3.037184</td>\n",
       "      <td>2.592989</td>\n",
       "      <td>3.366128</td>\n",
       "      <td>2.809742</td>\n",
       "      <td>1.250261</td>\n",
       "      <td>3.443677</td>\n",
       "      <td>2.818023</td>\n",
       "      <td>2.787013</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  0.7301891713.csv  0.6750726968.csv  0.7194704070.csv  0.7107007521.csv  \\\n",
       "0   0          0.913897          2.142608          1.502359          1.453620   \n",
       "1   1          0.678169          0.576310          0.298877          0.301982   \n",
       "2   2          1.222971          1.493293          1.997012          1.580462   \n",
       "3   3          0.896871          1.278293          0.973904          1.035144   \n",
       "4   4          3.058965          2.544616          2.858902          2.998670   \n",
       "\n",
       "   0.6952032365.csv  0.7311830751.csv  0.6736005999.csv  0.7205109360.csv  \\\n",
       "0          1.336538          1.599475          1.236069          1.054479   \n",
       "1          0.698756          1.131453          0.569003          0.426954   \n",
       "2          1.181609          1.527942          1.655077          1.549787   \n",
       "3          1.308422          0.783468          1.386619          0.824402   \n",
       "4          3.241791          2.775474          3.488716          2.573477   \n",
       "\n",
       "   0.7298116981.csv  0.6737624943.csv  0.7105097012.csv  0.7163693161.csv  \\\n",
       "0          1.131611          0.984009          1.171605          1.207453   \n",
       "1          0.510710          0.430144          0.221108          0.264951   \n",
       "2          1.668255          1.272369          1.688099          1.579537   \n",
       "3          2.278990          1.024375          0.975785          1.001671   \n",
       "4          3.378161          2.027585          2.722684          2.738472   \n",
       "\n",
       "   0.7225567087.csv  0.6872126167.csv  0.7292947020.csv  0.6798503735.csv  \\\n",
       "0          1.400562          1.156730          1.655767          0.405465   \n",
       "1          1.556663          1.082496          1.407428          1.152680   \n",
       "2         -0.373970          1.167650          1.493627          1.152680   \n",
       "3          0.373937          1.295792          1.057832          0.994623   \n",
       "4          3.371317          3.193845          2.805438          1.386294   \n",
       "\n",
       "   0.7130672940.csv  0.6709092823.csv  0.7391105353.csv  0.6616334858.csv  \\\n",
       "0          1.194848          1.542418          1.921342          0.641228   \n",
       "1          0.920520          0.907372          1.113602          0.179283   \n",
       "2          1.961822          1.120913          1.743965          0.915126   \n",
       "3          1.556343          1.081597          1.754679          0.603690   \n",
       "4          3.016500          3.235045          3.953457          1.255074   \n",
       "\n",
       "   0.6897190987.csv  0.6963558296.csv  0.7170726761.csv  0.7156669797.csv  \\\n",
       "0          1.025477          1.378848          1.257294          1.280277   \n",
       "1          1.125816          0.637407          0.381426          0.376108   \n",
       "2          1.470307          1.403919          1.821767          1.689505   \n",
       "3          0.924175          1.172713          0.905020          1.081385   \n",
       "4          2.829039          3.143655          2.941832          2.673290   \n",
       "\n",
       "   0.6814483512.csv  0.7167410246.csv  0.6981892492.csv  0.7119194656.csv  \\\n",
       "0          1.467210          1.443668          1.391678          1.419787   \n",
       "1          1.140530          0.507529          0.556460          0.232753   \n",
       "2          1.227045          1.376995          1.348107          1.619013   \n",
       "3          0.965547          1.095021          1.106409          1.159985   \n",
       "4          2.515048          3.085532          3.332468          3.181993   \n",
       "\n",
       "   0.6932545022.csv  0.6593521643.csv  0.6743157708.csv  0.7125579799.csv  \\\n",
       "0          1.054552          3.539317          0.983944          1.476214   \n",
       "1          0.004920          0.810242          1.154435          0.464158   \n",
       "2          0.584926          1.492138          1.441973          1.098612   \n",
       "3          0.324042          1.479513          1.223506          0.954404   \n",
       "4          2.018135          2.945808          3.904138          3.623315   \n",
       "\n",
       "   0.7069356577.csv  0.6884713711.csv  0.6702128703.csv  0.6564147356.csv  \\\n",
       "0          1.234424          1.307563          2.385436          2.001724   \n",
       "1          0.571005          0.498050          0.208698          0.693851   \n",
       "2          1.537328          1.360002          1.502614          1.400423   \n",
       "3          1.265987          1.090734          1.270504          1.310926   \n",
       "4          2.737631          2.846201          3.081115          2.864492   \n",
       "\n",
       "   0.6977737011.csv  0.6638768547.csv  0.7032756348.csv  0.6862982027.csv  \\\n",
       "0          1.257358          1.689581          1.290093          1.277571   \n",
       "1          0.753772          0.798899          0.556192          0.626027   \n",
       "2          1.176323          1.449841          1.581607          1.417900   \n",
       "3          0.944462          1.314942          1.200462          1.219480   \n",
       "4          3.006279          3.374874          2.746529          3.100904   \n",
       "\n",
       "   0.6864219834.csv  0.7088904051.csv  0.7212180410.csv  0.7421980731.csv  \\\n",
       "0          1.269417          0.953358          0.838936          1.437067   \n",
       "1          0.599505          0.451303          0.421232          1.578557   \n",
       "2          1.377263          1.590156          1.667259          1.592731   \n",
       "3          0.953552          1.150811          0.928220          1.056390   \n",
       "4          2.876392          2.644707          2.685909          2.617825   \n",
       "\n",
       "   0.6825341422.csv  0.6848737762.csv  0.7144978161.csv  0.6658856476.csv  \\\n",
       "0          1.378712         -0.090514          1.220395          2.258836   \n",
       "1          1.518368          0.012512          0.332251          0.617989   \n",
       "2          1.206471          0.565491          1.875071          1.503783   \n",
       "3          0.843976         -0.166987          1.022170          1.166871   \n",
       "4          2.506095          0.972523          2.827976          3.341034   \n",
       "\n",
       "   0.7195078907.csv  ...  0.6870791547.csv  0.6967096004.csv  \\\n",
       "0          1.228620  ...          1.172375          1.109329   \n",
       "1          0.828814  ...          0.323980          0.483191   \n",
       "2          1.565373  ...          1.275068          1.503897   \n",
       "3          1.137476  ...          0.976177          1.129621   \n",
       "4          3.003345  ...          3.121378          2.451609   \n",
       "\n",
       "   0.7060967827.csv  0.7113625862.csv  0.6694947166.csv  0.6900693308.csv  \\\n",
       "0          1.245783          1.377996          1.435160          1.195812   \n",
       "1          0.284638          0.408454          0.674884          0.355031   \n",
       "2          1.652910          1.629185          1.435720          1.459440   \n",
       "3          1.208627          1.062720          1.269867          1.039596   \n",
       "4          2.867927          2.945429          3.384986          3.164904   \n",
       "\n",
       "   0.7128591265.csv  0.6818718747.csv  0.6858034178.csv  0.7122377012.csv  \\\n",
       "0          1.595924          1.338426          1.170736          1.263989   \n",
       "1          0.396140          1.250862          0.718760          0.147479   \n",
       "2          1.472051          1.205913          1.217617          1.356744   \n",
       "3          1.197524          0.879743          1.172203          0.790902   \n",
       "4          2.863122          2.612441          3.120637          2.520899   \n",
       "\n",
       "   0.7144335192.csv  0.7083170570.csv  0.7142499638.csv  0.7136242881.csv  \\\n",
       "0          0.925031          0.862942          1.198344          1.414648   \n",
       "1          0.554945          0.257595          0.372066          0.811467   \n",
       "2          1.547532          1.491958          1.631098          1.402725   \n",
       "3          0.747272          1.001915          0.991064          1.215666   \n",
       "4          2.848479          2.800744          2.666093          2.965629   \n",
       "\n",
       "   0.6888742488.csv  0.7087145574.csv  0.6566530931.csv  0.7197358544.csv  \\\n",
       "0          1.305720          0.937820          0.674106          1.475712   \n",
       "1          0.535556          0.407907          0.046973          0.404249   \n",
       "2          1.328202          1.506278          0.673713          1.670106   \n",
       "3          1.112823          0.956059          0.008708          1.143529   \n",
       "4          2.998184          2.819584          1.508360          3.100806   \n",
       "\n",
       "   0.7101817035.csv  0.7520219713.csv  0.7215952204.csv  0.6795805487.csv  \\\n",
       "0          1.064896          0.678348          1.330522          1.242018   \n",
       "1          0.377877          0.950909          0.288525          0.595133   \n",
       "2          1.525643          1.863093          1.558563          1.463832   \n",
       "3          0.900674          1.387107          1.340876          1.136052   \n",
       "4          2.817177          3.202041          2.866316          3.209280   \n",
       "\n",
       "   0.7083706040.csv  0.6941218921.csv  0.6926641668.csv  0.6869857003.csv  \\\n",
       "0          1.256842          1.234825          1.252730          1.259111   \n",
       "1          0.411427          0.758979          0.378051          0.539336   \n",
       "2          1.483774          1.248124          1.574611          1.385882   \n",
       "3          1.200682          1.098196          1.233457          1.530723   \n",
       "4          2.889581          3.514552          2.836488          2.295757   \n",
       "\n",
       "   0.7198785912.csv  0.7287761089.csv  0.7035686386.csv  0.7336496744.csv  \\\n",
       "0          1.237227          1.548106          1.496501          1.588877   \n",
       "1          0.649062          0.266136          0.484547          0.557583   \n",
       "2          1.543597          1.735119          1.300939          0.887515   \n",
       "3          1.094137          0.994070          0.821261          0.726283   \n",
       "4          2.916437          3.205187          3.005812          3.575979   \n",
       "\n",
       "   0.6872533788.csv  0.7005346672.csv  0.7369069391.csv  0.7156537235.csv  \\\n",
       "0          1.322937          1.301587          1.143404          1.285821   \n",
       "1          0.529086          1.068429          0.249275          0.325167   \n",
       "2          1.389234          1.073305          1.716729          1.470162   \n",
       "3          1.197614          1.140601          0.525599          1.026553   \n",
       "4          2.869147          2.760525          3.139726          2.831662   \n",
       "\n",
       "   0.7121002711.csv  0.7289228406.csv  0.7174789519.csv  0.7032181627.csv  \\\n",
       "0          1.362650          1.133906          1.368576          1.159616   \n",
       "1          0.454034          0.363870          0.201669          0.191891   \n",
       "2          1.566478          1.463176          1.638943          1.477231   \n",
       "3          1.113339          1.366259          1.147440          1.110037   \n",
       "4          2.876452          2.794520          3.009266          2.896961   \n",
       "\n",
       "   0.6668147232.csv  0.7345858839.csv  0.7056743178.csv  0.6850481924.csv  \\\n",
       "0          2.507343          0.925301          1.451954          1.199074   \n",
       "1          0.913241          0.593457          0.576423          0.604565   \n",
       "2          1.721804          1.291973          1.703252          1.275325   \n",
       "3          1.107421          1.149415          1.293589          1.054019   \n",
       "4          2.852543          3.470820          3.095699          3.037184   \n",
       "\n",
       "   0.7359858591.csv  0.7068431477.csv  0.7121179915.csv  0.6613117872.csv  \\\n",
       "0          1.364975          1.650185          1.184054          0.660613   \n",
       "1          0.851182          0.545539          0.652546          0.147133   \n",
       "2          1.562809          1.723188          1.644262          0.939329   \n",
       "3          1.304492          1.696110          1.026975          0.627869   \n",
       "4          2.592989          3.366128          2.809742          1.250261   \n",
       "\n",
       "   0.6843637618.csv  0.6781392004.csv  0.7222809303.csv  label  \n",
       "0          1.385376          1.596353          1.544845    0.0  \n",
       "1          0.615547          0.412286          0.128416    1.0  \n",
       "2          1.258014          1.263616          1.449718    1.0  \n",
       "3          0.987531          0.870218          1.035372    1.0  \n",
       "4          3.443677          2.818023          2.787013    0.0  \n",
       "\n",
       "[5 rows x 5002 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from lightgbm import LGBMClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'analytics-data-science-competitions'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "file_key_1 = 'Tabular-Playground-Series/Tabular-Playground-Nov-2022/sample_submission.csv'\n",
    "file_key_2 = 'Tabular-Playground-Series/Tabular-Playground-Nov-2022/train_labels.csv'\n",
    "\n",
    "bucket_object_1 = bucket.Object(file_key_1)\n",
    "file_object_1 = bucket_object_1.get()\n",
    "file_content_stream_1 = file_object_1.get('Body')\n",
    "\n",
    "bucket_object_2 = bucket.Object(file_key_2)\n",
    "file_object_2 = bucket_object_2.get()\n",
    "file_content_stream_2 = file_object_2.get('Body')\n",
    "\n",
    "## Reading data-files\n",
    "submission = pd.read_csv(file_content_stream_1)\n",
    "y_true = pd.read_csv(file_content_stream_2)\n",
    "df = pd.read_parquet('s3://analytics-data-science-competitions/Tabular-Playground-Series/Tabular-Playground-Nov-2022/preds_logit_concat_gzip.parquet', engine = 'fastparquet')\n",
    "\n",
    "preds = pd.merge(df, y_true, on = 'id', how = 'left')\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9724cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20314/3277270779.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['label'] = train['label'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "train = preds[preds['label'].notnull()]\n",
    "train['label'] = train['label'].astype(int)\n",
    "\n",
    "test = preds[preds['label'].isnull()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b45c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coef</th>\n",
       "      <th>abs_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6564517315.csv</td>\n",
       "      <td>2.601288</td>\n",
       "      <td>2.601288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7183933428.csv</td>\n",
       "      <td>-2.560396</td>\n",
       "      <td>2.560396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6570599977.csv</td>\n",
       "      <td>2.443219</td>\n",
       "      <td>2.443219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6454375051.csv</td>\n",
       "      <td>2.201914</td>\n",
       "      <td>2.201914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7103145610.csv</td>\n",
       "      <td>2.114975</td>\n",
       "      <td>2.114975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Feature      Coef  abs_coef\n",
       "0  0.6564517315.csv  2.601288  2.601288\n",
       "1  0.7183933428.csv -2.560396  2.560396\n",
       "2  0.6570599977.csv  2.443219  2.443219\n",
       "3  0.6454375051.csv  2.201914  2.201914\n",
       "4  0.7103145610.csv  2.114975  2.114975"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_scores = pd.read_csv('lasso_scores_logit.csv')\n",
    "lasso_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b4ed820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.6564517315.csv', '0.7183933428.csv', '0.6570599977.csv',\n",
       "       '0.6454375051.csv', '0.7103145610.csv', '0.6495843938.csv',\n",
       "       '0.6802400070.csv', '0.7120881745.csv', '0.6708990553.csv',\n",
       "       '0.6911116549.csv'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_select = lasso_scores['Feature'][0:10]\n",
    "to_select.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92f671dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/training.py\", line 816, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 633, in apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/optimizer_v2/utils.py\", line 73, in filter_empty_gradients\n        raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\n    ValueError: No gradients provided for any variable: (['dense_6/kernel:0', 'dense_6/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'dense_8/kernel:0', 'dense_8/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_6/kernel:0' shape=(10, 8) dtype=float32>), (None, <tf.Variable 'dense_6/bias:0' shape=(8,) dtype=float32>), (None, <tf.Variable 'dense_7/kernel:0' shape=(8, 8) dtype=float32>), (None, <tf.Variable 'dense_7/bias:0' shape=(8,) dtype=float32>), (None, <tf.Variable 'dense_8/kernel:0' shape=(8, 2) dtype=float32>), (None, <tf.Variable 'dense_8/bias:0' shape=(2,) dtype=float32>)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20314/1355611536.py\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m## Building model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mlgb_md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/training.py\", line 816, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 633, in apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/optimizer_v2/utils.py\", line 73, in filter_empty_gradients\n        raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\n    ValueError: No gradients provided for any variable: (['dense_6/kernel:0', 'dense_6/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'dense_8/kernel:0', 'dense_8/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_6/kernel:0' shape=(10, 8) dtype=float32>), (None, <tf.Variable 'dense_6/bias:0' shape=(8,) dtype=float32>), (None, <tf.Variable 'dense_7/kernel:0' shape=(8, 8) dtype=float32>), (None, <tf.Variable 'dense_7/bias:0' shape=(8,) dtype=float32>), (None, <tf.Variable 'dense_8/kernel:0' shape=(8, 2) dtype=float32>), (None, <tf.Variable 'dense_8/bias:0' shape=(2,) dtype=float32>)).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = train[to_select.values]\n",
    "Y = train['label']\n",
    "\n",
    "test_new = test[to_select.values]\n",
    "\n",
    "## Defining model \n",
    "lgb_md = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(8, input_dim = 10, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(8, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    "lgb_md.compile(optimizer = 'adam', loss = 'logLoss')\n",
    "\n",
    "## Defining list to store results\n",
    "lgb_results = list()\n",
    "test_preds_lgb_fold_1 = list() \n",
    "test_preds_lgb_fold_2 = list()\n",
    "test_preds_lgb_fold_3 = list()\n",
    "test_preds_lgb_fold_4 = list()\n",
    "test_preds_lgb_fold_5 = list()\n",
    "\n",
    "fold = 1\n",
    "kfold = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "        \n",
    "for train_ix, test_ix in kfold.split(X, Y):\n",
    "    \n",
    "    ## Splitting the data \n",
    "    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "    Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)\n",
    "\n",
    "    ## Building model\n",
    "    lgb_md.fit(X_train, Y_train, verbose = 1, epochs = 30, batch_size = 32, validation_data = (X_test, Y_test))\n",
    "    \n",
    "    \n",
    "#     lgb_md = LGBMClassifier(n_estimators = 1000, \n",
    "#                             learning_rate = 0.01,\n",
    "#                             num_leaves = 50,\n",
    "#                             max_depth = 17, \n",
    "#                             lambda_l1 = 3, \n",
    "#                             lambda_l2 = 1, \n",
    "#                             bagging_fraction = 0.8, \n",
    "#                             feature_fraction = 0.8).fit(X_train, Y_train)\n",
    "        \n",
    "    ## Predicting on test\n",
    "    lgb_pred = lgb_md.predict(X_test)[:, 1]\n",
    "    score = log_loss(Y_test, lgb_pred)\n",
    "    lgb_results.append(score)\n",
    "        \n",
    "    print('Fold ', str(fold), ' result is:', score, '\\n')\n",
    "    \n",
    "    if (fold == 1):\n",
    "        test_preds_lgb_fold_1.append(lgb_md.predict(test_new)[:, 1])\n",
    "        \n",
    "    if (fold == 2):\n",
    "        test_preds_lgb_fold_2.append(lgb_md.predict(test_new)[:, 1])\n",
    "        \n",
    "    if (fold == 3):\n",
    "        test_preds_lgb_fold_3.append(lgb_md.predict(test_new)[:, 1])\n",
    "        \n",
    "    if (fold == 4):\n",
    "        test_preds_lgb_fold_4.append(lgb_md.predict(test_new)[:, 1])\n",
    "        \n",
    "    if (fold == 5):\n",
    "        test_preds_lgb_fold_5.append(lgb_md.predict(test_new)[:, 1])\n",
    "    \n",
    "    fold +=1\n",
    "\n",
    "print('The average log-loss over 5-fold CV is', np.mean(lgb_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
