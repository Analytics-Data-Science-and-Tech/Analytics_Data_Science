{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3091cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost lightgbm catboost optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b87dd046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor \n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "import optuna \n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'analytics-data-science-competitions'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "file_key_1 = 'Tabular-Playground-Series/PS-S3/Ep9/train.csv'\n",
    "file_key_2 = 'Tabular-Playground-Series/PS-S3/Ep9/test.csv'\n",
    "file_key_3 = 'Tabular-Playground-Series/PS-S3/Ep9/sample_submission.csv'\n",
    "\n",
    "bucket_object_1 = bucket.Object(file_key_1)\n",
    "file_object_1 = bucket_object_1.get()\n",
    "file_content_stream_1 = file_object_1.get('Body')\n",
    "\n",
    "bucket_object_2 = bucket.Object(file_key_2)\n",
    "file_object_2 = bucket_object_2.get()\n",
    "file_content_stream_2 = file_object_2.get('Body')\n",
    "\n",
    "bucket_object_3 = bucket.Object(file_key_3)\n",
    "file_object_3 = bucket_object_3.get()\n",
    "file_content_stream_3 = file_object_3.get('Body')\n",
    "\n",
    "## Reading data files\n",
    "train = pd.read_csv(file_content_stream_1)\n",
    "test = pd.read_csv(file_content_stream_2)\n",
    "submission = pd.read_csv(file_content_stream_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133de23",
   "metadata": {},
   "source": [
    "# Baseline Modeling 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ca565",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns = ['id', 'Strength'], axis = 1)\n",
    "Y = train['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "XGB_cv_scores, XGB_imp = list(), list()\n",
    "XGB_preds = list()\n",
    "\n",
    "lgb_cv_scores, lgb_imp = list(), list()\n",
    "lgb_preds = list()\n",
    "\n",
    "cat_cv_scores, cat_imp = list(), list()\n",
    "cat_preds = list()\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.7, \n",
    "                              gamma = 0.8, \n",
    "                              learning_rate = 0.01, \n",
    "                              max_depth = 7, \n",
    "                              min_child_weight = 10, \n",
    "                              n_estimators = 1000, \n",
    "                              subsample = 0.7).fit(X_train, Y_train)\n",
    "        XGB_imp.append(XGB_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        XGB_cv_scores.append(mean_squared_error(Y_test, XGB_pred_1, squared = False))\n",
    "        XGB_preds.append(XGB_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 1000,\n",
    "                               max_depth = 7,\n",
    "                               learning_rate = 0.01,\n",
    "                               num_leaves = 20,\n",
    "                               lambda_l1 = 3,\n",
    "                               lambda_l2 = 3,\n",
    "                               bagging_fraction = 0.7,\n",
    "                               feature_fraction = 0.7).fit(X_train, Y_train)\n",
    "        lgb_imp.append(lgb_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        lgb_cv_scores.append(mean_squared_error(Y_test, lgb_pred_1, squared = False))\n",
    "        lgb_preds.append(lgb_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 1000,\n",
    "                                   learning_rate = 0.01,\n",
    "                                   depth = 7,\n",
    "                                   random_strength = 0.5,\n",
    "                                   bagging_temperature = 0.7,\n",
    "                                   border_count = 30,\n",
    "                                   l2_leaf_reg = 5,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        cat_imp.append(cat_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        cat_cv_scores.append(mean_squared_error(Y_test, cat_pred_1, squared = False))\n",
    "        cat_preds.append(cat_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2916ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_cv_score = np.mean(XGB_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the XGBoost model is:', XGB_cv_score)\n",
    "\n",
    "lgb_cv_score = np.mean(lgb_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the LightGBM model is:', lgb_cv_score)\n",
    "\n",
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preds_test = pd.DataFrame(cat_preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = cat_preds_test\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('catboost_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52489f14",
   "metadata": {},
   "source": [
    "# Baseline Modeling 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97726c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].mean()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "XGB_cv_scores, XGB_imp = list(), list()\n",
    "XGB_preds = list()\n",
    "\n",
    "lgb_cv_scores, lgb_imp = list(), list()\n",
    "lgb_preds = list()\n",
    "\n",
    "cat_cv_scores, cat_imp = list(), list()\n",
    "cat_preds = list()\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.7, \n",
    "                              gamma = 0.8, \n",
    "                              learning_rate = 0.01, \n",
    "                              max_depth = 7, \n",
    "                              min_child_weight = 10, \n",
    "                              n_estimators = 1000, \n",
    "                              subsample = 0.7).fit(X_train, Y_train)\n",
    "        XGB_imp.append(XGB_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        XGB_cv_scores.append(mean_squared_error(Y_test, XGB_pred_1, squared = False))\n",
    "        XGB_preds.append(XGB_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 1000,\n",
    "                               max_depth = 7,\n",
    "                               learning_rate = 0.01,\n",
    "                               num_leaves = 20,\n",
    "                               lambda_l1 = 3,\n",
    "                               lambda_l2 = 3,\n",
    "                               bagging_fraction = 0.7,\n",
    "                               feature_fraction = 0.7).fit(X_train, Y_train)\n",
    "        lgb_imp.append(lgb_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        lgb_cv_scores.append(mean_squared_error(Y_test, lgb_pred_1, squared = False))\n",
    "        lgb_preds.append(lgb_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 1000,\n",
    "                                   learning_rate = 0.01,\n",
    "                                   depth = 7,\n",
    "                                   random_strength = 0.5,\n",
    "                                   bagging_temperature = 0.7,\n",
    "                                   border_count = 30,\n",
    "                                   l2_leaf_reg = 5,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        cat_imp.append(cat_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        cat_cv_scores.append(mean_squared_error(Y_test, cat_pred_1, squared = False))\n",
    "        cat_preds.append(cat_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb849316",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_cv_score = np.mean(XGB_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the XGBoost model is:', XGB_cv_score)\n",
    "\n",
    "lgb_cv_score = np.mean(lgb_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the LightGBM model is:', lgb_cv_score)\n",
    "\n",
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b57e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preds_test = pd.DataFrame(cat_preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = cat_preds_test\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d7eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('catboost_submission_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a740b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].median()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "XGB_cv_scores, XGB_imp = list(), list()\n",
    "XGB_preds = list()\n",
    "\n",
    "lgb_cv_scores, lgb_imp = list(), list()\n",
    "lgb_preds = list()\n",
    "\n",
    "cat_cv_scores, cat_imp = list(), list()\n",
    "cat_preds = list()\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "#         XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "#                               colsample_bytree = 0.7, \n",
    "#                               gamma = 0.8, \n",
    "#                               learning_rate = 0.01, \n",
    "#                               max_depth = 7, \n",
    "#                               min_child_weight = 10, \n",
    "#                               n_estimators = 1000, \n",
    "#                               subsample = 0.7).fit(X_train, Y_train)\n",
    "#         XGB_imp.append(XGB_md.feature_importances_)\n",
    "        \n",
    "#         ## Predicting on X_test and test\n",
    "#         XGB_pred_1 = XGB_md.predict(X_test)\n",
    "#         XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        \n",
    "#         ## Computing rmse\n",
    "#         XGB_cv_scores.append(mean_squared_error(Y_test, XGB_pred_1, squared = False))\n",
    "#         XGB_preds.append(XGB_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "#         lgb_md = LGBMRegressor(n_estimators = 1000,\n",
    "#                                max_depth = 7,\n",
    "#                                learning_rate = 0.01,\n",
    "#                                num_leaves = 20,\n",
    "#                                lambda_l1 = 3,\n",
    "#                                lambda_l2 = 3,\n",
    "#                                bagging_fraction = 0.7,\n",
    "#                                feature_fraction = 0.7).fit(X_train, Y_train)\n",
    "#         lgb_imp.append(lgb_md.feature_importances_)\n",
    "        \n",
    "#         ## Predicting on X_test and test\n",
    "#         lgb_pred_1 = lgb_md.predict(X_test)\n",
    "#         lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        \n",
    "#         ## Computing rmse\n",
    "#         lgb_cv_scores.append(mean_squared_error(Y_test, lgb_pred_1, squared = False))\n",
    "#         lgb_preds.append(lgb_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 4738,\n",
    "                                   learning_rate = 0.003143666241424718,\n",
    "                                   depth = 4,\n",
    "                                   random_strength = 0.29823973415192867,\n",
    "                                   bagging_temperature = 0.3408793603898661,\n",
    "                                   border_count = 112,\n",
    "                                   l2_leaf_reg = 17,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        cat_imp.append(cat_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        cat_cv_scores.append(mean_squared_error(Y_test, cat_pred_1, squared = False))\n",
    "        cat_preds.append(cat_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52da9007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is: 11.880628569091348\n"
     ]
    }
   ],
   "source": [
    "# XGB_cv_score = np.mean(XGB_cv_scores)    \n",
    "# print('The average oof rmse score over 5-folds (run 5 times) of the XGBoost model is:', XGB_cv_score)\n",
    "\n",
    "# lgb_cv_score = np.mean(lgb_cv_scores)    \n",
    "# print('The average oof rmse score over 5-folds (run 5 times) of the LightGBM model is:', lgb_cv_score)\n",
    "\n",
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a949d176",
   "metadata": {},
   "source": [
    "# Baseline 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5524c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedEnsemble(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _rmse_loss(self, coef, X, y):\n",
    "        \n",
    "        ens = coef[0]*X[:, 0] + coef[1]*X[:, 1] + coef[2]*X[:, 2]\n",
    "        ll = mean_squared_error(y, ens, squared = False)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._rmse_loss, X = X, y = y)\n",
    "        initial_coef = [1/3, 1/3, 1/3]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \n",
    "        ens = coef[0]*X[:, 0] + coef[1]*X[:, 1] + coef[2]*X[:, 2]\n",
    "        return ens\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7c74a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:05<08:21, 125.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [03:40<05:22, 107.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [04:11<02:25, 72.63s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [04:41<00:55, 55.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:14<00:00, 62.98s/it]\n"
     ]
    }
   ],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].median()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "ens_cv_scores, preds = list(), list()\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.671460244215802, \n",
    "                              gamma = 2.5281806276307384, \n",
    "                              learning_rate = 0.002046162779305807, \n",
    "                              max_depth = 8, \n",
    "                              min_child_weight = 80, \n",
    "                              n_estimators = 2690, \n",
    "                              subsample = 0.44886485549735244).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 5420,\n",
    "                               max_depth = 3,\n",
    "                               learning_rate = 0.0014779400349972686,\n",
    "                               num_leaves = 61,\n",
    "                               lambda_l1 = 7.384172796287736,\n",
    "                               lambda_l2 = 0.10456555506292783,\n",
    "                               bagging_fraction = 0.22841166601766863,\n",
    "                               feature_fraction = 0.659898030).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 4738,\n",
    "                                   learning_rate = 0.003143666241424718,\n",
    "                                   depth = 4,\n",
    "                                   random_strength = 0.29823973415192867,\n",
    "                                   bagging_temperature = 0.3408793603898661,\n",
    "                                   border_count = 112,\n",
    "                                   l2_leaf_reg = 17,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        \n",
    "        ######################\n",
    "        ## Optimal Ensemble ##\n",
    "        ######################\n",
    "        \n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        models_pred_oof = np.transpose((XGB_pred_1, lgb_pred_1, cat_pred_1))\n",
    "\n",
    "        opt_ens = OptimizedEnsemble()\n",
    "        opt_ens.fit(models_pred_oof, Y_test)\n",
    "        coef = opt_ens.coefficients()\n",
    "        \n",
    "        ens_pred = opt_ens.predict(models_pred_oof, coef)\n",
    "        ens_cv_scores.append(mean_squared_error(Y_test, ens_pred, squared = False))\n",
    "        \n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        models_pred = np.transpose((XGB_pred_2, lgb_pred_2, cat_pred_2))\n",
    "        \n",
    "        ens_preds = opt_ens.predict(models_pred, coef)\n",
    "        preds.append(ens_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74e2b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the ensemble model is: 11.835470374627613\n"
     ]
    }
   ],
   "source": [
    "ens_cv_score = np.mean(ens_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the ensemble model is:', ens_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958871c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_preds_test = pd.DataFrame(preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = ens_preds_test\n",
    "submission.to_csv('Ensemble_Optuna_baseline_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85f91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb12d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ec8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].median()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "ens_cv_scores, preds = list(), list()\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "#         #############    \n",
    "#         ## XGBoost ##\n",
    "#         #############\n",
    "        \n",
    "#         XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "#                               colsample_bytree = 0.671460244215802, \n",
    "#                               gamma = 2.5281806276307384, \n",
    "#                               learning_rate = 0.002046162779305807, \n",
    "#                               max_depth = 8, \n",
    "#                               min_child_weight = 80, \n",
    "#                               n_estimators = 2690, \n",
    "#                               subsample = 0.44886485549735244).fit(X_train, Y_train)\n",
    "        \n",
    "#         ##############\n",
    "#         ## LightGBM ##\n",
    "#         ##############\n",
    "        \n",
    "#         lgb_md = LGBMRegressor(n_estimators = 5420,\n",
    "#                                max_depth = 3,\n",
    "#                                learning_rate = 0.0014779400349972686,\n",
    "#                                num_leaves = 61,\n",
    "#                                lambda_l1 = 7.384172796287736,\n",
    "#                                lambda_l2 = 0.10456555506292783,\n",
    "#                                bagging_fraction = 0.22841166601766863,\n",
    "#                                feature_fraction = 0.659898030).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 4738,\n",
    "                                   learning_rate = 0.003143666241424718,\n",
    "                                   depth = 4,\n",
    "                                   random_strength = 0.29823973415192867,\n",
    "                                   bagging_temperature = 0.3408793603898661,\n",
    "                                   border_count = 112,\n",
    "                                   l2_leaf_reg = 17,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        \n",
    "        ######################\n",
    "        ## Optimal Ensemble ##\n",
    "        ######################\n",
    "        \n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        models_pred_oof = np.transpose((XGB_pred_1, lgb_pred_1, cat_pred_1))\n",
    "\n",
    "        opt_ens = OptimizedEnsemble()\n",
    "        opt_ens.fit(models_pred_oof, Y_test)\n",
    "        coef = opt_ens.coefficients()\n",
    "        \n",
    "        ens_pred = opt_ens.predict(models_pred_oof, coef)\n",
    "        ens_cv_scores.append(mean_squared_error(Y_test, ens_pred, squared = False))\n",
    "        \n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        models_pred = np.transpose((XGB_pred_2, lgb_pred_2, cat_pred_2))\n",
    "        \n",
    "        ens_preds = opt_ens.predict(models_pred, coef)\n",
    "        preds.append(ens_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
