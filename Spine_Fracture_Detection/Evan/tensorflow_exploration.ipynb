{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53832c01",
   "metadata": {},
   "source": [
    "## Learning from Kaggle Notebook\n",
    "\n",
    "### Using tensorflow for image recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ../input/for-pydicom/pylibjpeg-1.4.0-py3-none-any.whl\n",
    "!pip install -q ../input/for-pydicom/python_gdcm-3.0.14-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q ../input/for-pydicom/pylibjpeg_libjpeg-1.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70622097",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pylibjpeg pylibjpeg-libjpeg pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f979cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pathlib\n",
    "import glob \n",
    "from tqdm import tqdm \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pydicom\n",
    "from pydicom import dcmread\n",
    "import pylibjpeg\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34383a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'evan-callaghan-bucket'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "file_key = 'Kaggle-Spine-Fracture-Detection/train.csv'\n",
    "file_key2 = 'Kaggle-Spine-Fracture-Detection/train_bounding_boxes.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "bucket_object2 = bucket.Object(file_key2)\n",
    "\n",
    "file_object = bucket_object.get()\n",
    "file_object2 = bucket_object2.get()\n",
    "\n",
    "file_content_stream = file_object.get('Body')\n",
    "file_content_stream2 = file_object2.get('Body')\n",
    "\n",
    "## Reading the data\n",
    "train = pd.read_csv(file_content_stream)\n",
    "train_boxes = pd.read_csv(file_content_stream2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923edc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "## Defining parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = (512, 512)\n",
    "SEED = 42\n",
    "\n",
    "## Setting seed\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96baa118",
   "metadata": {},
   "source": [
    "## Loading the DICOM files properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subsetting the train data-frame\n",
    "train = train[np.isin(train['StudyInstanceUID'], ['1.2.826.0.1.3680043.10606', '1.2.826.0.1.3680043.10815', \n",
    "                                                  '1.2.826.0.1.3680043.12121'])].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining an empty list to store all images\n",
    "img_list = []\n",
    "\n",
    "## Looping through all study instances in example\n",
    "for instance in train['StudyInstanceUID'].unique():\n",
    "    \n",
    "    ## Getting all images for the particular instance\n",
    "    img_to_append = glob.glob(f'Sample_Images/{instance}/*.dcm')\n",
    "    \n",
    "    ## Appending all images to img_list\n",
    "    img_list.extend(img_to_append)\n",
    "    \n",
    "## Printing the length of the list\n",
    "print(len(img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom(path):\n",
    "    \"\"\"\n",
    "    reads a dicom file and loads the image array inside it\n",
    "    inputs:\n",
    "        path - the path of the required dicom file\n",
    "    returns:\n",
    "        data - image pixel arrays\n",
    "    \"\"\"\n",
    "    img = pydicom.dcmread(path)\n",
    "    data = img.pixel_array\n",
    "    data = data - np.min(data)\n",
    "    if np.max(data) != 0:\n",
    "        data = data / np.max(data)\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator():\n",
    "    \"\"\"\n",
    "    a function that will load the dataset from a list of image paths\n",
    "    \"\"\"\n",
    "    for path in img_list:\n",
    "        data = load_dicom(path)\n",
    "        yield data  # return the data has generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a tensorflow dataset variable that will use the generator to load the image data\n",
    "train_dataset = tf.data.Dataset.from_generator(data_generator, (tf.uint8))\n",
    "\n",
    "## Looking of the dataset contents\n",
    "for i in train_dataset.take(1):\n",
    "    print(i.shape)\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e415fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_single(img, cmap=\"inferno\"):\n",
    "    \"\"\"\n",
    "    plots a single image\n",
    "    \"\"\"\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "show_single(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74fcfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(cmap=\"inferno\"):\n",
    "    \"\"\"\n",
    "    visualizes a batch of images\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    for i, img in enumerate(train_dataset.take(10)):  # iterate through the dataset\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        show_single(img, cmap=cmap)\n",
    "    plt.show()\n",
    "    \n",
    "show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c420d684",
   "metadata": {},
   "source": [
    "## Generating data from the DICOM files to be used for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator():\n",
    "    \n",
    "    ## Looping through all unique StudyInstanceUID values\n",
    "    for i, study_instance in enumerate(train['StudyInstanceUID'].unique()):\n",
    "        \n",
    "        ## Loopig through all images in the associated instance folder\n",
    "        for dcm in os.listdir(f'Sample_Images/{study_instance}'):\n",
    "            \n",
    "            ## Defining an empty list to eventually store train labels\n",
    "            train_labels = []\n",
    "            \n",
    "            ## Defining the image path\n",
    "            path = f'Sample_Images/{study_instance}/{dcm}'\n",
    "            \n",
    "            ## Loading image from path\n",
    "            img = load_dicom(path)\n",
    "            \n",
    "            ## Resizing each image into a shape of (512, 512) and then normalizing\n",
    "            img = np.resize(img, (512, 512))\n",
    "            img = img / 255.0\n",
    "            \n",
    "            ## Converting from gray scale to rgb\n",
    "            img = tf.expand_dims(img, axis=-1)\n",
    "            img = tf.image.grayscale_to_rgb(img)\n",
    "            \n",
    "            ## Appending train labels to each image\n",
    "            train_labels.extend([\n",
    "                train.loc[i, \"C1\"],\n",
    "                train.loc[i, \"C2\"],\n",
    "                train.loc[i, \"C3\"],\n",
    "                train.loc[i, \"C4\"],\n",
    "                train.loc[i, \"C5\"],\n",
    "                train.loc[i, \"C6\"],\n",
    "                train.loc[i, \"C7\"],\n",
    "                train.loc[i, \"patient_overall\"]])\n",
    "            \n",
    "            ## Yielding the image and associated labels\n",
    "            yield img, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea317c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_generator(data_generator, (tf.float32, tf.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in train_data.take(1):\n",
    "    print(img.shape)\n",
    "    print(label.shape)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ad4c7",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counting the number of total images\n",
    "img_count = 0\n",
    "for _, _ in enumerate(train['StudyInstanceUID'].unique()):\n",
    "    for _ in os.listdir(f'Sample_Images/{_}'):\n",
    "        img_count += 1\n",
    "print(img_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f640bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the data into train and validation sets\n",
    "val_size = int(img_count * 0.2)\n",
    "train_data = train_data.skip(val_size)\n",
    "val_data = train_data.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance(data):\n",
    "    data = data.cache()\n",
    "    data = data.batch(16)\n",
    "    data = data.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1becd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = configure_for_performance(train_data)\n",
    "val_data = configure_for_performance(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad424320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e46ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Alex Net model\n",
    "\n",
    "def alex_net():\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(filters=96, input_shape=(512,512,3), kernel_size=(11,11),\\\n",
    "     strides=(4,4), padding='valid', activation=\"relu\"))\n",
    "    # Pooling \n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation before passing it to the next layer\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid', activation=\"relu\"))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid', activation=\"relu\"))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid', activation=\"relu\"))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid', activation=\"relu\"))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Passing it to a dense layer\n",
    "    model.add(Flatten())\n",
    "    # 1st Dense Layer\n",
    "    model.add(Dense(4096, input_shape=(512*512*3,), activation=\"relu\"))\n",
    "    # Add Dropout to prevent overfitting\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 2nd Dense Layer\n",
    "    model.add(Dense(4096, activation=\"relu\"))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 3rd Dense Layer\n",
    "    model.add(Dense(1000, activation=\"relu\"))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Output Layer with 8 probability classes\n",
    "    model.add(Dense(8, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calling the Alex Net model\n",
    "model = alex_net()\n",
    "\n",
    "## Printing model results\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec1121",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad63248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "history = model.fit(train_data, validation_data=val_data,\n",
    "                   epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training \n",
    "def viz_loss(history):\n",
    "    train_loss = history[\"loss\"]\n",
    "    val_loss = history[\"val_loss\"]\n",
    "    iters = [i for i in range(EPOCHS)]\n",
    "    \n",
    "    plt.plot(iters, train_loss, label=\"Training Loss\")\n",
    "    plt.plot(iters, val_loss, label=\"Validation Loss\")\n",
    "    plt.title(\"A plot of Loss against number of iterations\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def viz_acc(history):\n",
    "    train_loss = history[\"categorical_accuracy\"]\n",
    "    val_loss = history[\"val_categorical_accuracy\"]\n",
    "    iters = [i for i in range(EPOCHS)]\n",
    "    \n",
    "    plt.plot(iters, train_loss, label=\"Training Accuracy\")\n",
    "    plt.plot(iters, val_loss, label=\"Validation Accuracy\")\n",
    "    plt.title(\"A plot of Accuracy against number of iterations\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "viz_loss(history.history)\n",
    "viz_acc(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1947cd45",
   "metadata": {},
   "source": [
    "## More Tensorflow for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8557984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a71682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a553526",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing a count of total images in the data set\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ab569",
   "metadata": {},
   "outputs": [],
   "source": [
    "roses = list(data_dir.glob('roses/*'))\n",
    "PIL.Image.open(str(roses[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tulips = list(data_dir.glob('tulips/*'))\n",
    "PIL.Image.open(str(tulips[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting some parameters\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2daec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining training and validation sets\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory( data_dir, validation_split=0.2, subset=\"training\", seed=123, \n",
    "                                                       image_size=(img_height, img_width), batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory( data_dir, validation_split=0.2, subset=\"validation\", \n",
    "                                                     seed=123, image_size=(img_height, img_width), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12136b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the classes\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156434df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a visualization\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuring data sets for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizing the RBG values for each image\n",
    "\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a basic Keras model\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)), \n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'), \n",
    "    layers.MaxPooling2D((2,2), padding='same'), \n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'), \n",
    "    layers.MaxPooling2D((2,2), padding='same'),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'), \n",
    "    layers.MaxPooling2D((2,2), padding='same'), \n",
    "    layers.Flatten(), \n",
    "    layers.Dense(128, activation='relu'), \n",
    "    layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8948ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compiling the model\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b09eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the model\n",
    "\n",
    "epochs=10\n",
    "history = model.fit(train_ds, validation_data = val_ds, epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93467438",
   "metadata": {},
   "source": [
    "## Tensorflow practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "248ba264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1f385b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL TRAINING:\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4983 - accuracy: 0.8257\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3770 - accuracy: 0.8629\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3375 - accuracy: 0.8774\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3138 - accuracy: 0.8857\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2959 - accuracy: 0.8914\n",
      "\n",
      "MODEL EVALUATION:\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8668\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation = tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Setup training parameters\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print(f'\\nMODEL TRAINING:')\n",
    "model.fit(training_images, training_labels, epochs = 5)\n",
    "\n",
    "# Evaluate on the test set\n",
    "print(f'\\nMODEL EVALUATION:')\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79bfb043",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One of the dimensions in the output is <= 0 due to downsampling in conv2d_24. Consider increasing the input size. Received input shape [None, 28, 28, 1] which would produce output shape with a zero or negative value in a dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_612/1960416118.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = tf.keras.models.Sequential([\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Add convolutions and max pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m    303\u001b[0m           \u001b[0;34mf'One of the dimensions in the output is <= 0 '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m           \u001b[0;34mf'due to downsampling in {self.name}. Consider '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv2d_24. Consider increasing the input size. Received input shape [None, 28, 28, 1] which would produce output shape with a zero or negative value in a dimension."
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "                                                         \n",
    "  # Add convolutions and max pooling\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation = 'relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "  # Add the same layers as before\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "  tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Use same settings\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print(f'\\nMODEL TRAINING:')\n",
    "model.fit(training_images, training_labels, epochs = 5)\n",
    "\n",
    "# Evaluate on the test set\n",
    "print(f'\\nMODEL EVALUATION:')\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a84db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bf1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab5f75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "                                                          \n",
    "    # Add convolutions and max pooling \n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation = 'relu'), \n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2, 2)), \n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation = 'relu'), \n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2, 2)),\n",
    " \n",
    "    # Add the same layers as before \n",
    "    tf.keras.layers.Flatten(), \n",
    "    tf.keras.layers.Dense(128, activation = 'relu'), \n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9785d847",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv2d_22\" is incompatible with the layer: expected axis -3of input shape to have value 28, but received input with shape (32, 32, 28, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_612/3262948624.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Print the model summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    438\u001b[0m               'method accepts an `inputs` argument.')\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m           raise ValueError('You cannot build your model by calling `build` '\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    245\u001b[0m           \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshape_as_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m           raise ValueError(\n\u001b[0m\u001b[1;32m    248\u001b[0m               \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m               \u001b[0;34mf'incompatible with the layer: expected axis {axis}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"conv2d_22\" is incompatible with the layer: expected axis -3of input shape to have value 28, but received input with shape (32, 32, 28, 1)"
     ]
    }
   ],
   "source": [
    "model.build(input_shape = (32, 32, 28, 1))\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Use same settings\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print(f'\\nMODEL TRAINING:')\n",
    "model.fit(training_images, training_labels, epochs = 5)\n",
    "\n",
    "# Evaluate on the test set\n",
    "print(f'\\nMODEL EVALUATION:')\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b8bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
