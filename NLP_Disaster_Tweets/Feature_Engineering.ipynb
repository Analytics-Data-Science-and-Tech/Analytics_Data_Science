{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tidytext textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18461109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370aecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_columns', 200)\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Help_Funs import count_chars, count_words, count_capital_chars, count_capital_words, count_sent, count_unique_words, count_stopwords, count_hashtags \n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'analytics-data-science-competitions'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "## Defining files names\n",
    "file_key_1 = 'NLP-Disaster-Tweets/train.csv'\n",
    "file_key_2 = 'NLP-Disaster-Tweets/test.csv'\n",
    "file_key_3 = 'NLP-Disaster-Tweets/sample_submission.csv'\n",
    "\n",
    "bucket_object_1 = bucket.Object(file_key_1)\n",
    "file_object_1 = bucket_object_1.get()\n",
    "file_content_stream_1 = file_object_1.get('Body')\n",
    "\n",
    "bucket_object_2 = bucket.Object(file_key_2)\n",
    "file_object_2 = bucket_object_2.get()\n",
    "file_content_stream_2 = file_object_2.get('Body')\n",
    "\n",
    "bucket_object_3 = bucket.Object(file_key_3)\n",
    "file_object_3 = bucket_object_3.get()\n",
    "file_content_stream_3 = file_object_3.get('Body')\n",
    "\n",
    "## Reading data-files\n",
    "train = pd.read_csv(file_content_stream_1)\n",
    "test = pd.read_csv(file_content_stream_2)\n",
    "sample = pd.read_csv(file_content_stream_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd899578",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "others = set([\"1\", \"2\", \"it'll\", \"ill\", \"=\", '+', \"'s'\", '\"'])\n",
    "stop_words = stop_words.union(others)\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    if type(tweet) == np.float:\n",
    "        return \"\"\n",
    "    temp = tweet.lower()\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(r'http\\S+', '', temp)\n",
    "    temp = re.sub('[()!?]', ' ', temp)\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "    temp = temp.split()\n",
    "    temp = [w for w in temp if not w in stop_words]\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0aaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_tweet'] = np.nan\n",
    "\n",
    "for i in tqdm(range(0, train.shape[0])):\n",
    "    \n",
    "    train['clean_tweet'][i] =  clean_tweet(train['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['clean_tweet'] = np.nan\n",
    "\n",
    "for i in tqdm(range(0, test.shape[0])):\n",
    "    \n",
    "    test['clean_tweet'][i] =  clean_tweet(test['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a96654",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentiment'] = train['clean_tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "train['subjectivity'] = train['clean_tweet'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "test['sentiment'] = test['clean_tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "test['subjectivity'] = test['clean_tweet'].apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentiment_label'] = np.where(train['sentiment'] < 0, 'negative', \n",
    "                                    np.where(train['sentiment'] == 0, 'neutral', 'positive'))\n",
    "\n",
    "test['sentiment_label'] = np.where(test['sentiment'] < 0, 'negative', \n",
    "                                   np.where(test['sentiment'] == 0, 'neutral', 'positive'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb18882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dummies = pd.get_dummies(train['sentiment_label'])\n",
    "train = pd.concat([train.drop(columns = ['sentiment', 'sentiment_label'], axis = 1), train_dummies], axis = 1)\n",
    "\n",
    "test_dummies = pd.get_dummies(test['sentiment_label'])\n",
    "test = pd.concat([test.drop(columns = ['sentiment', 'sentiment_label'], axis = 1), test_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e198f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['char_count'] = train['clean_tweet'].apply(lambda x: count_chars(x))\n",
    "train['word_count'] = train['clean_tweet'].apply(lambda x: count_words(x))\n",
    "train['unique_word_count'] = train['clean_tweet'].apply(lambda x: count_unique_words(x))\n",
    "                                                 \n",
    "test['char_count'] = test['clean_tweet'].apply(lambda x: count_chars(x))\n",
    "test['word_count'] = test['clean_tweet'].apply(lambda x: count_words(x))\n",
    "test['unique_word_count'] = test['clean_tweet'].apply(lambda x: count_unique_words(x))\n",
    "                                                 \n",
    "## Average word length\n",
    "train['avg_wordlength'] = train['char_count'] / train['word_count']\n",
    "test['avg_wordlength'] = test['char_count'] / test['word_count']\n",
    "\n",
    "## Unique words vs count words\n",
    "train['unique_vs_words'] = train['unique_word_count'] / train['word_count']\n",
    "test['unique_vs_words'] = test['unique_word_count'] / test['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f909352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', index = False)\n",
    "test.to_csv('test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'].value_counts() / train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec96591",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train['target'], train['sentiment_label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
