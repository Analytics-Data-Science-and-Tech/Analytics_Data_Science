{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd895625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (458 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.0/459.0 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from wordcloud) (3.5.0)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib->wordcloud) (4.28.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib->wordcloud) (3.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.2.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79d846c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>country</th>\n",
       "      <th>Trump_flag</th>\n",
       "      <th>jfrketich_flag</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>avg_wordlength</th>\n",
       "      <th>unique_vs_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember the #WuhanCoronaVirus? The pandemic w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>WuhanCoronaVirus KillerCuomo</td>\n",
       "      <td>us</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>remember pandemic great percentage deaths resu...</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>156</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>7.428571</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My sources @WhiteHouse say 2 tactics will be u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump</td>\n",
       "      <td>us</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sources say tactics used get america open amp ...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>172</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>6.142857</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll venture a wild guess: If you were running...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>us</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>venture wild guess running usa crisis youd wan...</td>\n",
       "      <td>0.22500</td>\n",
       "      <td>175</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>6.034483</td>\n",
       "      <td>0.793103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Pakistan (#GreenStimulus = #Nature protection...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pakistan GreenStimulus Nature Green</td>\n",
       "      <td>us</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>protection jobs community youth within recentl...</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>87</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>🇺🇸 Pandémie de #coronavirus: 30 pasteurs améri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus COVID__19 COVIDー19</td>\n",
       "      <td>us</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pand mie de 30 pasteurs ricains qui avaient mi...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>201</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>6.483871</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text reply_to_screen_name  \\\n",
       "0  Remember the #WuhanCoronaVirus? The pandemic w...                  NaN   \n",
       "1  My sources @WhiteHouse say 2 tactics will be u...                  NaN   \n",
       "2  I'll venture a wild guess: If you were running...                  NaN   \n",
       "3  #Pakistan (#GreenStimulus = #Nature protection...                  NaN   \n",
       "4  🇺🇸 Pandémie de #coronavirus: 30 pasteurs améri...                  NaN   \n",
       "\n",
       "   is_quote  is_retweet                             hashtags country  \\\n",
       "0         1           1         WuhanCoronaVirus KillerCuomo      us   \n",
       "1         1           1                                Trump      us   \n",
       "2         1           1                              COVID19      us   \n",
       "3         1           1  Pakistan GreenStimulus Nature Green      us   \n",
       "4         1           1       coronavirus COVID__19 COVIDー19      us   \n",
       "\n",
       "   Trump_flag  jfrketich_flag  \\\n",
       "0           0               0   \n",
       "1           0               0   \n",
       "2           0               0   \n",
       "3           0               0   \n",
       "4           0               0   \n",
       "\n",
       "                                         clean_tweet  sentiment  char_count  \\\n",
       "0  remember pandemic great percentage deaths resu...    0.20000         156   \n",
       "1  sources say tactics used get america open amp ...    0.00000         172   \n",
       "2  venture wild guess running usa crisis youd wan...    0.22500         175   \n",
       "3  protection jobs community youth within recentl...    0.03125          87   \n",
       "4  pand mie de 30 pasteurs ricains qui avaient mi...    0.00000         201   \n",
       "\n",
       "   word_count  unique_word_count  avg_wordlength  unique_vs_words  \n",
       "0          21                 21        7.428571         1.000000  \n",
       "1          28                 25        6.142857         0.892857  \n",
       "2          29                 23        6.034483         0.793103  \n",
       "3          12                 12        7.250000         1.000000  \n",
       "4          31                 31        6.483871         1.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "## Reading csv file\n",
    "train = pd.read_csv('train_new.csv')\n",
    "test = pd.read_csv('test_new.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea8a9c",
   "metadata": {},
   "source": [
    "# USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31888f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    40000\n",
      "Name: is_quote, dtype: int64\n",
      "1    40000\n",
      "Name: is_retweet, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "usa = train[train['country'] ==  'us'].reset_index(drop = True)\n",
    "\n",
    "print(usa['is_quote'].value_counts())\n",
    "print(usa['is_retweet'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f30e20b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        remember pandemic great percentage deaths resu...\n",
       "1        sources say tactics used get america open amp ...\n",
       "2        venture wild guess running usa crisis youd wan...\n",
       "3        protection jobs community youth within recentl...\n",
       "4        pand mie de 30 pasteurs ricains qui avaient mi...\n",
       "                               ...                        \n",
       "39995                                 racist gadsden flags\n",
       "39996    todays day salute maryland washington c virgin...\n",
       "39997    contrary predictions economic recovery mimic r...\n",
       "39998    watch leads maines remote media briefing begin...\n",
       "39999                           turning america full blown\n",
       "Name: clean_tweet, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usa['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5e39862",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 98.2 GiB for an array with shape (240000, 54939) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9639/3067657919.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hashtags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mother_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 98.2 GiB for an array with shape (240000, 54939) and data type float64"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = [w for w in test['hashtags']]\n",
    "other_features = vectorizer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a2c0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = [w for w in test['hashtags']]\n",
    "other_features = vectorizer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5323508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_features[0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c88414d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9639/3343564356.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcomment_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     wordcloud = WordCloud(width = 800, height = 800,\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mbackground_color\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \"\"\"\n\u001b[1;32m    620\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;31m# recompute bottom right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0;31m# the order of the cumsum's is important for speed ?!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0moccupancy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m             \u001b[0mlast_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, img_array, pos_x, pos_y)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mpartial_integral\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_x\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos_y\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mpartial_integral\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_y\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial_integral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    " \n",
    "# iterate through the csv file\n",
    "for val in usa['hashtags']:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    " \n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "\n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfab0e0",
   "metadata": {},
   "source": [
    "# UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4203f632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    40000\n",
      "Name: is_quote, dtype: int64\n",
      "1    40000\n",
      "Name: is_retweet, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "uk = train[train['country'] ==  'uk'].reset_index(drop = True)\n",
    "\n",
    "print(uk['is_quote'].value_counts())\n",
    "print(uk['is_retweet'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2973af9",
   "metadata": {},
   "source": [
    "# Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8843b5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    40000\n",
      "Name: is_quote, dtype: int64\n",
      "1    40000\n",
      "Name: is_retweet, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "canada = train[train['country'] ==  'canada'].reset_index(drop = True)\n",
    "\n",
    "print(canada['is_quote'].value_counts())\n",
    "print(canada['is_retweet'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aaff5b",
   "metadata": {},
   "source": [
    "# Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6101ffed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    23807\n",
      "1    16193\n",
      "Name: is_quote, dtype: int64\n",
      "1    30785\n",
      "0     9215\n",
      "Name: is_retweet, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "australia = train[train['country'] ==  'australia'].reset_index(drop = True)\n",
    "\n",
    "print(australia['is_quote'].value_counts())\n",
    "print(australia['is_retweet'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c93ae",
   "metadata": {},
   "source": [
    "# New Zealand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38c80cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    33388\n",
      "1     6612\n",
      "Name: is_quote, dtype: int64\n",
      "1    27658\n",
      "0    12342\n",
      "Name: is_retweet, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "new_zealand = train[train['country'] ==  'new_zealand'].reset_index(drop = True)\n",
    "\n",
    "print(new_zealand['is_quote'].value_counts())\n",
    "print(new_zealand['is_retweet'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7649064",
   "metadata": {},
   "source": [
    "# Ireland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd095790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    32600\n",
      "1     7400\n",
      "Name: is_quote, dtype: int64\n",
      "1    27453\n",
      "0    12547\n",
      "Name: is_retweet, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ireland = train[train['country'] ==  'ireland'].reset_index(drop = True)\n",
    "\n",
    "print(ireland['is_quote'].value_counts())\n",
    "print(ireland['is_retweet'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb51392",
   "metadata": {},
   "source": [
    "# k-NN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "545636cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "# import tensorflow as tf\n",
    "\n",
    "train = pd.read_csv('train_new.csv')\n",
    "test = pd.read_csv('test_new.csv')\n",
    "\n",
    "train = pd.read_csv('train_new.csv')\n",
    "train = train.fillna(0)\n",
    "\n",
    "test = pd.read_csv('test_new.csv')\n",
    "test = test.fillna(0)\n",
    "\n",
    "test_id = test['Id']\n",
    "test = test.drop(columns = ['Id', 'text', 'reply_to_screen_name', 'hashtags'], axis = 1)\n",
    "\n",
    "## Defining input and target\n",
    "X = train.drop(columns = ['text', 'reply_to_screen_name', 'hashtags', 'country'], axis = 1)\n",
    "Y = train['country']\n",
    "Y = np.where(Y == 'us', 0, \n",
    "             np.where(Y == 'uk', 1, \n",
    "                      np.where(Y == 'canada', 2, \n",
    "                               np.where(Y == 'australia', 3,\n",
    "                                        np.where(Y == 'ireland', 4, 5)))))\n",
    "\n",
    "## Splitting the data \n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.2)\n",
    "\n",
    "# ## Scaling the data \n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "# X_test = pd.DataFrame(scaler.fit_transform(X_test), columns = X.columns)\n",
    "# test = pd.DataFrame(scaler.fit_transform(test), columns = test.columns)\n",
    "\n",
    "# ## Defining model\n",
    "# knn = KNeighborsClassifier(n_neighbors = 5).fit(X_train, Y_train)\n",
    "\n",
    "# ## Predicting on est \n",
    "# knn_pred = knn.predict_proba(test)\n",
    "# knn_pred = np.argmax(knn_pred, axis = 1)\n",
    "\n",
    "# ## Defining data to be exported\n",
    "# data_out = pd.DataFrame({'Id': test_id, 'Category': knn_pred})\n",
    "# data_out['Category'] = np.where(data_out['Category'] == 0, 'us',\n",
    "#                                 np.where(data_out['Category'] == 1, 'uk',\n",
    "#                                          np.where(data_out['Category'] == 2, 'canada',\n",
    "#                                                   np.where(data_out['Category'] == 3, 'australia',\n",
    "#                                                            np.where(data_out['Category'] == 4, 'ireland', 'new_zealand')))))\n",
    "# data_out.to_csv('knn_submission_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "326d5864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.1.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from gensim) (1.5.3)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.2.0 smart-open-6.1.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fc40aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "for i in X.index:\n",
    "    text.append(X['clean_tweet'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9a029fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12161/3471000482.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             self.train(\n\u001b[1;32m    428\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \"\"\"\n\u001b[1;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         total_words, corpus_count = self.scan_vocab(\n\u001b[0m\u001b[1;32m    489\u001b[0m             corpus_iterable=corpus_iterable, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[1;32m    490\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    564\u001b[0m                     \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                 )\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(text, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58164e65",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12161/2673041116.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mLda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             self.add_lifecycle_event(\n\u001b[1;32m     80\u001b[0m                 \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"built %s from %i documents (total %i corpus positions)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import gensim\n",
    "from gensim.matutils import cossim\n",
    "topic = 20\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "dictionary = Dictionary(X_train['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e5483ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12161/2909391512.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# # Helper function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             self.train(\n\u001b[1;32m    428\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \"\"\"\n\u001b[1;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         total_words, corpus_count = self.scan_vocab(\n\u001b[0m\u001b[1;32m    489\u001b[0m             corpus_iterable=corpus_iterable, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[1;32m    490\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    564\u001b[0m                     \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                 )\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(X_train['clean_tweet'], min_count = 1)\n",
    "\n",
    "# # Helper function\n",
    "# def get_w2v_vector(doc):\n",
    "#     tmp = []\n",
    "#     for w in doc:\n",
    "#         try:\n",
    "#             tmp.append(model[w])\n",
    "#         except:\n",
    "#             pass\n",
    "#     return np.mean(tmp, axis=0)\n",
    "\n",
    "# # Convert training text into vectors \n",
    "# train_vectors_w2v = []\n",
    "# for doc in X_train:\n",
    "#     try:\n",
    "#         train_vectors_w2v.append(get_w2v_vector(doc))\n",
    "#     except Exception:\n",
    "#         print('error...')\n",
    "\n",
    "# # Convert Test text into vectors\n",
    "# test_vectors_w2v = []\n",
    "# for doc in X_test:\n",
    "#     try:\n",
    "#         test_vectors_w2v.append(get_w2v_vector(doc))\n",
    "#     except Exception:\n",
    "#         print('error...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e7b617b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>Trump_flag</th>\n",
       "      <th>jfrketich_flag</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>avg_wordlength</th>\n",
       "      <th>unique_vs_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173892</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cook shot man asked wear mask police said susp...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>115</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235240</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>construction continue ontario moves toronto pe...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105645</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>imagine potus senate gop worked fast effective...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>90</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24930</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>im vacation week 5 cant miss zoom calls amp co...</td>\n",
       "      <td>-0.145833</td>\n",
       "      <td>122</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>5.083333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170841</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>say mongolians brew great beer</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8745</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>havent heard 43 living received education 99k ...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>128</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>7.111111</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118601</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>amid americas disaster must come clean lie spr...</td>\n",
       "      <td>-0.050794</td>\n",
       "      <td>196</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>5.939394</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144177</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>im point pandemic tell time day premier tv 19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151130</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>alive vote november guess</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154341</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>point 8 months strain pandemic trump taken ser...</td>\n",
       "      <td>-0.099242</td>\n",
       "      <td>134</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>6.380952</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_quote  is_retweet  Trump_flag  jfrketich_flag  \\\n",
       "173892         0           1           0               0   \n",
       "235240         0           0           0               0   \n",
       "105645         1           1           0               0   \n",
       "24930          1           1           0               0   \n",
       "170841         0           0           0               0   \n",
       "...          ...         ...         ...             ...   \n",
       "8745           1           1           0               0   \n",
       "118601         1           1           0               0   \n",
       "144177         0           1           0               0   \n",
       "151130         0           1           0               0   \n",
       "154341         0           1           0               0   \n",
       "\n",
       "                                              clean_tweet  sentiment  \\\n",
       "173892  cook shot man asked wear mask police said susp...   0.000000   \n",
       "235240  construction continue ontario moves toronto pe...   0.000000   \n",
       "105645  imagine potus senate gop worked fast effective...   0.400000   \n",
       "24930   im vacation week 5 cant miss zoom calls amp co...  -0.145833   \n",
       "170841                     say mongolians brew great beer   0.800000   \n",
       "...                                                   ...        ...   \n",
       "8745    havent heard 43 living received education 99k ...   0.900000   \n",
       "118601  amid americas disaster must come clean lie spr...  -0.050794   \n",
       "144177      im point pandemic tell time day premier tv 19   0.000000   \n",
       "151130                          alive vote november guess   0.100000   \n",
       "154341  point 8 months strain pandemic trump taken ser...  -0.099242   \n",
       "\n",
       "        char_count  word_count  unique_word_count  avg_wordlength  \\\n",
       "173892         115          20                 17        5.750000   \n",
       "235240          64           8                  8        8.000000   \n",
       "105645          90          12                 12        7.500000   \n",
       "24930          122          24                 24        5.083333   \n",
       "170841          30           5                  5        6.000000   \n",
       "...            ...         ...                ...             ...   \n",
       "8745           128          18                 18        7.111111   \n",
       "118601         196          33                 30        5.939394   \n",
       "144177          45           9                  9        5.000000   \n",
       "151130          25           4                  4        6.250000   \n",
       "154341         134          21                 21        6.380952   \n",
       "\n",
       "        unique_vs_words  \n",
       "173892         0.850000  \n",
       "235240         1.000000  \n",
       "105645         1.000000  \n",
       "24930          1.000000  \n",
       "170841         1.000000  \n",
       "...                 ...  \n",
       "8745           1.000000  \n",
       "118601         0.909091  \n",
       "144177         1.000000  \n",
       "151130         1.000000  \n",
       "154341         1.000000  \n",
       "\n",
       "[48000 rows x 11 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f4e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc7746d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29b25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "596d9a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Category\n",
       "0   0  australia\n",
       "1   1    ireland\n",
       "2   2  australia\n",
       "3   3    ireland\n",
       "4   4  australia"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e41373b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ireland        19250\n",
       "australia      18271\n",
       "new_zealand    16464\n",
       "us              2575\n",
       "uk              2018\n",
       "canada          1422\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_out['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6e9a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out.to_csv('knn_submission_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e3398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ec38d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95ba942e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
