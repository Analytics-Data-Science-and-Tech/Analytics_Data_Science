{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10726b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-0.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from fastparquet) (2021.11.1)\n",
      "Requirement already satisfied: numpy>=1.18 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from fastparquet) (1.20.3)\n",
      "Collecting cramjam>=2.3.0\n",
      "  Downloading cramjam-2.6.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from fastparquet) (1.3.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from fastparquet) (21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from pandas>=1.1.0->fastparquet) (2021.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from packaging->fastparquet) (3.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.16.0)\n",
      "Installing collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.6.1 fastparquet-0.8.3\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastparquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1824ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0.7301891713.csv</th>\n",
       "      <th>0.6750726968.csv</th>\n",
       "      <th>0.7194704070.csv</th>\n",
       "      <th>0.7107007521.csv</th>\n",
       "      <th>0.6952032365.csv</th>\n",
       "      <th>0.7311830751.csv</th>\n",
       "      <th>0.6736005999.csv</th>\n",
       "      <th>0.7205109360.csv</th>\n",
       "      <th>0.7298116981.csv</th>\n",
       "      <th>0.6737624943.csv</th>\n",
       "      <th>0.7105097012.csv</th>\n",
       "      <th>0.7163693161.csv</th>\n",
       "      <th>0.7225567087.csv</th>\n",
       "      <th>0.6872126167.csv</th>\n",
       "      <th>0.7292947020.csv</th>\n",
       "      <th>0.6798503735.csv</th>\n",
       "      <th>0.7130672940.csv</th>\n",
       "      <th>0.6709092823.csv</th>\n",
       "      <th>0.7391105353.csv</th>\n",
       "      <th>0.6616334858.csv</th>\n",
       "      <th>0.6897190987.csv</th>\n",
       "      <th>0.6963558296.csv</th>\n",
       "      <th>0.7170726761.csv</th>\n",
       "      <th>0.7156669797.csv</th>\n",
       "      <th>0.6814483512.csv</th>\n",
       "      <th>0.7167410246.csv</th>\n",
       "      <th>0.6981892492.csv</th>\n",
       "      <th>0.7119194656.csv</th>\n",
       "      <th>0.6932545022.csv</th>\n",
       "      <th>0.6593521643.csv</th>\n",
       "      <th>0.6743157708.csv</th>\n",
       "      <th>0.7125579799.csv</th>\n",
       "      <th>0.7069356577.csv</th>\n",
       "      <th>0.6884713711.csv</th>\n",
       "      <th>0.6702128703.csv</th>\n",
       "      <th>0.6564147356.csv</th>\n",
       "      <th>0.6977737011.csv</th>\n",
       "      <th>0.6638768547.csv</th>\n",
       "      <th>0.7032756348.csv</th>\n",
       "      <th>0.6862982027.csv</th>\n",
       "      <th>0.6864219834.csv</th>\n",
       "      <th>0.7088904051.csv</th>\n",
       "      <th>0.7212180410.csv</th>\n",
       "      <th>0.7421980731.csv</th>\n",
       "      <th>0.6825341422.csv</th>\n",
       "      <th>0.6848737762.csv</th>\n",
       "      <th>0.7144978161.csv</th>\n",
       "      <th>0.6658856476.csv</th>\n",
       "      <th>0.7195078907.csv</th>\n",
       "      <th>...</th>\n",
       "      <th>0.6870791547.csv</th>\n",
       "      <th>0.6967096004.csv</th>\n",
       "      <th>0.7060967827.csv</th>\n",
       "      <th>0.7113625862.csv</th>\n",
       "      <th>0.6694947166.csv</th>\n",
       "      <th>0.6900693308.csv</th>\n",
       "      <th>0.7128591265.csv</th>\n",
       "      <th>0.6818718747.csv</th>\n",
       "      <th>0.6858034178.csv</th>\n",
       "      <th>0.7122377012.csv</th>\n",
       "      <th>0.7144335192.csv</th>\n",
       "      <th>0.7083170570.csv</th>\n",
       "      <th>0.7142499638.csv</th>\n",
       "      <th>0.7136242881.csv</th>\n",
       "      <th>0.6888742488.csv</th>\n",
       "      <th>0.7087145574.csv</th>\n",
       "      <th>0.6566530931.csv</th>\n",
       "      <th>0.7197358544.csv</th>\n",
       "      <th>0.7101817035.csv</th>\n",
       "      <th>0.7520219713.csv</th>\n",
       "      <th>0.7215952204.csv</th>\n",
       "      <th>0.6795805487.csv</th>\n",
       "      <th>0.7083706040.csv</th>\n",
       "      <th>0.6941218921.csv</th>\n",
       "      <th>0.6926641668.csv</th>\n",
       "      <th>0.6869857003.csv</th>\n",
       "      <th>0.7198785912.csv</th>\n",
       "      <th>0.7287761089.csv</th>\n",
       "      <th>0.7035686386.csv</th>\n",
       "      <th>0.7336496744.csv</th>\n",
       "      <th>0.6872533788.csv</th>\n",
       "      <th>0.7005346672.csv</th>\n",
       "      <th>0.7369069391.csv</th>\n",
       "      <th>0.7156537235.csv</th>\n",
       "      <th>0.7121002711.csv</th>\n",
       "      <th>0.7289228406.csv</th>\n",
       "      <th>0.7174789519.csv</th>\n",
       "      <th>0.7032181627.csv</th>\n",
       "      <th>0.6668147232.csv</th>\n",
       "      <th>0.7345858839.csv</th>\n",
       "      <th>0.7056743178.csv</th>\n",
       "      <th>0.6850481924.csv</th>\n",
       "      <th>0.7359858591.csv</th>\n",
       "      <th>0.7068431477.csv</th>\n",
       "      <th>0.7121179915.csv</th>\n",
       "      <th>0.6613117872.csv</th>\n",
       "      <th>0.6843637618.csv</th>\n",
       "      <th>0.6781392004.csv</th>\n",
       "      <th>0.7222809303.csv</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.913897</td>\n",
       "      <td>2.142608</td>\n",
       "      <td>1.502359</td>\n",
       "      <td>1.453620</td>\n",
       "      <td>1.336538</td>\n",
       "      <td>1.599475</td>\n",
       "      <td>1.236069</td>\n",
       "      <td>1.054479</td>\n",
       "      <td>1.131611</td>\n",
       "      <td>0.984009</td>\n",
       "      <td>1.171605</td>\n",
       "      <td>1.207453</td>\n",
       "      <td>1.400562</td>\n",
       "      <td>1.156730</td>\n",
       "      <td>1.655767</td>\n",
       "      <td>0.405465</td>\n",
       "      <td>1.194848</td>\n",
       "      <td>1.542418</td>\n",
       "      <td>1.921342</td>\n",
       "      <td>0.641228</td>\n",
       "      <td>1.025477</td>\n",
       "      <td>1.378848</td>\n",
       "      <td>1.257294</td>\n",
       "      <td>1.280277</td>\n",
       "      <td>1.467210</td>\n",
       "      <td>1.443668</td>\n",
       "      <td>1.391678</td>\n",
       "      <td>1.419787</td>\n",
       "      <td>1.054552</td>\n",
       "      <td>3.539317</td>\n",
       "      <td>0.983944</td>\n",
       "      <td>1.476214</td>\n",
       "      <td>1.234424</td>\n",
       "      <td>1.307563</td>\n",
       "      <td>2.385436</td>\n",
       "      <td>2.001724</td>\n",
       "      <td>1.257358</td>\n",
       "      <td>1.689581</td>\n",
       "      <td>1.290093</td>\n",
       "      <td>1.277571</td>\n",
       "      <td>1.269417</td>\n",
       "      <td>0.953358</td>\n",
       "      <td>0.838936</td>\n",
       "      <td>1.437067</td>\n",
       "      <td>1.378712</td>\n",
       "      <td>-0.090514</td>\n",
       "      <td>1.220395</td>\n",
       "      <td>2.258836</td>\n",
       "      <td>1.228620</td>\n",
       "      <td>...</td>\n",
       "      <td>1.172375</td>\n",
       "      <td>1.109329</td>\n",
       "      <td>1.245783</td>\n",
       "      <td>1.377996</td>\n",
       "      <td>1.435160</td>\n",
       "      <td>1.195812</td>\n",
       "      <td>1.595924</td>\n",
       "      <td>1.338426</td>\n",
       "      <td>1.170736</td>\n",
       "      <td>1.263989</td>\n",
       "      <td>0.925031</td>\n",
       "      <td>0.862942</td>\n",
       "      <td>1.198344</td>\n",
       "      <td>1.414648</td>\n",
       "      <td>1.305720</td>\n",
       "      <td>0.937820</td>\n",
       "      <td>0.674106</td>\n",
       "      <td>1.475712</td>\n",
       "      <td>1.064896</td>\n",
       "      <td>0.678348</td>\n",
       "      <td>1.330522</td>\n",
       "      <td>1.242018</td>\n",
       "      <td>1.256842</td>\n",
       "      <td>1.234825</td>\n",
       "      <td>1.252730</td>\n",
       "      <td>1.259111</td>\n",
       "      <td>1.237227</td>\n",
       "      <td>1.548106</td>\n",
       "      <td>1.496501</td>\n",
       "      <td>1.588877</td>\n",
       "      <td>1.322937</td>\n",
       "      <td>1.301587</td>\n",
       "      <td>1.143404</td>\n",
       "      <td>1.285821</td>\n",
       "      <td>1.362650</td>\n",
       "      <td>1.133906</td>\n",
       "      <td>1.368576</td>\n",
       "      <td>1.159616</td>\n",
       "      <td>2.507343</td>\n",
       "      <td>0.925301</td>\n",
       "      <td>1.451954</td>\n",
       "      <td>1.199074</td>\n",
       "      <td>1.364975</td>\n",
       "      <td>1.650185</td>\n",
       "      <td>1.184054</td>\n",
       "      <td>0.660613</td>\n",
       "      <td>1.385376</td>\n",
       "      <td>1.596353</td>\n",
       "      <td>1.544845</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.678169</td>\n",
       "      <td>0.576310</td>\n",
       "      <td>0.298877</td>\n",
       "      <td>0.301982</td>\n",
       "      <td>0.698756</td>\n",
       "      <td>1.131453</td>\n",
       "      <td>0.569003</td>\n",
       "      <td>0.426954</td>\n",
       "      <td>0.510710</td>\n",
       "      <td>0.430144</td>\n",
       "      <td>0.221108</td>\n",
       "      <td>0.264951</td>\n",
       "      <td>1.556663</td>\n",
       "      <td>1.082496</td>\n",
       "      <td>1.407428</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>0.920520</td>\n",
       "      <td>0.907372</td>\n",
       "      <td>1.113602</td>\n",
       "      <td>0.179283</td>\n",
       "      <td>1.125816</td>\n",
       "      <td>0.637407</td>\n",
       "      <td>0.381426</td>\n",
       "      <td>0.376108</td>\n",
       "      <td>1.140530</td>\n",
       "      <td>0.507529</td>\n",
       "      <td>0.556460</td>\n",
       "      <td>0.232753</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.810242</td>\n",
       "      <td>1.154435</td>\n",
       "      <td>0.464158</td>\n",
       "      <td>0.571005</td>\n",
       "      <td>0.498050</td>\n",
       "      <td>0.208698</td>\n",
       "      <td>0.693851</td>\n",
       "      <td>0.753772</td>\n",
       "      <td>0.798899</td>\n",
       "      <td>0.556192</td>\n",
       "      <td>0.626027</td>\n",
       "      <td>0.599505</td>\n",
       "      <td>0.451303</td>\n",
       "      <td>0.421232</td>\n",
       "      <td>1.578557</td>\n",
       "      <td>1.518368</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>0.332251</td>\n",
       "      <td>0.617989</td>\n",
       "      <td>0.828814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323980</td>\n",
       "      <td>0.483191</td>\n",
       "      <td>0.284638</td>\n",
       "      <td>0.408454</td>\n",
       "      <td>0.674884</td>\n",
       "      <td>0.355031</td>\n",
       "      <td>0.396140</td>\n",
       "      <td>1.250862</td>\n",
       "      <td>0.718760</td>\n",
       "      <td>0.147479</td>\n",
       "      <td>0.554945</td>\n",
       "      <td>0.257595</td>\n",
       "      <td>0.372066</td>\n",
       "      <td>0.811467</td>\n",
       "      <td>0.535556</td>\n",
       "      <td>0.407907</td>\n",
       "      <td>0.046973</td>\n",
       "      <td>0.404249</td>\n",
       "      <td>0.377877</td>\n",
       "      <td>0.950909</td>\n",
       "      <td>0.288525</td>\n",
       "      <td>0.595133</td>\n",
       "      <td>0.411427</td>\n",
       "      <td>0.758979</td>\n",
       "      <td>0.378051</td>\n",
       "      <td>0.539336</td>\n",
       "      <td>0.649062</td>\n",
       "      <td>0.266136</td>\n",
       "      <td>0.484547</td>\n",
       "      <td>0.557583</td>\n",
       "      <td>0.529086</td>\n",
       "      <td>1.068429</td>\n",
       "      <td>0.249275</td>\n",
       "      <td>0.325167</td>\n",
       "      <td>0.454034</td>\n",
       "      <td>0.363870</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.191891</td>\n",
       "      <td>0.913241</td>\n",
       "      <td>0.593457</td>\n",
       "      <td>0.576423</td>\n",
       "      <td>0.604565</td>\n",
       "      <td>0.851182</td>\n",
       "      <td>0.545539</td>\n",
       "      <td>0.652546</td>\n",
       "      <td>0.147133</td>\n",
       "      <td>0.615547</td>\n",
       "      <td>0.412286</td>\n",
       "      <td>0.128416</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.222971</td>\n",
       "      <td>1.493293</td>\n",
       "      <td>1.997012</td>\n",
       "      <td>1.580462</td>\n",
       "      <td>1.181609</td>\n",
       "      <td>1.527942</td>\n",
       "      <td>1.655077</td>\n",
       "      <td>1.549787</td>\n",
       "      <td>1.668255</td>\n",
       "      <td>1.272369</td>\n",
       "      <td>1.688099</td>\n",
       "      <td>1.579537</td>\n",
       "      <td>-0.373970</td>\n",
       "      <td>1.167650</td>\n",
       "      <td>1.493627</td>\n",
       "      <td>1.152680</td>\n",
       "      <td>1.961822</td>\n",
       "      <td>1.120913</td>\n",
       "      <td>1.743965</td>\n",
       "      <td>0.915126</td>\n",
       "      <td>1.470307</td>\n",
       "      <td>1.403919</td>\n",
       "      <td>1.821767</td>\n",
       "      <td>1.689505</td>\n",
       "      <td>1.227045</td>\n",
       "      <td>1.376995</td>\n",
       "      <td>1.348107</td>\n",
       "      <td>1.619013</td>\n",
       "      <td>0.584926</td>\n",
       "      <td>1.492138</td>\n",
       "      <td>1.441973</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.537328</td>\n",
       "      <td>1.360002</td>\n",
       "      <td>1.502614</td>\n",
       "      <td>1.400423</td>\n",
       "      <td>1.176323</td>\n",
       "      <td>1.449841</td>\n",
       "      <td>1.581607</td>\n",
       "      <td>1.417900</td>\n",
       "      <td>1.377263</td>\n",
       "      <td>1.590156</td>\n",
       "      <td>1.667259</td>\n",
       "      <td>1.592731</td>\n",
       "      <td>1.206471</td>\n",
       "      <td>0.565491</td>\n",
       "      <td>1.875071</td>\n",
       "      <td>1.503783</td>\n",
       "      <td>1.565373</td>\n",
       "      <td>...</td>\n",
       "      <td>1.275068</td>\n",
       "      <td>1.503897</td>\n",
       "      <td>1.652910</td>\n",
       "      <td>1.629185</td>\n",
       "      <td>1.435720</td>\n",
       "      <td>1.459440</td>\n",
       "      <td>1.472051</td>\n",
       "      <td>1.205913</td>\n",
       "      <td>1.217617</td>\n",
       "      <td>1.356744</td>\n",
       "      <td>1.547532</td>\n",
       "      <td>1.491958</td>\n",
       "      <td>1.631098</td>\n",
       "      <td>1.402725</td>\n",
       "      <td>1.328202</td>\n",
       "      <td>1.506278</td>\n",
       "      <td>0.673713</td>\n",
       "      <td>1.670106</td>\n",
       "      <td>1.525643</td>\n",
       "      <td>1.863093</td>\n",
       "      <td>1.558563</td>\n",
       "      <td>1.463832</td>\n",
       "      <td>1.483774</td>\n",
       "      <td>1.248124</td>\n",
       "      <td>1.574611</td>\n",
       "      <td>1.385882</td>\n",
       "      <td>1.543597</td>\n",
       "      <td>1.735119</td>\n",
       "      <td>1.300939</td>\n",
       "      <td>0.887515</td>\n",
       "      <td>1.389234</td>\n",
       "      <td>1.073305</td>\n",
       "      <td>1.716729</td>\n",
       "      <td>1.470162</td>\n",
       "      <td>1.566478</td>\n",
       "      <td>1.463176</td>\n",
       "      <td>1.638943</td>\n",
       "      <td>1.477231</td>\n",
       "      <td>1.721804</td>\n",
       "      <td>1.291973</td>\n",
       "      <td>1.703252</td>\n",
       "      <td>1.275325</td>\n",
       "      <td>1.562809</td>\n",
       "      <td>1.723188</td>\n",
       "      <td>1.644262</td>\n",
       "      <td>0.939329</td>\n",
       "      <td>1.258014</td>\n",
       "      <td>1.263616</td>\n",
       "      <td>1.449718</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.896871</td>\n",
       "      <td>1.278293</td>\n",
       "      <td>0.973904</td>\n",
       "      <td>1.035144</td>\n",
       "      <td>1.308422</td>\n",
       "      <td>0.783468</td>\n",
       "      <td>1.386619</td>\n",
       "      <td>0.824402</td>\n",
       "      <td>2.278990</td>\n",
       "      <td>1.024375</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>1.001671</td>\n",
       "      <td>0.373937</td>\n",
       "      <td>1.295792</td>\n",
       "      <td>1.057832</td>\n",
       "      <td>0.994623</td>\n",
       "      <td>1.556343</td>\n",
       "      <td>1.081597</td>\n",
       "      <td>1.754679</td>\n",
       "      <td>0.603690</td>\n",
       "      <td>0.924175</td>\n",
       "      <td>1.172713</td>\n",
       "      <td>0.905020</td>\n",
       "      <td>1.081385</td>\n",
       "      <td>0.965547</td>\n",
       "      <td>1.095021</td>\n",
       "      <td>1.106409</td>\n",
       "      <td>1.159985</td>\n",
       "      <td>0.324042</td>\n",
       "      <td>1.479513</td>\n",
       "      <td>1.223506</td>\n",
       "      <td>0.954404</td>\n",
       "      <td>1.265987</td>\n",
       "      <td>1.090734</td>\n",
       "      <td>1.270504</td>\n",
       "      <td>1.310926</td>\n",
       "      <td>0.944462</td>\n",
       "      <td>1.314942</td>\n",
       "      <td>1.200462</td>\n",
       "      <td>1.219480</td>\n",
       "      <td>0.953552</td>\n",
       "      <td>1.150811</td>\n",
       "      <td>0.928220</td>\n",
       "      <td>1.056390</td>\n",
       "      <td>0.843976</td>\n",
       "      <td>-0.166987</td>\n",
       "      <td>1.022170</td>\n",
       "      <td>1.166871</td>\n",
       "      <td>1.137476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976177</td>\n",
       "      <td>1.129621</td>\n",
       "      <td>1.208627</td>\n",
       "      <td>1.062720</td>\n",
       "      <td>1.269867</td>\n",
       "      <td>1.039596</td>\n",
       "      <td>1.197524</td>\n",
       "      <td>0.879743</td>\n",
       "      <td>1.172203</td>\n",
       "      <td>0.790902</td>\n",
       "      <td>0.747272</td>\n",
       "      <td>1.001915</td>\n",
       "      <td>0.991064</td>\n",
       "      <td>1.215666</td>\n",
       "      <td>1.112823</td>\n",
       "      <td>0.956059</td>\n",
       "      <td>0.008708</td>\n",
       "      <td>1.143529</td>\n",
       "      <td>0.900674</td>\n",
       "      <td>1.387107</td>\n",
       "      <td>1.340876</td>\n",
       "      <td>1.136052</td>\n",
       "      <td>1.200682</td>\n",
       "      <td>1.098196</td>\n",
       "      <td>1.233457</td>\n",
       "      <td>1.530723</td>\n",
       "      <td>1.094137</td>\n",
       "      <td>0.994070</td>\n",
       "      <td>0.821261</td>\n",
       "      <td>0.726283</td>\n",
       "      <td>1.197614</td>\n",
       "      <td>1.140601</td>\n",
       "      <td>0.525599</td>\n",
       "      <td>1.026553</td>\n",
       "      <td>1.113339</td>\n",
       "      <td>1.366259</td>\n",
       "      <td>1.147440</td>\n",
       "      <td>1.110037</td>\n",
       "      <td>1.107421</td>\n",
       "      <td>1.149415</td>\n",
       "      <td>1.293589</td>\n",
       "      <td>1.054019</td>\n",
       "      <td>1.304492</td>\n",
       "      <td>1.696110</td>\n",
       "      <td>1.026975</td>\n",
       "      <td>0.627869</td>\n",
       "      <td>0.987531</td>\n",
       "      <td>0.870218</td>\n",
       "      <td>1.035372</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.058965</td>\n",
       "      <td>2.544616</td>\n",
       "      <td>2.858902</td>\n",
       "      <td>2.998670</td>\n",
       "      <td>3.241791</td>\n",
       "      <td>2.775474</td>\n",
       "      <td>3.488716</td>\n",
       "      <td>2.573477</td>\n",
       "      <td>3.378161</td>\n",
       "      <td>2.027585</td>\n",
       "      <td>2.722684</td>\n",
       "      <td>2.738472</td>\n",
       "      <td>3.371317</td>\n",
       "      <td>3.193845</td>\n",
       "      <td>2.805438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>3.016500</td>\n",
       "      <td>3.235045</td>\n",
       "      <td>3.953457</td>\n",
       "      <td>1.255074</td>\n",
       "      <td>2.829039</td>\n",
       "      <td>3.143655</td>\n",
       "      <td>2.941832</td>\n",
       "      <td>2.673290</td>\n",
       "      <td>2.515048</td>\n",
       "      <td>3.085532</td>\n",
       "      <td>3.332468</td>\n",
       "      <td>3.181993</td>\n",
       "      <td>2.018135</td>\n",
       "      <td>2.945808</td>\n",
       "      <td>3.904138</td>\n",
       "      <td>3.623315</td>\n",
       "      <td>2.737631</td>\n",
       "      <td>2.846201</td>\n",
       "      <td>3.081115</td>\n",
       "      <td>2.864492</td>\n",
       "      <td>3.006279</td>\n",
       "      <td>3.374874</td>\n",
       "      <td>2.746529</td>\n",
       "      <td>3.100904</td>\n",
       "      <td>2.876392</td>\n",
       "      <td>2.644707</td>\n",
       "      <td>2.685909</td>\n",
       "      <td>2.617825</td>\n",
       "      <td>2.506095</td>\n",
       "      <td>0.972523</td>\n",
       "      <td>2.827976</td>\n",
       "      <td>3.341034</td>\n",
       "      <td>3.003345</td>\n",
       "      <td>...</td>\n",
       "      <td>3.121378</td>\n",
       "      <td>2.451609</td>\n",
       "      <td>2.867927</td>\n",
       "      <td>2.945429</td>\n",
       "      <td>3.384986</td>\n",
       "      <td>3.164904</td>\n",
       "      <td>2.863122</td>\n",
       "      <td>2.612441</td>\n",
       "      <td>3.120637</td>\n",
       "      <td>2.520899</td>\n",
       "      <td>2.848479</td>\n",
       "      <td>2.800744</td>\n",
       "      <td>2.666093</td>\n",
       "      <td>2.965629</td>\n",
       "      <td>2.998184</td>\n",
       "      <td>2.819584</td>\n",
       "      <td>1.508360</td>\n",
       "      <td>3.100806</td>\n",
       "      <td>2.817177</td>\n",
       "      <td>3.202041</td>\n",
       "      <td>2.866316</td>\n",
       "      <td>3.209280</td>\n",
       "      <td>2.889581</td>\n",
       "      <td>3.514552</td>\n",
       "      <td>2.836488</td>\n",
       "      <td>2.295757</td>\n",
       "      <td>2.916437</td>\n",
       "      <td>3.205187</td>\n",
       "      <td>3.005812</td>\n",
       "      <td>3.575979</td>\n",
       "      <td>2.869147</td>\n",
       "      <td>2.760525</td>\n",
       "      <td>3.139726</td>\n",
       "      <td>2.831662</td>\n",
       "      <td>2.876452</td>\n",
       "      <td>2.794520</td>\n",
       "      <td>3.009266</td>\n",
       "      <td>2.896961</td>\n",
       "      <td>2.852543</td>\n",
       "      <td>3.470820</td>\n",
       "      <td>3.095699</td>\n",
       "      <td>3.037184</td>\n",
       "      <td>2.592989</td>\n",
       "      <td>3.366128</td>\n",
       "      <td>2.809742</td>\n",
       "      <td>1.250261</td>\n",
       "      <td>3.443677</td>\n",
       "      <td>2.818023</td>\n",
       "      <td>2.787013</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  0.7301891713.csv  0.6750726968.csv  0.7194704070.csv  0.7107007521.csv  \\\n",
       "0   0          0.913897          2.142608          1.502359          1.453620   \n",
       "1   1          0.678169          0.576310          0.298877          0.301982   \n",
       "2   2          1.222971          1.493293          1.997012          1.580462   \n",
       "3   3          0.896871          1.278293          0.973904          1.035144   \n",
       "4   4          3.058965          2.544616          2.858902          2.998670   \n",
       "\n",
       "   0.6952032365.csv  0.7311830751.csv  0.6736005999.csv  0.7205109360.csv  \\\n",
       "0          1.336538          1.599475          1.236069          1.054479   \n",
       "1          0.698756          1.131453          0.569003          0.426954   \n",
       "2          1.181609          1.527942          1.655077          1.549787   \n",
       "3          1.308422          0.783468          1.386619          0.824402   \n",
       "4          3.241791          2.775474          3.488716          2.573477   \n",
       "\n",
       "   0.7298116981.csv  0.6737624943.csv  0.7105097012.csv  0.7163693161.csv  \\\n",
       "0          1.131611          0.984009          1.171605          1.207453   \n",
       "1          0.510710          0.430144          0.221108          0.264951   \n",
       "2          1.668255          1.272369          1.688099          1.579537   \n",
       "3          2.278990          1.024375          0.975785          1.001671   \n",
       "4          3.378161          2.027585          2.722684          2.738472   \n",
       "\n",
       "   0.7225567087.csv  0.6872126167.csv  0.7292947020.csv  0.6798503735.csv  \\\n",
       "0          1.400562          1.156730          1.655767          0.405465   \n",
       "1          1.556663          1.082496          1.407428          1.152680   \n",
       "2         -0.373970          1.167650          1.493627          1.152680   \n",
       "3          0.373937          1.295792          1.057832          0.994623   \n",
       "4          3.371317          3.193845          2.805438          1.386294   \n",
       "\n",
       "   0.7130672940.csv  0.6709092823.csv  0.7391105353.csv  0.6616334858.csv  \\\n",
       "0          1.194848          1.542418          1.921342          0.641228   \n",
       "1          0.920520          0.907372          1.113602          0.179283   \n",
       "2          1.961822          1.120913          1.743965          0.915126   \n",
       "3          1.556343          1.081597          1.754679          0.603690   \n",
       "4          3.016500          3.235045          3.953457          1.255074   \n",
       "\n",
       "   0.6897190987.csv  0.6963558296.csv  0.7170726761.csv  0.7156669797.csv  \\\n",
       "0          1.025477          1.378848          1.257294          1.280277   \n",
       "1          1.125816          0.637407          0.381426          0.376108   \n",
       "2          1.470307          1.403919          1.821767          1.689505   \n",
       "3          0.924175          1.172713          0.905020          1.081385   \n",
       "4          2.829039          3.143655          2.941832          2.673290   \n",
       "\n",
       "   0.6814483512.csv  0.7167410246.csv  0.6981892492.csv  0.7119194656.csv  \\\n",
       "0          1.467210          1.443668          1.391678          1.419787   \n",
       "1          1.140530          0.507529          0.556460          0.232753   \n",
       "2          1.227045          1.376995          1.348107          1.619013   \n",
       "3          0.965547          1.095021          1.106409          1.159985   \n",
       "4          2.515048          3.085532          3.332468          3.181993   \n",
       "\n",
       "   0.6932545022.csv  0.6593521643.csv  0.6743157708.csv  0.7125579799.csv  \\\n",
       "0          1.054552          3.539317          0.983944          1.476214   \n",
       "1          0.004920          0.810242          1.154435          0.464158   \n",
       "2          0.584926          1.492138          1.441973          1.098612   \n",
       "3          0.324042          1.479513          1.223506          0.954404   \n",
       "4          2.018135          2.945808          3.904138          3.623315   \n",
       "\n",
       "   0.7069356577.csv  0.6884713711.csv  0.6702128703.csv  0.6564147356.csv  \\\n",
       "0          1.234424          1.307563          2.385436          2.001724   \n",
       "1          0.571005          0.498050          0.208698          0.693851   \n",
       "2          1.537328          1.360002          1.502614          1.400423   \n",
       "3          1.265987          1.090734          1.270504          1.310926   \n",
       "4          2.737631          2.846201          3.081115          2.864492   \n",
       "\n",
       "   0.6977737011.csv  0.6638768547.csv  0.7032756348.csv  0.6862982027.csv  \\\n",
       "0          1.257358          1.689581          1.290093          1.277571   \n",
       "1          0.753772          0.798899          0.556192          0.626027   \n",
       "2          1.176323          1.449841          1.581607          1.417900   \n",
       "3          0.944462          1.314942          1.200462          1.219480   \n",
       "4          3.006279          3.374874          2.746529          3.100904   \n",
       "\n",
       "   0.6864219834.csv  0.7088904051.csv  0.7212180410.csv  0.7421980731.csv  \\\n",
       "0          1.269417          0.953358          0.838936          1.437067   \n",
       "1          0.599505          0.451303          0.421232          1.578557   \n",
       "2          1.377263          1.590156          1.667259          1.592731   \n",
       "3          0.953552          1.150811          0.928220          1.056390   \n",
       "4          2.876392          2.644707          2.685909          2.617825   \n",
       "\n",
       "   0.6825341422.csv  0.6848737762.csv  0.7144978161.csv  0.6658856476.csv  \\\n",
       "0          1.378712         -0.090514          1.220395          2.258836   \n",
       "1          1.518368          0.012512          0.332251          0.617989   \n",
       "2          1.206471          0.565491          1.875071          1.503783   \n",
       "3          0.843976         -0.166987          1.022170          1.166871   \n",
       "4          2.506095          0.972523          2.827976          3.341034   \n",
       "\n",
       "   0.7195078907.csv  ...  0.6870791547.csv  0.6967096004.csv  \\\n",
       "0          1.228620  ...          1.172375          1.109329   \n",
       "1          0.828814  ...          0.323980          0.483191   \n",
       "2          1.565373  ...          1.275068          1.503897   \n",
       "3          1.137476  ...          0.976177          1.129621   \n",
       "4          3.003345  ...          3.121378          2.451609   \n",
       "\n",
       "   0.7060967827.csv  0.7113625862.csv  0.6694947166.csv  0.6900693308.csv  \\\n",
       "0          1.245783          1.377996          1.435160          1.195812   \n",
       "1          0.284638          0.408454          0.674884          0.355031   \n",
       "2          1.652910          1.629185          1.435720          1.459440   \n",
       "3          1.208627          1.062720          1.269867          1.039596   \n",
       "4          2.867927          2.945429          3.384986          3.164904   \n",
       "\n",
       "   0.7128591265.csv  0.6818718747.csv  0.6858034178.csv  0.7122377012.csv  \\\n",
       "0          1.595924          1.338426          1.170736          1.263989   \n",
       "1          0.396140          1.250862          0.718760          0.147479   \n",
       "2          1.472051          1.205913          1.217617          1.356744   \n",
       "3          1.197524          0.879743          1.172203          0.790902   \n",
       "4          2.863122          2.612441          3.120637          2.520899   \n",
       "\n",
       "   0.7144335192.csv  0.7083170570.csv  0.7142499638.csv  0.7136242881.csv  \\\n",
       "0          0.925031          0.862942          1.198344          1.414648   \n",
       "1          0.554945          0.257595          0.372066          0.811467   \n",
       "2          1.547532          1.491958          1.631098          1.402725   \n",
       "3          0.747272          1.001915          0.991064          1.215666   \n",
       "4          2.848479          2.800744          2.666093          2.965629   \n",
       "\n",
       "   0.6888742488.csv  0.7087145574.csv  0.6566530931.csv  0.7197358544.csv  \\\n",
       "0          1.305720          0.937820          0.674106          1.475712   \n",
       "1          0.535556          0.407907          0.046973          0.404249   \n",
       "2          1.328202          1.506278          0.673713          1.670106   \n",
       "3          1.112823          0.956059          0.008708          1.143529   \n",
       "4          2.998184          2.819584          1.508360          3.100806   \n",
       "\n",
       "   0.7101817035.csv  0.7520219713.csv  0.7215952204.csv  0.6795805487.csv  \\\n",
       "0          1.064896          0.678348          1.330522          1.242018   \n",
       "1          0.377877          0.950909          0.288525          0.595133   \n",
       "2          1.525643          1.863093          1.558563          1.463832   \n",
       "3          0.900674          1.387107          1.340876          1.136052   \n",
       "4          2.817177          3.202041          2.866316          3.209280   \n",
       "\n",
       "   0.7083706040.csv  0.6941218921.csv  0.6926641668.csv  0.6869857003.csv  \\\n",
       "0          1.256842          1.234825          1.252730          1.259111   \n",
       "1          0.411427          0.758979          0.378051          0.539336   \n",
       "2          1.483774          1.248124          1.574611          1.385882   \n",
       "3          1.200682          1.098196          1.233457          1.530723   \n",
       "4          2.889581          3.514552          2.836488          2.295757   \n",
       "\n",
       "   0.7198785912.csv  0.7287761089.csv  0.7035686386.csv  0.7336496744.csv  \\\n",
       "0          1.237227          1.548106          1.496501          1.588877   \n",
       "1          0.649062          0.266136          0.484547          0.557583   \n",
       "2          1.543597          1.735119          1.300939          0.887515   \n",
       "3          1.094137          0.994070          0.821261          0.726283   \n",
       "4          2.916437          3.205187          3.005812          3.575979   \n",
       "\n",
       "   0.6872533788.csv  0.7005346672.csv  0.7369069391.csv  0.7156537235.csv  \\\n",
       "0          1.322937          1.301587          1.143404          1.285821   \n",
       "1          0.529086          1.068429          0.249275          0.325167   \n",
       "2          1.389234          1.073305          1.716729          1.470162   \n",
       "3          1.197614          1.140601          0.525599          1.026553   \n",
       "4          2.869147          2.760525          3.139726          2.831662   \n",
       "\n",
       "   0.7121002711.csv  0.7289228406.csv  0.7174789519.csv  0.7032181627.csv  \\\n",
       "0          1.362650          1.133906          1.368576          1.159616   \n",
       "1          0.454034          0.363870          0.201669          0.191891   \n",
       "2          1.566478          1.463176          1.638943          1.477231   \n",
       "3          1.113339          1.366259          1.147440          1.110037   \n",
       "4          2.876452          2.794520          3.009266          2.896961   \n",
       "\n",
       "   0.6668147232.csv  0.7345858839.csv  0.7056743178.csv  0.6850481924.csv  \\\n",
       "0          2.507343          0.925301          1.451954          1.199074   \n",
       "1          0.913241          0.593457          0.576423          0.604565   \n",
       "2          1.721804          1.291973          1.703252          1.275325   \n",
       "3          1.107421          1.149415          1.293589          1.054019   \n",
       "4          2.852543          3.470820          3.095699          3.037184   \n",
       "\n",
       "   0.7359858591.csv  0.7068431477.csv  0.7121179915.csv  0.6613117872.csv  \\\n",
       "0          1.364975          1.650185          1.184054          0.660613   \n",
       "1          0.851182          0.545539          0.652546          0.147133   \n",
       "2          1.562809          1.723188          1.644262          0.939329   \n",
       "3          1.304492          1.696110          1.026975          0.627869   \n",
       "4          2.592989          3.366128          2.809742          1.250261   \n",
       "\n",
       "   0.6843637618.csv  0.6781392004.csv  0.7222809303.csv  label  \n",
       "0          1.385376          1.596353          1.544845    0.0  \n",
       "1          0.615547          0.412286          0.128416    1.0  \n",
       "2          1.258014          1.263616          1.449718    1.0  \n",
       "3          0.987531          0.870218          1.035372    1.0  \n",
       "4          3.443677          2.818023          2.787013    0.0  \n",
       "\n",
       "[5 rows x 5002 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from lightgbm import LGBMClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'analytics-data-science-competitions'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "file_key_1 = 'Tabular-Playground-Series/Tabular-Playground-Nov-2022/sample_submission.csv'\n",
    "file_key_2 = 'Tabular-Playground-Series/Tabular-Playground-Nov-2022/train_labels.csv'\n",
    "\n",
    "bucket_object_1 = bucket.Object(file_key_1)\n",
    "file_object_1 = bucket_object_1.get()\n",
    "file_content_stream_1 = file_object_1.get('Body')\n",
    "\n",
    "bucket_object_2 = bucket.Object(file_key_2)\n",
    "file_object_2 = bucket_object_2.get()\n",
    "file_content_stream_2 = file_object_2.get('Body')\n",
    "\n",
    "## Reading data-files\n",
    "submission = pd.read_csv(file_content_stream_1)\n",
    "y_true = pd.read_csv(file_content_stream_2)\n",
    "df = pd.read_parquet('s3://analytics-data-science-competitions/Tabular-Playground-Series/Tabular-Playground-Nov-2022/preds_logit_concat_gzip.parquet', engine = 'fastparquet')\n",
    "\n",
    "preds = pd.merge(df, y_true, on = 'id', how = 'left')\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6410da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10277/3277270779.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['label'] = train['label'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "train = preds[preds['label'].notnull()]\n",
    "train['label'] = train['label'].astype(int)\n",
    "\n",
    "test = preds[preds['label'].isnull()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd375e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coef</th>\n",
       "      <th>abs_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6564517315.csv</td>\n",
       "      <td>2.601288</td>\n",
       "      <td>2.601288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7183933428.csv</td>\n",
       "      <td>-2.560396</td>\n",
       "      <td>2.560396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6570599977.csv</td>\n",
       "      <td>2.443219</td>\n",
       "      <td>2.443219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6454375051.csv</td>\n",
       "      <td>2.201914</td>\n",
       "      <td>2.201914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7103145610.csv</td>\n",
       "      <td>2.114975</td>\n",
       "      <td>2.114975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Feature      Coef  abs_coef\n",
       "0  0.6564517315.csv  2.601288  2.601288\n",
       "1  0.7183933428.csv -2.560396  2.560396\n",
       "2  0.6570599977.csv  2.443219  2.443219\n",
       "3  0.6454375051.csv  2.201914  2.201914\n",
       "4  0.7103145610.csv  2.114975  2.114975"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_scores = pd.read_csv('lasso_scores_logit.csv')\n",
    "lasso_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e83d8504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.6564517315.csv', '0.7183933428.csv', '0.6570599977.csv',\n",
       "       '0.6454375051.csv', '0.7103145610.csv', '0.6495843938.csv',\n",
       "       '0.6802400070.csv', '0.7120881745.csv', '0.6708990553.csv',\n",
       "       '0.6911116549.csv', '0.6575661514.csv', '0.6982536185.csv',\n",
       "       '0.6988389476.csv', '0.7123959613.csv', '0.6603097589.csv',\n",
       "       '0.6666609229.csv', '0.6601920939.csv', '0.7204986049.csv',\n",
       "       '0.7113211865.csv', '0.6492627054.csv', '0.7203099744.csv',\n",
       "       '0.7298182840.csv', '0.6541278746.csv', '0.6657348857.csv',\n",
       "       '0.6917509910.csv', '0.7215499173.csv', '0.6893289917.csv',\n",
       "       '0.6507043652.csv', '0.7145930700.csv', '0.7410778621.csv',\n",
       "       '0.6856099530.csv', '0.6598940198.csv', '0.6914345720.csv',\n",
       "       '0.7121114883.csv', '0.7086765052.csv', '0.7136612407.csv',\n",
       "       '0.7163657677.csv', '0.6891839618.csv', '0.6552358052.csv',\n",
       "       '0.6839403095.csv', '0.7143591677.csv', '0.6785443179.csv',\n",
       "       '0.7081765641.csv', '0.7168899938.csv', '0.7170741747.csv',\n",
       "       '0.7102063924.csv', '0.7047759534.csv', '0.7105139967.csv',\n",
       "       '0.7104104660.csv', '0.7191820280.csv'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_select = lasso_scores['Feature'][0:50]\n",
    "to_select.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f95c2bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5745 - val_loss: 0.7201\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5389 - val_loss: 0.5934\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5401 - val_loss: 0.5978\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5312 - val_loss: 0.6151\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5340 - val_loss: 0.5946\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5323 - val_loss: 0.6442\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5325 - val_loss: 0.6144\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5312 - val_loss: 0.5906\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5303 - val_loss: 0.7546\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5341 - val_loss: 0.5707\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5296 - val_loss: 0.6482\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5293 - val_loss: 0.5923\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5279 - val_loss: 0.6123\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5271 - val_loss: 0.6023\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5279 - val_loss: 0.6016\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5263 - val_loss: 0.6032\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5288 - val_loss: 0.6172\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5288 - val_loss: 0.6097\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5281 - val_loss: 0.5523\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5265 - val_loss: 0.5914\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5266 - val_loss: 0.5592\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5257 - val_loss: 0.5434\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5264 - val_loss: 0.7234\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5280 - val_loss: 0.5637\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5244 - val_loss: 0.5871\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5249 - val_loss: 0.5721\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5266 - val_loss: 0.5613\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5249 - val_loss: 0.5970\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5261 - val_loss: 0.6384\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5253 - val_loss: 0.5892\n",
      "Fold  1  result is: 0.7657275237884315 \n",
      "\n",
      "Epoch 1/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5241 - val_loss: 0.5254\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5238 - val_loss: 0.5269\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5225 - val_loss: 0.5231\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5230 - val_loss: 0.5377\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5217 - val_loss: 0.5273\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5232 - val_loss: 0.5279\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5227 - val_loss: 0.5273\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5212 - val_loss: 0.5358\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5211 - val_loss: 0.5241\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5219 - val_loss: 0.5331\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5210 - val_loss: 0.5245\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5231 - val_loss: 0.5343\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5213 - val_loss: 0.5245\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5218 - val_loss: 0.5247\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5215 - val_loss: 0.5300\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5220 - val_loss: 0.5672\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5222 - val_loss: 0.5242\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5206 - val_loss: 0.5332\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5219 - val_loss: 0.5446\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5213 - val_loss: 0.5240\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5207 - val_loss: 0.5281\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5201 - val_loss: 0.5247\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5219 - val_loss: 0.5230\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5213 - val_loss: 0.5285\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5213 - val_loss: 0.5524\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5204 - val_loss: 0.5261\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5210 - val_loss: 0.5309\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5201 - val_loss: 0.5241\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5196 - val_loss: 0.5240\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5194 - val_loss: 0.5259\n",
      "Fold  2  result is: 0.5929623178251637 \n",
      "\n",
      "Epoch 1/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5195 - val_loss: 0.5348\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5192 - val_loss: 0.5293\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5199 - val_loss: 0.5242\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5181 - val_loss: 0.5258\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5198 - val_loss: 0.5294\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5187 - val_loss: 0.5356\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5199 - val_loss: 0.5312\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5194 - val_loss: 0.5247\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5188 - val_loss: 0.5258\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5182 - val_loss: 0.5264\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5195 - val_loss: 0.5293\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5190 - val_loss: 0.5299\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5184 - val_loss: 0.5234\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5181 - val_loss: 0.5238\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5181 - val_loss: 0.5274\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5183 - val_loss: 0.5250\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5169 - val_loss: 0.5314\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5183 - val_loss: 0.5243\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5183 - val_loss: 0.5267\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5185 - val_loss: 0.5235\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5172 - val_loss: 0.5236\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5186 - val_loss: 0.5280\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5173 - val_loss: 0.5250\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5179 - val_loss: 0.5281\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5184 - val_loss: 0.5244\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5173 - val_loss: 0.5248\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5173 - val_loss: 0.5308\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5179 - val_loss: 0.5244\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5171 - val_loss: 0.5243\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5166 - val_loss: 0.5283\n",
      "Fold  3  result is: 0.6006166381717424 \n",
      "\n",
      "Epoch 1/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5208 - val_loss: 0.5138\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5197 - val_loss: 0.5239\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5215 - val_loss: 0.5147\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5199 - val_loss: 0.5126\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5189 - val_loss: 0.5128\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5201 - val_loss: 0.5130\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5202 - val_loss: 0.5166\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5199 - val_loss: 0.5139\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5204 - val_loss: 0.5307\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5189 - val_loss: 0.5213\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5191 - val_loss: 0.5145\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5200 - val_loss: 0.5175\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5199 - val_loss: 0.5153\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5196 - val_loss: 0.5130\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5182 - val_loss: 0.5145\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5196 - val_loss: 0.5159\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5187 - val_loss: 0.5182\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5180 - val_loss: 0.5176\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5197 - val_loss: 0.5131\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5191 - val_loss: 0.5159\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5188 - val_loss: 0.5135\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5176 - val_loss: 0.5195\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5189 - val_loss: 0.5231\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5182 - val_loss: 0.5162\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5176 - val_loss: 0.5192\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5184 - val_loss: 0.5229\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5199 - val_loss: 0.5204\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5187 - val_loss: 0.5136\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5171 - val_loss: 0.5146\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5183 - val_loss: 0.5162\n",
      "Fold  4  result is: 0.5799200599546968 \n",
      "\n",
      "Epoch 1/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5191 - val_loss: 0.5836\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5210 - val_loss: 0.6082\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5196 - val_loss: 0.5831\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5189 - val_loss: 0.6107\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5183 - val_loss: 0.6675\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5177 - val_loss: 0.5656\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5179 - val_loss: 0.6896\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5190 - val_loss: 0.6012\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5177 - val_loss: 0.5675\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5189 - val_loss: 0.7131\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5185 - val_loss: 0.6065\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5187 - val_loss: 0.6492\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5181 - val_loss: 0.5784\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5175 - val_loss: 0.6490\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5184 - val_loss: 0.6669\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5187 - val_loss: 0.6102\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5186 - val_loss: 0.6031\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5185 - val_loss: 0.6345\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5181 - val_loss: 0.6535\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5168 - val_loss: 0.6445\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5172 - val_loss: 0.6909\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5179 - val_loss: 0.6000\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5193 - val_loss: 0.6052\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5191 - val_loss: 0.6050\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5172 - val_loss: 0.5867\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5188 - val_loss: 0.6241\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5170 - val_loss: 0.5431\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5174 - val_loss: 0.6379\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5164 - val_loss: 0.5817\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.5167 - val_loss: 0.7062\n",
      "Fold  5  result is: 1.0831540324525613 \n",
      "\n",
      "The average log-loss over 5-fold CV is 0.724476114438519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = train[to_select.values]\n",
    "Y = train['label']\n",
    "\n",
    "test_new = test[to_select.values]\n",
    "test_new = pd.DataFrame(scaler.fit_transform(test_new), columns = test_new.columns)\n",
    "\n",
    "## Defining model \n",
    "lgb_md = tf.keras.models.Sequential([\n",
    "         tf.keras.layers.Dense(32, input_dim = 50, activation = 'relu'),\n",
    "         tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "         tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "         tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    "lgb_md.compile(optimizer = 'adam', \n",
    "               loss = 'binary_crossentropy')\n",
    "\n",
    "# lgb_md.compile(optimizer = optimizers.RMSprop(learning_rate = 0.001), \n",
    "#                loss = 'binary_crossentropy')\n",
    "\n",
    "## Defining list to store results\n",
    "lgb_results = list()\n",
    "test_preds_lgb_fold_1 = list() \n",
    "test_preds_lgb_fold_2 = list()\n",
    "test_preds_lgb_fold_3 = list()\n",
    "test_preds_lgb_fold_4 = list()\n",
    "test_preds_lgb_fold_5 = list()\n",
    "\n",
    "fold = 1\n",
    "kfold = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "        \n",
    "for train_ix, test_ix in kfold.split(X, Y):\n",
    "    \n",
    "    ## Splitting the data \n",
    "    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "    Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)\n",
    "\n",
    "    ## Building model\n",
    "    lgb_md.fit(X_train, tf.keras.utils.to_categorical(Y_train, num_classes = 2), \n",
    "               verbose = 1, epochs = 30, batch_size = 32, \n",
    "               validation_data = (X_test, \n",
    "                                  tf.keras.utils.to_categorical(Y_test, num_classes = 2)))\n",
    "    \n",
    "    \n",
    "#     lgb_md = LGBMClassifier(n_estimators = 1000, \n",
    "#                             learning_rate = 0.01,\n",
    "#                             num_leaves = 50,\n",
    "#                             max_depth = 17, \n",
    "#                             lambda_l1 = 3, \n",
    "#                             lambda_l2 = 1, \n",
    "#                             bagging_fraction = 0.8, \n",
    "#                             feature_fraction = 0.8).fit(X_train, Y_train)\n",
    "        \n",
    "    ## Predicting on test\n",
    "    lgb_pred = lgb_md.predict(X_test)[:, 1].astype('float64')\n",
    "    score = log_loss(Y_test, lgb_pred)\n",
    "    lgb_results.append(score)\n",
    "        \n",
    "    print('Fold ', str(fold), ' result is:', score, '\\n')\n",
    "    \n",
    "    if (fold == 1):\n",
    "        test_preds_lgb_fold_1.append(lgb_md.predict(test_new)[:, 1].astype('float64'))\n",
    "        \n",
    "    if (fold == 2):\n",
    "        test_preds_lgb_fold_2.append(lgb_md.predict(test_new)[:, 1].astype('float64'))\n",
    "        \n",
    "    if (fold == 3):\n",
    "        test_preds_lgb_fold_3.append(lgb_md.predict(test_new)[:, 1].astype('float64'))\n",
    "        \n",
    "    if (fold == 4):\n",
    "        test_preds_lgb_fold_4.append(lgb_md.predict(test_new)[:, 1].astype('float64'))\n",
    "        \n",
    "    if (fold == 5):\n",
    "        test_preds_lgb_fold_5.append(lgb_md.predict(test_new)[:, 1].astype('float64'))\n",
    "    \n",
    "    fold +=1\n",
    "\n",
    "print('The average log-loss over 5-fold CV is', np.mean(lgb_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fca22a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64723808, 0.77032477, 0.14611351, ..., 0.60055375, 0.54604065,\n",
       "       0.24457301])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_lgb_fold_4[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5aa2be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 1/ lgb_results[0]\n",
    "w2 = 1/ lgb_results[1]\n",
    "w3 = 1/ lgb_results[2]\n",
    "w4 = 1/ lgb_results[3]\n",
    "w5 = 1/ lgb_results[4]\n",
    "w_tot = w1 + w2 + w3+ w4 + w5\n",
    "w1 = w1 / w_tot\n",
    "w2 = w2 / w_tot\n",
    "w3 = w3 / w_tot\n",
    "w4 = w4 / w_tot\n",
    "w5 = w5 / w_tot\n",
    "\n",
    "pred1 = w1*test_preds_lgb_fold_1[0]\n",
    "pred2 = w2*test_preds_lgb_fold_2[0]\n",
    "pred3 = w3*test_preds_lgb_fold_3[0]\n",
    "pred4 = w4*test_preds_lgb_fold_4[0]\n",
    "pred5 = w4*test_preds_lgb_fold_5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb75608a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.647238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>0.770325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20002</td>\n",
       "      <td>0.146114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20003</td>\n",
       "      <td>0.511132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20004</td>\n",
       "      <td>0.006267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      pred\n",
       "0  20000  0.647238\n",
       "1  20001  0.770325\n",
       "2  20002  0.146114\n",
       "3  20003  0.511132\n",
       "4  20004  0.006267"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['pred'] = test_preds_lgb_fold_4[0]\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "223384c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>0.791941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20002</td>\n",
       "      <td>0.099367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20003</td>\n",
       "      <td>0.488424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20004</td>\n",
       "      <td>0.014468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      pred\n",
       "0  20000  0.624641\n",
       "1  20001  0.791941\n",
       "2  20002  0.099367\n",
       "3  20003  0.488424\n",
       "4  20004  0.014468"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['pred'] = pred1 + pred2 + pred3 + pred4 + pred5\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c964d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_TF_lasso_50_fold4.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
