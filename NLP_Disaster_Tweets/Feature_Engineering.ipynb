{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ce739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting tidytext\n",
      "  Downloading tidytext-0.0.1.tar.gz (4.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 KB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting siuba\n",
      "  Downloading siuba-0.3.0-py3-none-any.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 KB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from siuba->tidytext) (1.20.3)\n",
      "Requirement already satisfied: PyYAML>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from siuba->tidytext) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.2.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from siuba->tidytext) (1.4.27)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from siuba->tidytext) (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=0.24.0->siuba->tidytext) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=0.24.0->siuba->tidytext) (2021.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from SQLAlchemy>=1.2.19->siuba->tidytext) (1.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->siuba->tidytext) (1.16.0)\n",
      "Building wheels for collected packages: tidytext\n",
      "  Building wheel for tidytext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tidytext: filename=tidytext-0.0.1-py3-none-any.whl size=3895 sha256=ce0fe0e1887c3e2ee298834c3c589b2feab22ec757eb3d0a86c64a99476dfaf5\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/07/03/c0/f73eeef462dd66dbca0288a338fcbcdc78e3588937ccc907d8\n",
      "Successfully built tidytext\n",
      "Installing collected packages: textblob, siuba, tidytext\n",
      "Successfully installed siuba-0.3.0 textblob-0.17.1 tidytext-0.0.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tidytext textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb57e8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a482b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_columns', 200)\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Help_Funs import count_chars, count_words, count_capital_chars, count_capital_words, count_sent, count_unique_words, count_stopwords, count_hashtags \n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'analytics-data-science-competitions'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "## Defining files names\n",
    "file_key_1 = 'NLP-Disaster-Tweets/train.csv'\n",
    "file_key_2 = 'NLP-Disaster-Tweets/test.csv'\n",
    "file_key_3 = 'NLP-Disaster-Tweets/sample_submission.csv'\n",
    "\n",
    "bucket_object_1 = bucket.Object(file_key_1)\n",
    "file_object_1 = bucket_object_1.get()\n",
    "file_content_stream_1 = file_object_1.get('Body')\n",
    "\n",
    "bucket_object_2 = bucket.Object(file_key_2)\n",
    "file_object_2 = bucket_object_2.get()\n",
    "file_content_stream_2 = file_object_2.get('Body')\n",
    "\n",
    "bucket_object_3 = bucket.Object(file_key_3)\n",
    "file_object_3 = bucket_object_3.get()\n",
    "file_content_stream_3 = file_object_3.get('Body')\n",
    "\n",
    "## Reading data-files\n",
    "train = pd.read_csv(file_content_stream_1)\n",
    "test = pd.read_csv(file_content_stream_2)\n",
    "sample = pd.read_csv(file_content_stream_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4d8c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "others = set([\"1\", \"2\", \"it'll\", \"ill\", \"=\", '+', \"'s'\", '\"'])\n",
    "stop_words = stop_words.union(others)\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    if type(tweet) == np.float:\n",
    "        return \"\"\n",
    "    temp = tweet.lower()\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(r'http\\S+', '', temp)\n",
    "    temp = re.sub('[()!?]', ' ', temp)\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "    temp = temp.split()\n",
    "    temp = [w for w in temp if not w in stop_words]\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b818bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7613 [00:00<?, ?it/s]/tmp/ipykernel_22854/4248643825.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['clean_tweet'][i] =  clean_tweet(train['text'][i])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "100%|██████████| 7613/7613 [00:00<00:00, 23668.28it/s]\n"
     ]
    }
   ],
   "source": [
    "train['clean_tweet'] = np.nan\n",
    "\n",
    "for i in tqdm(range(0, train.shape[0])):\n",
    "    \n",
    "    train['clean_tweet'][i] =  clean_tweet(train['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9325e369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3263 [00:00<?, ?it/s]/tmp/ipykernel_22854/1846826793.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['clean_tweet'][i] =  clean_tweet(test['text'][i])\n",
      "100%|██████████| 3263/3263 [00:00<00:00, 30411.00it/s]\n"
     ]
    }
   ],
   "source": [
    "test['clean_tweet'] = np.nan\n",
    "\n",
    "for i in tqdm(range(0, test.shape[0])):\n",
    "    \n",
    "    test['clean_tweet'][i] =  clean_tweet(test['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76af2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentiment'] = train['clean_tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "train['subjectivity'] = train['clean_tweet'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "test['sentiment'] = test['clean_tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "test['subjectivity'] = test['clean_tweet'].apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a46c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentiment_label'] = np.where(train['sentiment'] < 0, 'negative', \n",
    "                                    np.where(train['sentiment'] == 0, 'neutral', 'positive'))\n",
    "\n",
    "test['sentiment_label'] = np.where(test['sentiment'] < 0, 'negative', \n",
    "                                   np.where(test['sentiment'] == 0, 'neutral', 'positive'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f14727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dummies = pd.get_dummies(train['sentiment_label'])\n",
    "train = pd.concat([train.drop(columns = ['sentiment', 'sentiment_label'], axis = 1), train_dummies], axis = 1)\n",
    "\n",
    "test_dummies = pd.get_dummies(test['sentiment_label'])\n",
    "test = pd.concat([test.drop(columns = ['sentiment', 'sentiment_label'], axis = 1), test_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55c868c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['char_count'] = train['text'].apply(lambda x: count_chars(x))\n",
    "train['word_count'] = train['text'].apply(lambda x: count_words(x))\n",
    "train['sent_count'] = train['text'].apply(lambda x: count_sent(x))\n",
    "train['capital_char_count'] = train['text'].apply(lambda x: count_capital_chars(x))\n",
    "train['capital_word_count'] = train['text'].apply(lambda x: count_capital_words(x))\n",
    "# train['quoted_word_count'] = train['text'].apply(lambda x: count_words_in_quotes(x))\n",
    "train['stopword_count'] = train['text'].apply(lambda x: count_stopwords(x))\n",
    "train['unique_word_count'] = train['text'].apply(lambda x: count_unique_words(x))\n",
    "                                                 \n",
    "test['char_count'] = test['text'].apply(lambda x: count_chars(x))\n",
    "test['word_count'] = test['text'].apply(lambda x: count_words(x))\n",
    "test['sent_count'] = test['text'].apply(lambda x: count_sent(x))\n",
    "test['capital_char_count'] = test['text'].apply(lambda x: count_capital_chars(x))\n",
    "test['capital_word_count'] = test['text'].apply(lambda x: count_capital_words(x))\n",
    "# test['quoted_word_count'] = test['text'].apply(lambda x: count_words_in_quotes(x))\n",
    "test['stopword_count'] = test['text'].apply(lambda x: count_stopwords(x))\n",
    "test['unique_word_count'] = test['text'].apply(lambda x: count_unique_words(x))\n",
    "                                                 \n",
    "## Average word length\n",
    "train['avg_wordlength'] = train['char_count'] / train['word_count']\n",
    "test['avg_wordlength'] = test['char_count'] / test['word_count']\n",
    "\n",
    "## Average sentence lenght\n",
    "train['avg_sentlength'] = train['word_count'] / train['sent_count']\n",
    "test['avg_sentlength'] = test['word_count'] / test['sent_count']\n",
    "\n",
    "## Unique words vs count words\n",
    "train['unique_vs_words'] = train['unique_word_count'] / train['word_count']\n",
    "test['unique_vs_words'] = test['unique_word_count'] / test['word_count']\n",
    "\n",
    "## stopwords vs count words\n",
    "train['stopwords_vs_words'] = train['stopword_count'] / train['word_count']\n",
    "test['stopwords_vs_words'] = test['stopword_count'] / test['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "298524dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 150)\n",
    "# vectorizer = TfidfVectorizer()\n",
    "train_docs = [train['clean_tweet'][i] for i in range(0, train.shape[0])]\n",
    "test_docs = [test['clean_tweet'][i] for i in range(0, test.shape[0])]\n",
    "train_tf_idf = vectorizer.fit_transform(train_docs).toarray()\n",
    "test_tf_idf = vectorizer.transform(test_docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c28f73bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = ['feature_' + str(i) for i in range(1, 151)]\n",
    "train_tf_idf = pd.DataFrame(train_tf_idf, columns = features_names)\n",
    "test_tf_idf = pd.DataFrame(test_tf_idf, columns = features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b37c3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, train_tf_idf], axis = 1)\n",
    "test = pd.concat([test, test_tf_idf], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03439b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_2.csv', index = False)\n",
    "test.to_csv('test_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62581ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['char_count'] = train['clean_tweet'].apply(lambda x: count_chars(x))\n",
    "# train['word_count'] = train['clean_tweet'].apply(lambda x: count_words(x))\n",
    "# train['unique_word_count'] = train['clean_tweet'].apply(lambda x: count_unique_words(x))\n",
    "                                                 \n",
    "# test['char_count'] = test['clean_tweet'].apply(lambda x: count_chars(x))\n",
    "# test['word_count'] = test['clean_tweet'].apply(lambda x: count_words(x))\n",
    "# test['unique_word_count'] = test['clean_tweet'].apply(lambda x: count_unique_words(x))\n",
    "                                                 \n",
    "# ## Average word length\n",
    "# train['avg_wordlength'] = train['char_count'] / train['word_count']\n",
    "# test['avg_wordlength'] = test['char_count'] / test['word_count']\n",
    "\n",
    "# ## Unique words vs count words\n",
    "# train['unique_vs_words'] = train['unique_word_count'] / train['word_count']\n",
    "# test['unique_vs_words'] = test['unique_word_count'] / test['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1142854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "149278ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>capital_char_count</th>\n",
       "      <th>capital_word_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>avg_wordlength</th>\n",
       "      <th>avg_sentlength</th>\n",
       "      <th>unique_vs_words</th>\n",
       "      <th>stopwords_vs_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason may allah forgive us</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>5.307692</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>6.045455</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13 000 people receive evacuation orders califo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby smoke pours school</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                        clean_tweet  subjectivity  \\\n",
       "0       1                  deeds reason may allah forgive us           0.0   \n",
       "1       1              forest fire near la ronge sask canada           0.4   \n",
       "2       1  residents asked shelter place notified officer...           0.4   \n",
       "3       1  13 000 people receive evacuation orders califo...           0.0   \n",
       "4       1             got sent photo ruby smoke pours school           0.0   \n",
       "\n",
       "   negative  neutral  positive  char_count  word_count  sent_count  \\\n",
       "0         0        1         0          69          13           1   \n",
       "1         0        0         1          38           7           2   \n",
       "2         1        0         0         133          22           2   \n",
       "3         0        1         0          65           8           1   \n",
       "4         0        1         0          88          16           1   \n",
       "\n",
       "   capital_char_count  capital_word_count  stopword_count  unique_word_count  \\\n",
       "0                  10                   1               5                 13   \n",
       "1                   5                   0               0                  7   \n",
       "2                   2                   0               9                 20   \n",
       "3                   1                   0               1                  8   \n",
       "4                   3                   0               6                 15   \n",
       "\n",
       "   avg_wordlength  avg_sentlength  unique_vs_words  stopwords_vs_words  \n",
       "0        5.307692            13.0         1.000000            0.384615  \n",
       "1        5.428571             3.5         1.000000            0.000000  \n",
       "2        6.045455            11.0         0.909091            0.409091  \n",
       "3        8.125000             8.0         1.000000            0.125000  \n",
       "4        5.500000            16.0         0.937500            0.375000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daec9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', index = False)\n",
    "test.to_csv('test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821025eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'].value_counts() / train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755daecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train['target'], train['sentiment_label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
