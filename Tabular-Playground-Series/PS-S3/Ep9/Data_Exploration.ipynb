{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5dfdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.4-py3-none-manylinux2014_x86_64.whl (193.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lightgbm\n",
      "  Downloading lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting catboost\n",
      "  Downloading catboost-1.1.1-cp310-none-manylinux1_x86_64.whl (76.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting optuna\n",
      "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xgboost) (1.10.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xgboost) (1.22.3)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.2.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (0.38.4)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: plotly in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from catboost) (5.11.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from catboost) (3.6.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from catboost) (1.5.2)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (1.4.46)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (4.64.1)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (5.4.1)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=0.24.0->catboost) (2022.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->catboost) (1.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->catboost) (9.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->catboost) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from plotly->catboost) (8.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Installing collected packages: Mako, graphviz, colorlog, cmaes, xgboost, alembic, optuna, lightgbm, catboost\n",
      "Successfully installed Mako-1.2.4 alembic-1.9.4 catboost-1.1.1 cmaes-0.9.1 colorlog-6.7.0 graphviz-0.20.1 lightgbm-3.3.5 optuna-3.1.0 xgboost-1.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost lightgbm catboost optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcba852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor \n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "import optuna \n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'analytics-data-science-competitions'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "file_key_1 = 'Tabular-Playground-Series/PS-S3/Ep9/train.csv'\n",
    "file_key_2 = 'Tabular-Playground-Series/PS-S3/Ep9/test.csv'\n",
    "file_key_3 = 'Tabular-Playground-Series/PS-S3/Ep9/sample_submission.csv'\n",
    "\n",
    "bucket_object_1 = bucket.Object(file_key_1)\n",
    "file_object_1 = bucket_object_1.get()\n",
    "file_content_stream_1 = file_object_1.get('Body')\n",
    "\n",
    "bucket_object_2 = bucket.Object(file_key_2)\n",
    "file_object_2 = bucket_object_2.get()\n",
    "file_content_stream_2 = file_object_2.get('Body')\n",
    "\n",
    "bucket_object_3 = bucket.Object(file_key_3)\n",
    "file_object_3 = bucket_object_3.get()\n",
    "file_content_stream_3 = file_object_3.get('Body')\n",
    "\n",
    "## Reading data files\n",
    "train = pd.read_csv(file_content_stream_1)\n",
    "test = pd.read_csv(file_content_stream_2)\n",
    "submission = pd.read_csv(file_content_stream_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c026c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5407</td>\n",
       "      <td>47.949070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5408</td>\n",
       "      <td>19.403589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5409</td>\n",
       "      <td>33.624952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5410</td>\n",
       "      <td>46.432587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5411</td>\n",
       "      <td>32.492635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id   Strength\n",
       "0  5407  47.949070\n",
       "1  5408  19.403589\n",
       "2  5409  33.624952\n",
       "3  5410  46.432587\n",
       "4  5411  32.492635"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train.drop(columns = ['id', 'Strength'], axis = 1)\n",
    "Y = train['Strength']\n",
    "X['WaterComponent_to_Cement_ratio'] = X['WaterComponent'] / (X['CementComponent'] + 1e-6)\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "test_baseline['WaterComponent_to_Cement_ratio'] = test_baseline['WaterComponent'] / (test_baseline['CementComponent'] + 1e-6)\n",
    "\n",
    "\n",
    "cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                           iterations = 1000,\n",
    "                           learning_rate = 0.01,\n",
    "                           depth = 3,\n",
    "                           random_strength = 0.5,\n",
    "                           bagging_temperature = 0.7,\n",
    "                           border_count = 30,\n",
    "                           l2_leaf_reg = 5,\n",
    "                           verbose = False).fit(X, Y)\n",
    "\n",
    "cat_pred = cat_md.predict(test_baseline)\n",
    "submission['Strength'] = cat_pred\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4fafe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('catboost_full_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "402ba599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [13:31<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "X = train.drop(columns = ['id', 'Strength'], axis = 1)\n",
    "Y = train['Strength']\n",
    "X['WaterComponent_to_Cement_ratio'] = X['WaterComponent'] / (X['CementComponent'] + 1e-6)\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "test_baseline['WaterComponent_to_Cement_ratio'] = test_baseline['WaterComponent'] / (test_baseline['CementComponent'] + 1e-6)\n",
    "\n",
    "preds_train, preds_test = list(), list()\n",
    "\n",
    "for i in tqdm(range(0, 1000)):\n",
    "    \n",
    "    CatBoostRegressor(loss_function = 'RMSE',\n",
    "                           iterations = 1000,\n",
    "                           learning_rate = 0.01,\n",
    "                           depth = 3,\n",
    "                           random_strength = 0.5,\n",
    "                           bagging_temperature = 0.7,\n",
    "                           border_count = 30,\n",
    "                           l2_leaf_reg = 5,\n",
    "                           verbose = False, \n",
    "                           random_seed = i).fit(X, Y)\n",
    "    \n",
    "#     cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "#                                iterations = 1756,\n",
    "#                                learning_rate = 0.017975994415651907,\n",
    "#                                depth = 3,\n",
    "#                                random_strength = 2.8219170899365174,\n",
    "#                                bagging_temperature = 0.19623966374044916,\n",
    "#                                border_count = 70,\n",
    "#                                l2_leaf_reg = 29,\n",
    "#                                verbose = False,\n",
    "#                                random_seed = i).fit(X, Y)\n",
    "    \n",
    "    preds_train.append(cat_md.predict(X))\n",
    "    preds_test.append(cat_md.predict(test_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b17108b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5407</td>\n",
       "      <td>48.030337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5408</td>\n",
       "      <td>18.512752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5409</td>\n",
       "      <td>33.690971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5410</td>\n",
       "      <td>48.222575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5411</td>\n",
       "      <td>28.180064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id   Strength\n",
       "0  5407  48.030337\n",
       "1  5408  18.512752\n",
       "2  5409  33.690971\n",
       "3  5410  48.222575\n",
       "4  5411  28.180064"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_preds_test = pd.DataFrame(preds_test).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = cat_preds_test\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71410895",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('catboost_full_1000_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91a1f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedEnsemble(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _rmse_loss(self, coef, X, y):\n",
    "        \n",
    "        n = X.shape[1]\n",
    "        ens = 0\n",
    "        for i in range(0, n):\n",
    "            ens += coef[i]*X[:, i]\n",
    "        \n",
    "        ll = mean_squared_error(y, ens, squared = False)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._rmse_loss, X = X, y = y)\n",
    "        n = X.shape[1]\n",
    "        initial_coef = np.repeat(1/n, n)\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \n",
    "        n = X.shape[1]\n",
    "        ens = 0\n",
    "        for i in range(0, n):\n",
    "            ens += coef[i]*X[:, i]\n",
    "        return ens\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43bffd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_pred_train = np.array(pd.DataFrame(preds_train).T)\n",
    "models_pred_oof = np.array(pd.DataFrame(preds_test).T)\n",
    "\n",
    "# n = models_pred_oof.shape[1]\n",
    "# ens = 0\n",
    "# for i in range(0, n):\n",
    "#     ens += models_pred_oof[:, i]\n",
    "\n",
    "opt_ens = OptimizedEnsemble()\n",
    "opt_ens.fit(models_pred_train, Y)\n",
    "coef = opt_ens.coefficients()\n",
    "ens_pred_train = opt_ens.predict(models_pred_train, coef)\n",
    "ens_pred_test = opt_ens.predict(models_pred_oof, coef)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66808c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.542104753650678"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(Y, ens_pred_train, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71778deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0020166053882114"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "098d1271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5407</td>\n",
       "      <td>49.647288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5408</td>\n",
       "      <td>16.885482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5409</td>\n",
       "      <td>34.435151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5410</td>\n",
       "      <td>48.597436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5411</td>\n",
       "      <td>23.453819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id   Strength\n",
       "0  5407  49.647288\n",
       "1  5408  16.885482\n",
       "2  5409  34.435151\n",
       "3  5410  48.597436\n",
       "4  5411  23.453819"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['Strength'] = ens_pred_test\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0171522",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('catboost_full_50_opt_ens_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d5044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d63a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e71a5984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3605, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pd.DataFrame(preds).T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a18b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ecdfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns = ['id', 'Strength'], axis = 1)\n",
    "Y = train['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "XGB_cv_scores, XGB_imp = list(), list()\n",
    "XGB_preds = list()\n",
    "\n",
    "lgb_cv_scores, lgb_imp = list(), list()\n",
    "lgb_preds = list()\n",
    "\n",
    "cat_cv_scores, cat_imp = list(), list()\n",
    "cat_preds = list()\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.7, \n",
    "                              gamma = 0.8, \n",
    "                              learning_rate = 0.01, \n",
    "                              max_depth = 7, \n",
    "                              min_child_weight = 10, \n",
    "                              n_estimators = 1000, \n",
    "                              subsample = 0.7).fit(X_train, Y_train)\n",
    "        XGB_imp.append(XGB_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        XGB_cv_scores.append(mean_squared_error(Y_test, XGB_pred_1, squared = False))\n",
    "        XGB_preds.append(XGB_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 1000,\n",
    "                               max_depth = 7,\n",
    "                               learning_rate = 0.01,\n",
    "                               num_leaves = 20,\n",
    "                               lambda_l1 = 3,\n",
    "                               lambda_l2 = 3,\n",
    "                               bagging_fraction = 0.7,\n",
    "                               feature_fraction = 0.7).fit(X_train, Y_train)\n",
    "        lgb_imp.append(lgb_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        lgb_cv_scores.append(mean_squared_error(Y_test, lgb_pred_1, squared = False))\n",
    "        lgb_preds.append(lgb_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 1000,\n",
    "                                   learning_rate = 0.01,\n",
    "                                   depth = 7,\n",
    "                                   random_strength = 0.5,\n",
    "                                   bagging_temperature = 0.7,\n",
    "                                   border_count = 30,\n",
    "                                   l2_leaf_reg = 5,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        cat_imp.append(cat_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        cat_cv_scores.append(mean_squared_error(Y_test, cat_pred_1, squared = False))\n",
    "        cat_preds.append(cat_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4696e0e",
   "metadata": {},
   "source": [
    "# Baseline Modeling 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns = ['id', 'Strength'], axis = 1)\n",
    "Y = train['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "XGB_cv_scores, XGB_imp = list(), list()\n",
    "XGB_preds = list()\n",
    "\n",
    "lgb_cv_scores, lgb_imp = list(), list()\n",
    "lgb_preds = list()\n",
    "\n",
    "cat_cv_scores, cat_imp = list(), list()\n",
    "cat_preds = list()\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.7, \n",
    "                              gamma = 0.8, \n",
    "                              learning_rate = 0.01, \n",
    "                              max_depth = 7, \n",
    "                              min_child_weight = 10, \n",
    "                              n_estimators = 1000, \n",
    "                              subsample = 0.7).fit(X_train, Y_train)\n",
    "        XGB_imp.append(XGB_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        XGB_cv_scores.append(mean_squared_error(Y_test, XGB_pred_1, squared = False))\n",
    "        XGB_preds.append(XGB_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 1000,\n",
    "                               max_depth = 7,\n",
    "                               learning_rate = 0.01,\n",
    "                               num_leaves = 20,\n",
    "                               lambda_l1 = 3,\n",
    "                               lambda_l2 = 3,\n",
    "                               bagging_fraction = 0.7,\n",
    "                               feature_fraction = 0.7).fit(X_train, Y_train)\n",
    "        lgb_imp.append(lgb_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        lgb_cv_scores.append(mean_squared_error(Y_test, lgb_pred_1, squared = False))\n",
    "        lgb_preds.append(lgb_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 1000,\n",
    "                                   learning_rate = 0.01,\n",
    "                                   depth = 7,\n",
    "                                   random_strength = 0.5,\n",
    "                                   bagging_temperature = 0.7,\n",
    "                                   border_count = 30,\n",
    "                                   l2_leaf_reg = 5,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        cat_imp.append(cat_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        cat_cv_scores.append(mean_squared_error(Y_test, cat_pred_1, squared = False))\n",
    "        cat_preds.append(cat_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bac712",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_cv_score = np.mean(XGB_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the XGBoost model is:', XGB_cv_score)\n",
    "\n",
    "lgb_cv_score = np.mean(lgb_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the LightGBM model is:', lgb_cv_score)\n",
    "\n",
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ef0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preds_test = pd.DataFrame(cat_preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = cat_preds_test\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b767939",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('catboost_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f368ba1",
   "metadata": {},
   "source": [
    "# Baseline Modeling 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].mean()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "XGB_cv_scores, XGB_imp = list(), list()\n",
    "XGB_preds = list()\n",
    "\n",
    "lgb_cv_scores, lgb_imp = list(), list()\n",
    "lgb_preds = list()\n",
    "\n",
    "cat_cv_scores, cat_imp = list(), list()\n",
    "cat_preds = list()\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.7, \n",
    "                              gamma = 0.8, \n",
    "                              learning_rate = 0.01, \n",
    "                              max_depth = 7, \n",
    "                              min_child_weight = 10, \n",
    "                              n_estimators = 1000, \n",
    "                              subsample = 0.7).fit(X_train, Y_train)\n",
    "        XGB_imp.append(XGB_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        XGB_cv_scores.append(mean_squared_error(Y_test, XGB_pred_1, squared = False))\n",
    "        XGB_preds.append(XGB_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 1000,\n",
    "                               max_depth = 7,\n",
    "                               learning_rate = 0.01,\n",
    "                               num_leaves = 20,\n",
    "                               lambda_l1 = 3,\n",
    "                               lambda_l2 = 3,\n",
    "                               bagging_fraction = 0.7,\n",
    "                               feature_fraction = 0.7).fit(X_train, Y_train)\n",
    "        lgb_imp.append(lgb_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        lgb_cv_scores.append(mean_squared_error(Y_test, lgb_pred_1, squared = False))\n",
    "        lgb_preds.append(lgb_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 1000,\n",
    "                                   learning_rate = 0.01,\n",
    "                                   depth = 7,\n",
    "                                   random_strength = 0.5,\n",
    "                                   bagging_temperature = 0.7,\n",
    "                                   border_count = 30,\n",
    "                                   l2_leaf_reg = 5,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        cat_imp.append(cat_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        cat_cv_scores.append(mean_squared_error(Y_test, cat_pred_1, squared = False))\n",
    "        cat_preds.append(cat_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ae2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_cv_score = np.mean(XGB_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the XGBoost model is:', XGB_cv_score)\n",
    "\n",
    "lgb_cv_score = np.mean(lgb_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the LightGBM model is:', lgb_cv_score)\n",
    "\n",
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4973db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preds_test = pd.DataFrame(cat_preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = cat_preds_test\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('catboost_submission_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6401512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].median()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "XGB_cv_scores, XGB_imp = list(), list()\n",
    "XGB_preds = list()\n",
    "\n",
    "lgb_cv_scores, lgb_imp = list(), list()\n",
    "lgb_preds = list()\n",
    "\n",
    "cat_cv_scores, cat_imp = list(), list()\n",
    "cat_preds = list()\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "#         XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "#                               colsample_bytree = 0.7, \n",
    "#                               gamma = 0.8, \n",
    "#                               learning_rate = 0.01, \n",
    "#                               max_depth = 7, \n",
    "#                               min_child_weight = 10, \n",
    "#                               n_estimators = 1000, \n",
    "#                               subsample = 0.7).fit(X_train, Y_train)\n",
    "#         XGB_imp.append(XGB_md.feature_importances_)\n",
    "        \n",
    "#         ## Predicting on X_test and test\n",
    "#         XGB_pred_1 = XGB_md.predict(X_test)\n",
    "#         XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        \n",
    "#         ## Computing rmse\n",
    "#         XGB_cv_scores.append(mean_squared_error(Y_test, XGB_pred_1, squared = False))\n",
    "#         XGB_preds.append(XGB_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "#         lgb_md = LGBMRegressor(n_estimators = 1000,\n",
    "#                                max_depth = 7,\n",
    "#                                learning_rate = 0.01,\n",
    "#                                num_leaves = 20,\n",
    "#                                lambda_l1 = 3,\n",
    "#                                lambda_l2 = 3,\n",
    "#                                bagging_fraction = 0.7,\n",
    "#                                feature_fraction = 0.7).fit(X_train, Y_train)\n",
    "#         lgb_imp.append(lgb_md.feature_importances_)\n",
    "        \n",
    "#         ## Predicting on X_test and test\n",
    "#         lgb_pred_1 = lgb_md.predict(X_test)\n",
    "#         lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        \n",
    "#         ## Computing rmse\n",
    "#         lgb_cv_scores.append(mean_squared_error(Y_test, lgb_pred_1, squared = False))\n",
    "#         lgb_preds.append(lgb_pred_2)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 4738,\n",
    "                                   learning_rate = 0.003143666241424718,\n",
    "                                   depth = 4,\n",
    "                                   random_strength = 0.29823973415192867,\n",
    "                                   bagging_temperature = 0.3408793603898661,\n",
    "                                   border_count = 112,\n",
    "                                   l2_leaf_reg = 17,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        cat_imp.append(cat_md.feature_importances_)\n",
    "        \n",
    "        ## Predicting on X_test and test\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        \n",
    "        ## Computing rmse\n",
    "        cat_cv_scores.append(mean_squared_error(Y_test, cat_pred_1, squared = False))\n",
    "        cat_preds.append(cat_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b78728bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is: 11.880628569091348\n"
     ]
    }
   ],
   "source": [
    "# XGB_cv_score = np.mean(XGB_cv_scores)    \n",
    "# print('The average oof rmse score over 5-folds (run 5 times) of the XGBoost model is:', XGB_cv_score)\n",
    "\n",
    "# lgb_cv_score = np.mean(lgb_cv_scores)    \n",
    "# print('The average oof rmse score over 5-folds (run 5 times) of the LightGBM model is:', lgb_cv_score)\n",
    "\n",
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d6b1e0",
   "metadata": {},
   "source": [
    "# Baseline 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a077f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedEnsemble(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _rmse_loss(self, coef, X, y):\n",
    "        \n",
    "        ens = coef[0]*X[:, 0] + coef[1]*X[:, 1] + coef[2]*X[:, 2]\n",
    "        ll = mean_squared_error(y, ens, squared = False)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._rmse_loss, X = X, y = y)\n",
    "        initial_coef = [1/3, 1/3, 1/3]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \n",
    "        ens = coef[0]*X[:, 0] + coef[1]*X[:, 1] + coef[2]*X[:, 2]\n",
    "        return ens\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5b6bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:27<09:50, 147.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [04:48<07:11, 143.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [07:05<04:40, 140.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [09:13<02:15, 135.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.384172796287736, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.384172796287736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.65989803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.65989803\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10456555506292783, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10456555506292783\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.22841166601766863, subsample=1.0 will be ignored. Current value: bagging_fraction=0.22841166601766863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:35<00:00, 139.09s/it]\n"
     ]
    }
   ],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].median()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "ens_cv_scores, preds = list(), list()\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.671460244215802, \n",
    "                              gamma = 2.5281806276307384, \n",
    "                              learning_rate = 0.002046162779305807, \n",
    "                              max_depth = 8, \n",
    "                              min_child_weight = 80, \n",
    "                              n_estimators = 2690, \n",
    "                              subsample = 0.44886485549735244).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 5420,\n",
    "                               max_depth = 3,\n",
    "                               learning_rate = 0.0014779400349972686,\n",
    "                               num_leaves = 61,\n",
    "                               lambda_l1 = 7.384172796287736,\n",
    "                               lambda_l2 = 0.10456555506292783,\n",
    "                               bagging_fraction = 0.22841166601766863,\n",
    "                               feature_fraction = 0.659898030).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 4738,\n",
    "                                   learning_rate = 0.003143666241424718,\n",
    "                                   depth = 4,\n",
    "                                   random_strength = 0.29823973415192867,\n",
    "                                   bagging_temperature = 0.3408793603898661,\n",
    "                                   border_count = 112,\n",
    "                                   l2_leaf_reg = 17,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        \n",
    "        ######################\n",
    "        ## Optimal Ensemble ##\n",
    "        ######################\n",
    "        \n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        models_pred_oof = np.transpose((XGB_pred_1, lgb_pred_1, cat_pred_1))\n",
    "\n",
    "        opt_ens = OptimizedEnsemble()\n",
    "        opt_ens.fit(models_pred_oof, Y_test)\n",
    "        coef = opt_ens.coefficients()\n",
    "        \n",
    "        ens_pred = opt_ens.predict(models_pred_oof, coef)\n",
    "        ens_cv_scores.append(mean_squared_error(Y_test, ens_pred, squared = False))\n",
    "        \n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        models_pred = np.transpose((XGB_pred_2, lgb_pred_2, cat_pred_2))\n",
    "        \n",
    "        ens_preds = opt_ens.predict(models_pred, coef)\n",
    "        preds.append(ens_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af3385e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the ensemble model is: 11.835470374627613\n"
     ]
    }
   ],
   "source": [
    "ens_cv_score = np.mean(ens_cv_scores) # first run    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the ensemble model is:', ens_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "026f4df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the ensemble model is: 11.835470374627613\n"
     ]
    }
   ],
   "source": [
    "ens_cv_score = np.mean(ens_cv_scores)  # second run  \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the ensemble model is:', ens_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_preds_test = pd.DataFrame(preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = ens_preds_test\n",
    "submission.to_csv('Ensemble_Optuna_baseline_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce1c15",
   "metadata": {},
   "source": [
    "## Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d8d0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CementComponent</th>\n",
       "      <th>BlastFurnaceSlag</th>\n",
       "      <th>FlyAshComponent</th>\n",
       "      <th>WaterComponent</th>\n",
       "      <th>SuperplasticizerComponent</th>\n",
       "      <th>CoarseAggregateComponent</th>\n",
       "      <th>FineAggregateComponent</th>\n",
       "      <th>AgeInDays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>879.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CementComponent  BlastFurnaceSlag  FlyAshComponent  WaterComponent  \\\n",
       "0            102.0               0.0              0.0           192.0   \n",
       "1            102.0             153.0              0.0           192.0   \n",
       "2            102.0             153.0              0.0           192.0   \n",
       "3            102.0             153.0              0.0           192.0   \n",
       "4            102.0             153.0              0.0           192.0   \n",
       "\n",
       "   SuperplasticizerComponent  CoarseAggregateComponent  \\\n",
       "0                        0.0                     879.0   \n",
       "1                        0.0                     887.0   \n",
       "2                        0.0                     887.0   \n",
       "3                        0.0                     887.0   \n",
       "4                        0.0                     887.0   \n",
       "\n",
       "   FineAggregateComponent  AgeInDays  \n",
       "0                   942.0          3  \n",
       "1                   942.0          3  \n",
       "2                   942.0          7  \n",
       "3                   942.0         28  \n",
       "4                   942.0         90  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7977c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c1fa79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b364d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb6ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd352075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5617344e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].median()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "\n",
    "ens_cv_scores, preds = list(), list()\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "#         #############    \n",
    "#         ## XGBoost ##\n",
    "#         #############\n",
    "        \n",
    "#         XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "#                               colsample_bytree = 0.671460244215802, \n",
    "#                               gamma = 2.5281806276307384, \n",
    "#                               learning_rate = 0.002046162779305807, \n",
    "#                               max_depth = 8, \n",
    "#                               min_child_weight = 80, \n",
    "#                               n_estimators = 2690, \n",
    "#                               subsample = 0.44886485549735244).fit(X_train, Y_train)\n",
    "        \n",
    "#         ##############\n",
    "#         ## LightGBM ##\n",
    "#         ##############\n",
    "        \n",
    "#         lgb_md = LGBMRegressor(n_estimators = 5420,\n",
    "#                                max_depth = 3,\n",
    "#                                learning_rate = 0.0014779400349972686,\n",
    "#                                num_leaves = 61,\n",
    "#                                lambda_l1 = 7.384172796287736,\n",
    "#                                lambda_l2 = 0.10456555506292783,\n",
    "#                                bagging_fraction = 0.22841166601766863,\n",
    "#                                feature_fraction = 0.659898030).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 4738,\n",
    "                                   learning_rate = 0.003143666241424718,\n",
    "                                   depth = 4,\n",
    "                                   random_strength = 0.29823973415192867,\n",
    "                                   bagging_temperature = 0.3408793603898661,\n",
    "                                   border_count = 112,\n",
    "                                   l2_leaf_reg = 17,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        \n",
    "        ######################\n",
    "        ## Optimal Ensemble ##\n",
    "        ######################\n",
    "        \n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        models_pred_oof = np.transpose((XGB_pred_1, lgb_pred_1, cat_pred_1))\n",
    "\n",
    "        opt_ens = OptimizedEnsemble()\n",
    "        opt_ens.fit(models_pred_oof, Y_test)\n",
    "        coef = opt_ens.coefficients()\n",
    "        \n",
    "        ens_pred = opt_ens.predict(models_pred_oof, coef)\n",
    "        ens_cv_scores.append(mean_squared_error(Y_test, ens_pred, squared = False))\n",
    "        \n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        models_pred = np.transpose((XGB_pred_2, lgb_pred_2, cat_pred_2))\n",
    "        \n",
    "        ens_preds = opt_ens.predict(models_pred, coef)\n",
    "        preds.append(ens_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebead5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b279e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3902c2a",
   "metadata": {},
   "source": [
    "# Baseline 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7e4d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedEnsemble(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _rmse_loss(self, coef, X, y):\n",
    "        \n",
    "        ens = coef[0]*X[:, 0] + coef[1]*X[:, 1] + coef[2]*X[:, 2]\n",
    "        ll = mean_squared_error(y, ens, squared = False)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._rmse_loss, X = X, y = y)\n",
    "        initial_coef = [1/3, 1/3, 1/3]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \n",
    "        ens = coef[0]*X[:, 0] + coef[1]*X[:, 1] + coef[2]*X[:, 2]\n",
    "        return ens\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2bafac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:24<01:37, 24.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:45<01:08, 22.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:12<00:48, 24.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:37<00:24, 24.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.2582711296889206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.2582711296889206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4895760311486668, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4895760311486668\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.018040323452000916, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018040323452000916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7658802307213928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7658802307213928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:06<00:00, 25.23s/it]\n"
     ]
    }
   ],
   "source": [
    "train_no_dup = train.drop(columns = 'id', axis = 1)\n",
    "train_no_dup = pd.DataFrame(train_no_dup.groupby(train_no_dup.columns.tolist()[0:8])['Strength'].median()).reset_index()\n",
    "\n",
    "X = train_no_dup.drop(columns = ['Strength'], axis = 1)\n",
    "Y = train_no_dup['Strength']\n",
    "\n",
    "X['WaterComponent_to_Cement_ratio'] = X['WaterComponent'] / (X['CementComponent'] + 1e-6)\n",
    "\n",
    "test_baseline = test.drop(columns = ['id'], axis = 1)\n",
    "test_baseline['WaterComponent_to_Cement_ratio'] = test_baseline['WaterComponent'] / (test_baseline['CementComponent'] + 1e-6)\n",
    "\n",
    "ens_cv_scores, preds = list(), list()\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "\n",
    "    skf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X, Y):\n",
    "        \n",
    "        ## Splitting the data \n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                \n",
    "        #############    \n",
    "        ## XGBoost ##\n",
    "        #############\n",
    "        \n",
    "        XGB_md = XGBRegressor(tree_method = 'hist',\n",
    "                              colsample_bytree = 0.7940111565728297, \n",
    "                              gamma = 3.3316249893010292, \n",
    "                              learning_rate = 0.0009167011149361065, \n",
    "                              max_depth = 8, \n",
    "                              min_child_weight = 89, \n",
    "                              n_estimators = 6766, \n",
    "                              subsample = 0.3771654611184001).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## LightGBM ##\n",
    "        ##############\n",
    "        \n",
    "        lgb_md = LGBMRegressor(n_estimators = 4273,\n",
    "                               max_depth = 4,\n",
    "                               learning_rate = 0.002480759916271656,\n",
    "                               num_leaves = 5,\n",
    "                               lambda_l1 = 1.2582711296889206,\n",
    "                               lambda_l2 = 0.018040323452000916,\n",
    "                               bagging_fraction = 0.7658802307213928,\n",
    "                               feature_fraction = 0.4895760311486668).fit(X_train, Y_train)\n",
    "        \n",
    "        ##############\n",
    "        ## CatBoost ##\n",
    "        ##############\n",
    "        \n",
    "        cat_md = CatBoostRegressor(loss_function = 'RMSE',\n",
    "                                   iterations = 416,\n",
    "                                   learning_rate = 0.031225761812299576,\n",
    "                                   depth = 4,\n",
    "                                   random_strength = 0.19789440193456237,\n",
    "                                   bagging_temperature = 0.2831892755259466,\n",
    "                                   border_count = 56,\n",
    "                                   l2_leaf_reg = 26,\n",
    "                                   verbose = False).fit(X_train, Y_train)\n",
    "        \n",
    "        ######################\n",
    "        ## Optimal Ensemble ##\n",
    "        ######################\n",
    "        \n",
    "        XGB_pred_1 = XGB_md.predict(X_test)\n",
    "        lgb_pred_1 = lgb_md.predict(X_test)\n",
    "        cat_pred_1 = cat_md.predict(X_test)\n",
    "        models_pred_oof = np.transpose((XGB_pred_1, lgb_pred_1, cat_pred_1))\n",
    "\n",
    "        opt_ens = OptimizedEnsemble()\n",
    "        opt_ens.fit(models_pred_oof, Y_test)\n",
    "        coef = opt_ens.coefficients()\n",
    "        \n",
    "        ens_pred = opt_ens.predict(models_pred_oof, coef)\n",
    "        ens_cv_scores.append(mean_squared_error(Y_test, ens_pred, squared = False))\n",
    "        \n",
    "        XGB_pred_2 = XGB_md.predict(test_baseline)\n",
    "        lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "        cat_pred_2 = cat_md.predict(test_baseline)\n",
    "        models_pred = np.transpose((XGB_pred_2, lgb_pred_2, cat_pred_2))\n",
    "        \n",
    "        ens_preds = opt_ens.predict(models_pred, coef)\n",
    "        preds.append(ens_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc4d044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the ensemble model is: 11.813790317617261\n"
     ]
    }
   ],
   "source": [
    "ens_cv_score = np.mean(ens_cv_scores) \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the ensemble model is:', ens_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e92065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_preds_test = pd.DataFrame(preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = ens_preds_test\n",
    "submission.to_csv('Ensemble_Optuna_baseline_submission_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b02e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b4b5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is: 12.08682356233276\n"
     ]
    }
   ],
   "source": [
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e3d6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is: 12.069039135775649\n"
     ]
    }
   ],
   "source": [
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0102e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is: 12.088690775686844\n"
     ]
    }
   ],
   "source": [
    "cat_cv_score = np.mean(cat_cv_scores)    \n",
    "print('The average oof rmse score over 5-folds (run 5 times) of the CatBoost model is:', cat_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065898bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5407</td>\n",
       "      <td>47.415894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5408</td>\n",
       "      <td>19.540387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5409</td>\n",
       "      <td>33.601072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5410</td>\n",
       "      <td>46.470701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5411</td>\n",
       "      <td>32.705374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id   Strength\n",
       "0  5407  47.415894\n",
       "1  5408  19.540387\n",
       "2  5409  33.601072\n",
       "3  5410  46.470701\n",
       "4  5411  32.705374"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_preds_test = pd.DataFrame(cat_preds).apply(np.mean, axis = 0)\n",
    "\n",
    "submission['Strength'] = cat_preds_test\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05e17188",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('catboost_submission_6.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
